{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "# !pip install geopandas rasterio shapely scipy scikit-learn scikit-image seaborn matplotlib_scalebar",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Environment location:\", os.path.dirname(sys.executable))"
   ],
   "id": "8cd00d9620efd105",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup",
   "id": "816e84f8aa0bc9db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from unittest.mock import sentinel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling, calculate_default_transform\n",
    "from rasterio.transform import from_bounds, rowcol\n",
    "from rasterio.mask import mask\n",
    "from rasterio import features\n",
    "from shapely.geometry import Point, box, shape\n",
    "from rasterio.features import shapes\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn as sns\n",
    "from dbfread import DBF\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "print(DATA_DIR)\n",
    "\n",
    "COHERENCE_THRESHOLD = 0.3\n",
    "NODATA = -9999\n",
    "GRID_SIZE = 10\n",
    "TARGET_CRS = 'EPSG:32632'\n",
    "\n",
    "CORINE_CLASSES = {\n",
    "    111: 'Continuous urban fabric', 112: 'Discontinuous urban fabric',\n",
    "    121: 'Industrial or commercial units', 122: 'Road and rail networks and associated land',\n",
    "    123: 'Port areas', 124: 'Airports', 131: 'Mineral extraction sites',\n",
    "    132: 'Dump sites', 133: 'Construction sites', 141: 'Green urban areas',\n",
    "    142: 'Sport and leisure facilities', 211: 'Non-irrigated arable land',\n",
    "    212: 'Permanently irrigated land', 213: 'Rice fields', 221: 'Vineyards',\n",
    "    222: 'Fruit trees and berry plantations', 223: 'Olive groves',\n",
    "    231: 'Pastures', 241: 'Annual crops associated with permanent crops',\n",
    "    242: 'Complex cultivation patterns', 243: 'Agriculture/natural vegetation mix',\n",
    "    244: 'Agro-forestry areas', 311: 'Broad-leaved forest',\n",
    "    312: 'Coniferous forest', 313: 'Mixed forest', 321: 'Natural grasslands',\n",
    "    322: 'Moors and heathland', 323: 'Sclerophyllous vegetation',\n",
    "    324: 'Transitional woodland-shrub', 331: 'Beaches, dunes, sands',\n",
    "    332: 'Bare rocks', 333: 'Sparsely vegetated areas', 334: 'Burnt areas',\n",
    "    335: 'Glaciers and perpetual snow', 411: 'Inland marshes',\n",
    "    412: 'Peat bogs', 421: 'Salt marshes', 422: 'Salines',\n",
    "    423: 'Intertidal flats', 511: 'Water courses', 512: 'Water bodies',\n",
    "    521: 'Coastal lagoons', 522: 'Estuaries', 523: 'Sea and ocean'\n",
    "}\n",
    "\n",
    "CORINE_COLORS = {\n",
    "    111: (102, 0, 102), 112: (153, 51, 153), 121: (204, 102, 204), 122: (80, 80, 80),\n",
    "    123: (120, 120, 120), 124: (160, 160, 160), 131: (255, 0, 255), 132: (178, 34, 34),\n",
    "    133: (255, 150, 180), 141: (120, 200, 120), 142: (100, 180, 100),\n",
    "    211: (230, 230, 50), 212: (235, 200, 0), 213: (220, 180, 0), 221: (255, 140, 0),\n",
    "    222: (255, 165, 79), 223: (204, 153, 0), 231: (210, 210, 80), 241: (200, 170, 100),\n",
    "    242: (210, 160, 70), 243: (190, 150, 80), 244: (179, 143, 0),\n",
    "    311: (0, 153, 102), 312: (0, 102, 76), 313: (0, 128, 128), 321: (150, 220, 150),\n",
    "    322: (102, 204, 153), 323: (130, 180, 130), 324: (51, 153, 102), 331: (210, 180, 140),\n",
    "    332: (140, 140, 140), 333: (170, 170, 120), 334: (40, 40, 40), 335: (180, 210, 230),\n",
    "    411: (120, 170, 230), 412: (80, 140, 220), 421: (150, 190, 240), 422: (140, 170, 210),\n",
    "    423: (100, 160, 210), 511: (0, 102, 204), 512: (0, 76, 153), 521: (51, 102, 153),\n",
    "    522: (0, 51, 102), 523: (0, 25, 76)\n",
    "}\n",
    "CORINE_COLORS_MPL = {k: (r/255, g/255, b/255) for k, (r, g, b) in CORINE_COLORS.items()}\n",
    "\n",
    "file_discovery = {\n",
    "    'saocom': (\"saocom_csv\", \"*.csv\"),\n",
    "    'tinitaly': (\"tinitaly\", \"*.tif\"),\n",
    "    'copernicus': (\"copernicus\", \"*.tif\"),\n",
    "    'corine': (\"ground_cover\", \"*.tif\"),\n",
    "    'sentinel': (\"sentinel_data\", \"*.tif\")\n",
    "}\n",
    "for key, (subdir, pattern) in file_discovery.items():\n",
    "    files = list((DATA_DIR / subdir).glob(pattern))\n",
    "    globals()[f'{key}_path'] = files[0] if files else None\n",
    "\n",
    "corine_dbf_path = None\n",
    "if corine_path:\n",
    "    candidates = list((DATA_DIR / \"ground_cover\").glob(f\"{corine_path.name}.vat.dbf\"))\n",
    "    corine_dbf_path = candidates[0] if candidates else None"
   ],
   "id": "86d3c5cc48f5afdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Data",
   "id": "61c9d37ea014e0e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(saocom_path)\n",
    "df.columns = ['ID', 'SVET', 'LVET', 'LAT', 'LAT2', 'LON', 'LON2', 'HEIGHT', 'HEIGHT_WRT_DEM', 'SIGMA_HEIGHT', 'COHER']\n",
    "\n",
    "for col in ['LAT', 'LON', 'LAT2', 'LON2', 'HEIGHT', 'COHER']:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df = df.dropna(subset=['LAT', 'LON', 'LAT2', 'LON2', 'HEIGHT', 'COHER']).query('LAT2 != 0 and LON2 != 0')\n",
    "df.rename(columns={'LAT': 'LAT_old', 'LON': 'LON_old', 'LAT2': 'LAT', 'LON2': 'LON'}, inplace=True)\n",
    "df_filtered = df[df['COHER'] >= COHERENCE_THRESHOLD]\n",
    "\n",
    "saocom_gdf = gpd.GeoDataFrame(\n",
    "    df_filtered,\n",
    "    geometry=[Point(lon, lat) for lon, lat in zip(df_filtered['LON'], df_filtered['LAT'])],\n",
    "    crs='EPSG:4326'\n",
    ").to_crs(TARGET_CRS)\n",
    "saocom_gdf['x_utm'] = saocom_gdf.geometry.x\n",
    "saocom_gdf['y_utm'] = saocom_gdf.geometry.y\n",
    "\n",
    "def _read_raster_meta(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        return src.crs, src.res, src.bounds, src.nodata\n",
    "\n",
    "tinitaly_crs, tinitaly_res, tinitaly_bounds, tinitaly_nodata = _read_raster_meta(tinitaly_path)\n",
    "copernicus_crs, copernicus_res, copernicus_bounds, copernicus_nodata = _read_raster_meta(copernicus_path)"
   ],
   "id": "6968c5b6b7f26923",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### HORIZONTAL DATUM VERIFICATION",
   "id": "8c02719332a8f08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def remove_isolated_knn(gdf, k=5, distance_threshold=100):\n",
    "    coords = np.column_stack((gdf.geometry.x, gdf.geometry.y))\n",
    "    distances = NearestNeighbors(n_neighbors=k+1).fit(coords).kneighbors(coords, return_distance=True)[0]\n",
    "    return gdf[(distances[:, 1:].mean(1) < distance_threshold)].reset_index(drop=True)\n",
    "\n",
    "# ---- Horizontal datum verification\n",
    "tinitaly_needs_reproject  = str(tinitaly_crs)  != TARGET_CRS\n",
    "copernicus_needs_reproject = str(copernicus_crs) != TARGET_CRS\n",
    "# corine_needs_reproject   = str(corine_crs)   != TARGET_CRS\n",
    "\n",
    "# ---- Vertical datum verification\n",
    "tinitaly_wkt, copernicus_wkt = tinitaly_crs.to_wkt(), copernicus_crs.to_wkt()\n",
    "tinitaly_vertical   = ('EGM2008' in tinitaly_wkt)   or ('geoid' in tinitaly_wkt.lower())\n",
    "copernicus_vertical = ('EGM2008' in copernicus_wkt) or ('geoid' in copernicus_wkt.lower())\n",
    "\n",
    "# ---- Study area bounds / hull\n",
    "study_bounds = saocom_gdf.total_bounds  # [xmin, ymin, xmax, ymax]\n",
    "study_area_poly = box(*study_bounds)\n",
    "study_area_gdf = gpd.GeoDataFrame([1], geometry=[study_area_poly], crs=TARGET_CRS)\n",
    "saocom_gdf = remove_isolated_knn(saocom_gdf, k=5, distance_threshold=100)\n",
    "data_hull = saocom_gdf.unary_union.convex_hull\n",
    "hull_gdf = gpd.GeoDataFrame(geometry=[data_hull], crs=TARGET_CRS)\n",
    "\n",
    "# ---- 10m grid parameters\n",
    "xmin_grid, ymin_grid, xmax_grid, ymax_grid = (\n",
    "    np.floor(study_bounds[0]/GRID_SIZE)*GRID_SIZE,\n",
    "    np.floor(study_bounds[1]/GRID_SIZE)*GRID_SIZE,\n",
    "    np.ceil( study_bounds[2]/GRID_SIZE)*GRID_SIZE,\n",
    "    np.ceil( study_bounds[3]/GRID_SIZE)*GRID_SIZE\n",
    ")\n",
    "grid_width  = int((xmax_grid - xmin_grid)/GRID_SIZE)\n",
    "grid_height = int((ymax_grid - ymin_grid)/GRID_SIZE)\n",
    "target_transform = from_bounds(xmin_grid, ymin_grid, xmax_grid, ymax_grid, grid_width, grid_height)\n",
    "\n",
    "# ---- Reference dataset metadata\n",
    "reference_dems = {\n",
    "    'tinitaly_crop': {\n",
    "        'path': tinitaly_path,\n",
    "        'crs': tinitaly_crs,\n",
    "        'needs_reproject': tinitaly_needs_reproject,\n",
    "        'vertical_datum': 'WGS84 ellipsoid'\n",
    "    },\n",
    "    'copernicus': {\n",
    "        'path': copernicus_path,\n",
    "        'crs': copernicus_crs,\n",
    "        'needs_reproject': copernicus_needs_reproject,\n",
    "        'vertical_datum': 'EGM2008 geoid'\n",
    "    }\n",
    "}\n"
   ],
   "id": "b2950bf77e5862ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### RESAMPLE TO 10M",
   "id": "53349768947cd60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Resample helpers (10 m)\n",
    "profile = {\n",
    "    'driver':'GTiff','dtype':'float32','width':grid_width,'height':grid_height,\n",
    "    'count':1,'crs':TARGET_CRS,'transform':target_transform,\n",
    "    'nodata':NODATA,'compress':'lzw'\n",
    "}\n",
    "\n",
    "def _resample_to_10m(src_path, out_name):\n",
    "    arr = np.full((grid_height, grid_width), NODATA, np.float32)\n",
    "    with rasterio.open(src_path) as src:\n",
    "        reproject(\n",
    "            rasterio.band(src, 1), arr,\n",
    "            src_transform=src.transform, src_crs=src.crs,\n",
    "            dst_transform=target_transform, dst_crs=TARGET_CRS,\n",
    "            resampling=Resampling.cubic,\n",
    "            src_nodata=src.nodata, dst_nodata=NODATA\n",
    "        )\n",
    "    out_path = RESULTS_DIR / out_name\n",
    "    with rasterio.open(out_path, 'w', **profile) as dst: dst.write(arr, 1)\n",
    "    return arr, out_path\n",
    "\n",
    "# --- Resample TINITALY & Copernicus\n",
    "tinitaly_10m, tinitaly_10m_path       = _resample_to_10m(tinitaly_path,  \"tinitaly_10m.tif\")\n",
    "copernicus_10m, copernicus_10m_path   = _resample_to_10m(copernicus_path,\"copernicus_10m.tif\")\n",
    "\n",
    "# --- Update reference metadata\n",
    "reference_dems['tinitaly_crop'].update(resampled_path=tinitaly_10m_path, is_10m=True)\n",
    "reference_dems['copernicus'].update(resampled_path=copernicus_10m_path, is_10m=True)\n"
   ],
   "id": "3894d56fa3b5acd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CREATE RASTERIZED MASK FROM SAOCOM CONVEX HULL",
   "id": "f05f4e85f1fc9ad9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Rasterized hull mask (10 m grid)\n",
    "hull_mask = features.rasterize(\n",
    "    [data_hull], out_shape=(grid_height, grid_width),\n",
    "    transform=target_transform, fill=0, all_touched=True, dtype='uint8'\n",
    ").astype(bool)\n",
    "\n",
    "def _mask_and_write(arr, out_name):\n",
    "    masked = arr.copy()\n",
    "    masked[~hull_mask] = NODATA\n",
    "    out_path = RESULTS_DIR / out_name\n",
    "    with rasterio.open(out_path, 'w', **profile) as dst: dst.write(masked, 1)\n",
    "    return masked, out_path\n",
    "\n",
    "# --- Mask + save\n",
    "tinitaly_10m,     tinitaly_masked_path   = _mask_and_write(tinitaly_10m,   \"tinitaly_10m_masked.tif\")\n",
    "copernicus_10m,   copernicus_masked_path = _mask_and_write(copernicus_10m, \"copernicus_10m_masked.tif\")\n",
    "\n",
    "# --- Update reference metadata\n",
    "reference_dems['tinitaly_crop']['masked_path'] = tinitaly_masked_path\n",
    "reference_dems['copernicus']['masked_path']    = copernicus_masked_path\n"
   ],
   "id": "d2b3cac818bd758c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SAMPLE REFERENCE DEMS AT SAOCOM LOCATIONS",
   "id": "651324acaed34b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Sample reference DEMs at SAOCOM locations (vectorized)\n",
    "xs, ys = saocom_gdf.geometry.x.values, saocom_gdf.geometry.y.values\n",
    "rows, cols = rowcol(target_transform, xs, ys)\n",
    "inb = (rows>=0)&(rows<grid_height)&(cols>=0)&(cols<grid_width)\n",
    "\n",
    "def _sample(arr):\n",
    "    out = np.full(len(saocom_gdf), np.nan, dtype=np.float32)\n",
    "    v = arr[rows[inb], cols[inb]]\n",
    "    out[inb] = np.where(v==NODATA, np.nan, v)\n",
    "    return out\n",
    "\n",
    "saocom_gdf['tinitaly_height']   = _sample(tinitaly_10m)\n",
    "saocom_gdf['copernicus_height'] = _sample(copernicus_10m)\n",
    "saocom_gdf['HEIGHT_RELATIVE']   = saocom_gdf['HEIGHT']\n",
    "\n",
    "# --- Calibration helper (constant offset)\n",
    "def _calibrate(ref_col, out_col):\n",
    "    m = (\n",
    "        (saocom_gdf['COHER']>=0.8) &\n",
    "        saocom_gdf[ref_col].notna() &\n",
    "        saocom_gdf['HEIGHT_RELATIVE'].notna() &\n",
    "        (np.abs(saocom_gdf['HEIGHT_RELATIVE'])<1000)\n",
    "    )\n",
    "    sp = saocom_gdf[m]\n",
    "    diff = sp[ref_col] - sp['HEIGHT_RELATIVE']\n",
    "    offset = np.median(diff)\n",
    "    saocom_gdf[out_col] = saocom_gdf['HEIGHT_RELATIVE'] + offset\n",
    "    rmse = np.sqrt(np.mean((sp[ref_col] - (sp['HEIGHT_RELATIVE'] + offset))**2))\n",
    "    return offset, rmse, len(sp)\n",
    "\n",
    "# --- Calibrate to TINITALY & Copernicus\n",
    "offset_tinitaly,  rmse_tin, n_tin = _calibrate('tinitaly_height',   'HEIGHT_ABSOLUTE_TIN')\n",
    "offset_copernicus, rmse_cop, n_cop = _calibrate('copernicus_height', 'HEIGHT_ABSOLUTE_COP')\n",
    "\n",
    "# --- Concise report\n",
    "print(f\"TINITALY: n={n_tin:,}, offset={offset_tinitaly:.3f} m, RMSE={rmse_tin:.3f} m\")\n",
    "print(f\"COPERNICUS: n={n_cop:,}, offset={offset_copernicus:.3f} m, RMSE={rmse_cop:.3f} m\")\n",
    "print(\"Recommendation: Use HEIGHT_ABSOLUTE_TIN (usually lower RMSE).\")\n"
   ],
   "id": "b545c5dfde927107",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CREATE SAOCOM COVERAGE GRID",
   "id": "7ce1b23f2deecf20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- SAOCOM coverage grid (10 m)\n",
    "saocom_rows, saocom_cols = rowcol(\n",
    "    target_transform,\n",
    "    saocom_gdf.geometry.x.values,\n",
    "    saocom_gdf.geometry.y.values\n",
    ")\n",
    "inb = (saocom_rows>=0)&(saocom_rows<grid_height)&(saocom_cols>=0)&(saocom_cols<grid_width)\n",
    "saocom_coverage = np.zeros((grid_height, grid_width), dtype=bool)\n",
    "saocom_coverage[saocom_rows[inb], saocom_cols[inb]] = True\n",
    "\n",
    "# --- Void stats (inside hull, excluding nodata)\n",
    "study_area_mask = hull_mask\n",
    "void_mask = study_area_mask & ~saocom_coverage\n",
    "\n",
    "n_total_cells     = study_area_mask.sum()\n",
    "n_occupied_cells  = (study_area_mask & saocom_coverage).sum()\n",
    "n_void_cells      = void_mask.sum()\n",
    "void_percentage   = 100 * n_void_cells / n_total_cells if n_total_cells else 0\n",
    "print(void_percentage)\n",
    "\n",
    "# --- Save void mask raster (0=data area, 1=void, 255=outside)\n",
    "void_mask_path = RESULTS_DIR / \"saocom_void_mask.tif\"\n",
    "profile_void = {**profile, 'dtype':'uint8', 'nodata':255}\n",
    "void_raster = np.where(~study_area_mask, 255, np.where(void_mask, 1, 0)).astype(np.uint8)\n",
    "\n",
    "with rasterio.open(void_mask_path, 'w', **profile_void) as dst:\n",
    "    dst.write(void_raster, 1)\n"
   ],
   "id": "54fb161ba66aeecc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LOAD REFERENCE DEM DATA (Already in memory from Cell 4)",
   "id": "e6663a8a847aa387"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Load reference DEM data (already in memory)\n",
    "tinitaly_data   = tinitaly_10m.copy()\n",
    "copernicus_data = copernicus_10m.copy()\n",
    "\n",
    "# --- Elevation difference & valid mask\n",
    "elevation_diff = tinitaly_data - copernicus_data\n",
    "valid_mask     = (tinitaly_data != NODATA) & (copernicus_data != NODATA)\n",
    "\n",
    "valid_pixels    = int(valid_mask.sum())\n",
    "valid_diffs     = elevation_diff[valid_mask]\n",
    "valid_tinitaly  = tinitaly_data[valid_mask]\n",
    "valid_copernicus= copernicus_data[valid_mask]\n",
    "\n",
    "# --- Reference comparison statistics\n",
    "m  = float(np.mean(valid_diffs))\n",
    "md = float(np.median(valid_diffs))\n",
    "sd = float(np.std(valid_diffs))\n",
    "rm = float(np.sqrt(np.mean(valid_diffs**2)))\n",
    "ma = float(np.mean(np.abs(valid_diffs)))\n",
    "nmad = float(1.4826 * np.median(np.abs(valid_diffs - md)))\n",
    "ref_metrics = {\n",
    "    'n_pixels': valid_pixels, 'mean_diff': m, 'median_diff': md, 'std_diff': sd,\n",
    "    'rmse': rm, 'mae': ma, 'nmad': nmad,\n",
    "    'min_diff': float(np.min(valid_diffs)), 'max_diff': float(np.max(valid_diffs)),\n",
    "    'correlation': float(np.corrcoef(valid_tinitaly, valid_copernicus)[0,1])\n",
    "}\n",
    "\n",
    "# --- Equality tolerance using NMAD\n",
    "equal_tolerance = ref_metrics['nmad']\n",
    "\n",
    "# --- Directional comparison grids with equality buffer\n",
    "tinitaly_higher_mask = valid_mask & (elevation_diff >  equal_tolerance)\n",
    "tinitaly_lower_mask  = valid_mask & (elevation_diff < -equal_tolerance)\n",
    "roughly_equal_mask   = valid_mask & (np.abs(elevation_diff) <= equal_tolerance)\n",
    "\n",
    "tinitaly_higher_data = np.where(tinitaly_higher_mask, elevation_diff, np.nan)\n",
    "tinitaly_lower_data  = np.where(tinitaly_lower_mask,  elevation_diff, np.nan)\n",
    "roughly_equal_data   = np.where(roughly_equal_mask,   elevation_diff, np.nan)\n",
    "\n",
    "higher_pixels = int(tinitaly_higher_mask.sum())\n",
    "lower_pixels  = int(tinitaly_lower_mask.sum())\n",
    "equal_pixels  = int(roughly_equal_mask.sum())\n",
    "\n",
    "pct_higher = float(100 * higher_pixels / valid_pixels) if valid_pixels else 0.0\n",
    "pct_lower  = float(100 * lower_pixels  / valid_pixels) if valid_pixels else 0.0\n",
    "pct_equal  = float(100 * equal_pixels  / valid_pixels) if valid_pixels else 0.0\n",
    "\n",
    "# --- Height statistics comparison\n",
    "def calculate_height_stats(data, name):\n",
    "    v = np.asarray(data)\n",
    "    v = v[~np.isnan(v)]\n",
    "    if v.size == 0: return None\n",
    "    q25, q75 = np.percentile(v, [25, 75])\n",
    "    return {\n",
    "        'Dataset': name, 'Count': int(v.size),\n",
    "        'Min': float(v.min()), 'Max': float(v.max()),\n",
    "        'Mean': float(v.mean()), 'Median': float(np.median(v)),\n",
    "        'Std Dev': float(v.std()), 'Range': float(v.max()-v.min()),\n",
    "        'Q25': float(q25), 'Q75': float(q75), 'IQR': float(q75-q25)\n",
    "    }\n",
    "\n",
    "stats_list = [\n",
    "    calculate_height_stats(saocom_gdf['HEIGHT_RELATIVE'].values, 'SAOCOM (Relative)'),\n",
    "    calculate_height_stats(saocom_gdf['tinitaly_height'].values,  'TINITALY (at SAOCOM pts)'),\n",
    "    calculate_height_stats(saocom_gdf['copernicus_height'].values,'Copernicus (at SAOCOM pts)'),\n",
    "    calculate_height_stats(tinitaly_10m[tinitaly_10m!=NODATA],     'TINITALY (Full Grid)'),\n",
    "    calculate_height_stats(copernicus_10m[copernicus_10m!=NODATA], 'Copernicus (Full Grid)')\n",
    "]\n",
    "stats_df = pd.DataFrame([s for s in stats_list if s])\n",
    "\n",
    "# --- Compact displays\n",
    "print(\"\\nHEIGHT STATS SUMMARY (m)\\n\", stats_df.to_string(index=False, float_format=lambda x: f'{x:.2f}'), sep='')\n",
    "\n",
    "# SAOCOM vs references\n",
    "diff_tin_valid = (saocom_gdf['HEIGHT_RELATIVE'] - saocom_gdf['tinitaly_height']).dropna().values\n",
    "diff_cop_valid = (saocom_gdf['HEIGHT_RELATIVE'] - saocom_gdf['copernicus_height']).dropna().values\n",
    "\n",
    "def _summ(v):\n",
    "    return f\"mean={np.mean(v):+.3f}, median={np.median(v):+.3f}, sd={np.std(v):.3f}, rmse={np.sqrt(np.mean(v**2)):.3f} m\"\n",
    "\n",
    "print(\"\\nSAOCOM\u2212TINITALY:\",  _summ(diff_tin_valid))\n",
    "print(\"SAOCOM\u2212Copernicus:\", _summ(diff_cop_valid))\n",
    "\n",
    "# Reference check (TINITALY\u2212Copernicus)\n",
    "ref_diff_valid = (tinitaly_10m[valid_mask] - copernicus_10m[valid_mask])\n",
    "ref_diff_valid = ref_diff_valid[~np.isnan(ref_diff_valid)]\n",
    "print(\"\\nTINITALY\u2212Copernicus:\", _summ(ref_diff_valid))\n"
   ],
   "id": "86bbe0cb6f0b1b68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LOAD DBF LOOKUP TABLE",
   "id": "b265fb24777a1820"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 3) Load DBF lookup\n",
    "dbf_table  = DBF(corine_dbf_path, load=True)\n",
    "lookup_df  = pd.DataFrame(iter(dbf_table))\n",
    "value_to_code = dict(zip(lookup_df['Value'],   lookup_df['CODE_18']))\n",
    "value_to_label= dict(zip(lookup_df['Value'],   lookup_df['LABEL3']))\n",
    "code_to_label = dict(zip(lookup_df['CODE_18'], lookup_df['LABEL3']))\n",
    "\n",
    "# --- 4) Load, crop by hull, and remap CORINE\n",
    "with rasterio.open(corine_path) as src:\n",
    "    hull_corine_crs = hull_gdf.to_crs(src.crs)\n",
    "    corine_raw, crop_transform = mask(src, hull_corine_crs.geometry, crop=True, filled=False)\n",
    "    corine_raw     = corine_raw[0]\n",
    "    corine_crs     = src.crs\n",
    "    corine_res     = src.res\n",
    "    corine_nodata  = src.nodata if src.nodata is not None else 255\n",
    "    corine_bounds  = src.bounds\n",
    "\n",
    "# Remap: Value \u2192 CODE_18 (default 0); keep 0 for nodata\n",
    "corine_remapped = np.vectorize(value_to_code.get, otypes=[np.uint16])(corine_raw)\n",
    "corine_remapped[corine_raw == corine_nodata] = 0\n",
    "\n",
    "# (Optional) Save intermediate remapped crop\n",
    "corine_remapped_path = RESULTS_DIR / \"corine_remapped_cropped.tif\"\n",
    "profile_remapped = {\n",
    "    'driver':'GTiff','dtype':'uint16','width':corine_remapped.shape[1],'height':corine_remapped.shape[0],\n",
    "    'count':1,'crs':corine_crs,'transform':crop_transform,'nodata':0,'compress':'lzw'\n",
    "}\n",
    "with rasterio.open(corine_remapped_path,'w',**profile_remapped) as dst: dst.write(corine_remapped,1)\n",
    "\n",
    "# --- 5) Resample to 10 m grid (nearest for categorical)\n",
    "corine_10m = np.zeros((grid_height, grid_width), dtype=np.uint16)\n",
    "reproject(\n",
    "    source=corine_remapped, destination=corine_10m,\n",
    "    src_transform=crop_transform, src_crs=corine_crs,\n",
    "    dst_transform=target_transform, dst_crs=TARGET_CRS,\n",
    "    resampling=Resampling.nearest, src_nodata=0, dst_nodata=0\n",
    ")\n",
    "\n",
    "corine_10m_path = RESULTS_DIR / \"corine_10m.tif\"\n",
    "profile_10m = {'driver':'GTiff','dtype':'uint16','width':grid_width,'height':grid_height,\n",
    "               'count':1,'crs':TARGET_CRS,'transform':target_transform,'nodata':0,'compress':'lzw'}\n",
    "with rasterio.open(corine_10m_path,'w',**profile_10m) as dst: dst.write(corine_10m,1)\n",
    "\n",
    "# --- 6) Mask to hull\n",
    "corine_10m_masked = corine_10m.copy()\n",
    "corine_10m_masked[~hull_mask] = 0\n",
    "corine_masked_path = RESULTS_DIR / \"corine_10m_masked.tif\"\n",
    "with rasterio.open(corine_masked_path,'w',**profile_10m) as dst: dst.write(corine_10m_masked,1)\n",
    "corine_10m = corine_10m_masked  # update working array\n",
    "\n",
    "# --- Summary (concise)\n",
    "unique_codes = np.unique(corine_10m[corine_10m>0])\n",
    "print(f\"CORINE done | CRS={corine_crs} | classes={len(unique_codes)} | res={GRID_SIZE} m\")\n",
    "print(f\"Classes: {sorted(unique_codes)}\")\n",
    "print(f\"Output: {corine_masked_path}\")\n"
   ],
   "id": "aaf3725115ec5742",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SIMPLE SPATIAL OVERLAP VISUALIZATION",
   "id": "573965626fbb4f93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Simple spatial overlap visualization\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10), facecolor='white')\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "# 1) TINITALY extent (reprojected for plotting)\n",
    "with rasterio.open(tinitaly_path) as src:\n",
    "    dem_bounds_target = rasterio.warp.transform_bounds(src.crs, TARGET_CRS, *src.bounds)\n",
    "    ax.add_patch(Rectangle(\n",
    "        (dem_bounds_target[0], dem_bounds_target[1]),\n",
    "        dem_bounds_target[2]-dem_bounds_target[0],\n",
    "        dem_bounds_target[3]-dem_bounds_target[1],\n",
    "        linewidth=3, edgecolor='blue', facecolor='none', label='TINITALY Extent'\n",
    "    ))\n",
    "\n",
    "# 2) SAOCOM points + 3) hull\n",
    "saocom_gdf.plot(ax=ax, markersize=1, color='red', alpha=0.5, label='SAOCOM Points')\n",
    "hull_gdf.boundary.plot(ax=ax, color='green', linewidth=2, linestyle='--', label='Study Area Hull')\n",
    "\n",
    "# Labels / legend / grid\n",
    "ax.set(xlabel='UTM Easting (m)', ylabel='UTM Northing (m)', title='Spatial Coverage: SAOCOM vs TINITALY DEM')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3, color='gray')\n",
    "\n",
    "# Extent textbox\n",
    "sxmin, sxmax = saocom_gdf.geometry.x.min(), saocom_gdf.geometry.x.max()\n",
    "symin, symax = saocom_gdf.geometry.y.min(), saocom_gdf.geometry.y.max()\n",
    "info_text = (\n",
    "    f\"SAOCOM Extent:\\nX: [{sxmin:.0f}, {sxmax:.0f}]\\nY: [{symin:.0f}, {symax:.0f}]\\n\\n\"\n",
    "    f\"TINITALY Extent (UTM 32N):\\nX: [{dem_bounds_target[0]:.0f}, {dem_bounds_target[2]:.0f}]\\n\"\n",
    "    f\"Y: [{dem_bounds_target[1]:.0f}, {dem_bounds_target[3]:.0f}]\"\n",
    ")\n",
    "ax.text(0.02, 0.98, info_text, transform=ax.transAxes, fontsize=9, va='top', family='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='black'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"spatial_coverage.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- Overlap check\n",
    "print(\"\\n=== OVERLAP CHECK ===\")\n",
    "overlap_x = not (sxmax < dem_bounds_target[0] or sxmin > dem_bounds_target[2])\n",
    "overlap_y = not (symax < dem_bounds_target[1] or symin > dem_bounds_target[3])\n",
    "print(f\"X-axis overlap: {overlap_x}\")\n",
    "print(f\"Y-axis overlap: {overlap_y}\")\n",
    "print(f\"Full overlap: {overlap_x and overlap_y}\")\n",
    "if not (overlap_x and overlap_y):\n",
    "    print(\"\\n\u26a0\ufe0f NO OVERLAP DETECTED - SAOCOM data is outside TINITALY coverage!\\n\"\n",
    "          \"Use a different TINITALY tile that covers this area.\")\n"
   ],
   "id": "64f7bca6720b4967",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### COMPREHENSIVE REFERENCE DEM COMPARISON VISUALIZATION",
   "id": "b8f504554b1d9f36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Comprehensive reference DEM comparison visualization (condensed)\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(20, 28), facecolor='white')\n",
    "extent = [xmin_grid, xmax_grid, ymin_grid, ymax_grid]\n",
    "\n",
    "def _imshow(ax, data, cmap, title, cbar_label, vlims=None, stats_arr=None):\n",
    "    ax.set_facecolor('white')\n",
    "    disp = np.ma.masked_equal(data, NODATA) if (data is tinitaly_data or data is copernicus_data) else np.ma.masked_invalid(data)\n",
    "    cm = cmap.copy(); cm.set_bad(color='white', alpha=0)\n",
    "    im = ax.imshow(disp, cmap=cm, origin='upper', extent=extent, **({'vmin':vlims[0],'vmax':vlims[1]} if vlims else {}))\n",
    "    hull_gdf.boundary.plot(ax=ax, color='darkred', linewidth=2.5, label='Study Area')\n",
    "    ax.set(title=title, xlabel='UTM Easting (m)', ylabel='UTM Northing (m)')\n",
    "    ax.grid(True, color='black', alpha=0.3, linewidth=0.5); ax.tick_params(colors='black')\n",
    "    cb = plt.colorbar(im, ax=ax, label=cbar_label, shrink=0.8)\n",
    "    cb.ax.yaxis.label.set_color('black'); cb.ax.tick_params(colors='black')\n",
    "    if stats_arr is not None and stats_arr.size:\n",
    "        txt = f\"Min: {np.nanmin(stats_arr):.1f}m\\nMax: {np.nanmax(stats_arr):.1f}m\\nMean: {np.nanmean(stats_arr):.1f}m\\nStd: {np.nanstd(stats_arr):.1f}m\"\n",
    "        ax.text(0.02, 0.98, txt, transform=ax.transAxes, fontsize=9, va='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='black'))\n",
    "    return im\n",
    "\n",
    "# 1) TINITALY elevation\n",
    "_imshow(axes[0,0], tinitaly_data, plt.cm.terrain, 'TINITALY Elevation', 'Elevation (m)', stats_arr=np.where(tinitaly_data!=NODATA, tinitaly_data, np.nan))\n",
    "\n",
    "# 2) Copernicus elevation\n",
    "_imshow(axes[0,1], copernicus_data, plt.cm.terrain, 'Copernicus Elevation', 'Elevation (m)', stats_arr=np.where(copernicus_data!=NODATA, copernicus_data, np.nan))\n",
    "\n",
    "# 3) Difference map (TINITALY \u2212 Copernicus) over valid area\n",
    "diff_display = np.where(valid_mask, elevation_diff, np.nan)\n",
    "diff_limit   = np.percentile(np.abs(valid_diffs), 95)\n",
    "stats3 = (\n",
    "    f\"Pixels: {valid_pixels:,}\\n\"\n",
    "    f\"Mean: {ref_metrics['mean_diff']:+.2f}m\\nRMSE: {ref_metrics['rmse']:.2f}m\\n\"\n",
    "    f\"NMAD: {ref_metrics['nmad']:.2f}m\\nMAE: {ref_metrics['mae']:.2f}m\\n\"\n",
    "    f\"Std: {ref_metrics['std_diff']:.2f}m\\nCorr: {ref_metrics['correlation']:.3f}\\n\"\n",
    "    f\"Range: [{ref_metrics['min_diff']:.1f}, {ref_metrics['max_diff']:.1f}]m\"\n",
    ")\n",
    "ax = axes[1,0]\n",
    "_imshow(ax, diff_display, plt.cm.coolwarm, 'Elevation Difference\\n(TINITALY - Copernicus)', 'Difference (m)',\n",
    "        vlims=(-diff_limit, diff_limit))\n",
    "ax.text(0.02, 0.98, stats3, transform=ax.transAxes, fontsize=8, va='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='black'))\n",
    "\n",
    "# 4) Statistics summary\n",
    "ax = axes[1,1]; ax.set_facecolor('white'); ax.axis('off'); ax.set_title('Summary Statistics', fontweight='bold', fontsize=12, color='black')\n",
    "le68, le90, le95 = np.percentile(np.abs(valid_diffs), [68.27, 90, 95])\n",
    "stats_text = f\"\"\"REFERENCE DEM COMPARISON\n",
    "\n",
    "Valid Pixels: {valid_pixels:,}\n",
    "\n",
    "CRITICAL METRICS:\n",
    "Mean Error (Bias): {ref_metrics['mean_diff']:+.2f} m\n",
    "RMSE: {ref_metrics['rmse']:.2f} m\n",
    "NMAD (robust): {ref_metrics['nmad']:.2f} m\n",
    "Std Deviation: {ref_metrics['std_diff']:.2f} m\n",
    "\n",
    "SECONDARY METRICS:\n",
    "MAE: {ref_metrics['mae']:.2f} m\n",
    "Correlation: {ref_metrics['correlation']:.4f}\n",
    "LE68: {le68:.2f} m\n",
    "LE90: {le90:.2f} m\n",
    "LE95: {le95:.2f} m\n",
    "Median: {ref_metrics['median_diff']:+.2f} m\n",
    "\n",
    "DIRECTIONAL BREAKDOWN:\n",
    "(Tolerance: \u00b1{equal_tolerance:.2f} m)\n",
    "TINITALY Higher: {higher_pixels:,} ({pct_higher:.1f}%)\n",
    "Copernicus Higher: {lower_pixels:,} ({pct_lower:.1f}%)\n",
    "Roughly Equal: {equal_pixels:,} ({pct_equal:.1f}%)\n",
    "\n",
    "Range: {ref_metrics['min_diff']:+.1f} to {ref_metrics['max_diff']:+.1f} m\n",
    "\"\"\"\n",
    "ax.text(0.05, 0.5, stats_text, transform=ax.transAxes, fontfamily='monospace', fontsize=9, va='center', color='black')\n",
    "\n",
    "# 5) Where TINITALY > Copernicus\n",
    "ax = axes[2,0]\n",
    "_imshow(ax, tinitaly_higher_data, plt.cm.YlOrRd, 'TINITALY > Copernicus', 'Difference (m)', vlims=(0, np.nanmax(tinitaly_higher_data)))\n",
    "hv = tinitaly_higher_data[~np.isnan(tinitaly_higher_data)]\n",
    "ax.text(0.02, 0.98, f\"Pixels: {higher_pixels:,} ({pct_higher:.1f}%)\\nMean: {np.mean(hv):.2f}m\\nStd: {np.std(hv):.2f}m\\nRMSE: {np.sqrt((hv**2).mean()):.2f}m\\nMax: {np.max(hv):.2f}m\",\n",
    "        transform=ax.transAxes, fontsize=8, va='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='black'))\n",
    "\n",
    "# 6) Where TINITALY < Copernicus\n",
    "ax = axes[2,1]\n",
    "_imshow(ax, tinitaly_lower_data, plt.cm.Blues_r, 'Copernicus > TINITALY', 'Difference (m)',\n",
    "        vlims=(np.nanmin(tinitaly_lower_data), 0))\n",
    "lv = tinitaly_lower_data[~np.isnan(tinitaly_lower_data)]\n",
    "ax.text(0.02, 0.98, f\"Pixels: {lower_pixels:,} ({pct_lower:.1f}%)\\nMean: {np.mean(lv):.2f}m\\nStd: {np.std(lv):.2f}m\\nRMSE: {np.sqrt((lv**2).mean()):.2f}m\\nMin: {np.min(lv):.2f}m\",\n",
    "        transform=ax.transAxes, fontsize=8, va='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='black'))\n",
    "\n",
    "# 7) Where roughly equal\n",
    "ax = axes[3,0]\n",
    "_imshow(ax, roughly_equal_data, plt.cm.Greens, f'Roughly Equal (\u00b1{equal_tolerance:.2f}m)', 'Difference (m)',\n",
    "        vlims=(-equal_tolerance, equal_tolerance))\n",
    "ev = roughly_equal_data[~np.isnan(roughly_equal_data)]\n",
    "ax.text(0.02, 0.98, f\"Pixels: {equal_pixels:,} ({pct_equal:.1f}%)\\nMean: {np.mean(ev):.2f}m\\nStd: {np.std(ev):.2f}m\\nRMSE: {np.sqrt((ev**2).mean()):.2f}m\\nMAE: {np.mean(np.abs(ev)):.2f}m\",\n",
    "        transform=ax.transAxes, fontsize=8, va='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='black'))\n",
    "\n",
    "# 8) Histogram of differences\n",
    "ax = axes[3,1]; ax.set_facecolor('white')\n",
    "n, bins, patches = ax.hist(valid_diffs, bins=50, alpha=0.75, color='steelblue', edgecolor='black')\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero')\n",
    "ax.axvline(ref_metrics['mean_diff'], color='green', linestyle='-', linewidth=2, label=f\"Mean: {ref_metrics['mean_diff']:+.2f}m\")\n",
    "ax.axvline(equal_tolerance, color='orange', linestyle='--', linewidth=1.5, label=f'\u00b1NMAD: {equal_tolerance:.2f}m')\n",
    "ax.axvline(-equal_tolerance, color='orange', linestyle='--', linewidth=1.5)\n",
    "ax.text(0.97, 0.97,\n",
    "        f\"Mean = {valid_diffs.mean():+.2f} m\\nStd Dev = {valid_diffs.std():.2f} m\\nMin = {valid_diffs.min():.2f} m\\nMax = {valid_diffs.max():.2f} m\\nRMSE = {ref_metrics['rmse']:.2f} m\\nNMAD = {ref_metrics['nmad']:.2f} m\",\n",
    "        transform=ax.transAxes, fontsize=10, va='top', ha='right',\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor='black'))\n",
    "x_min, x_max = float(valid_diffs.min()), float(valid_diffs.max())\n",
    "x_pad = (x_max - x_min) * 0.05\n",
    "ax.set_xlim(x_min - x_pad, x_max + x_pad)\n",
    "ax.set(xlabel='Elevation Difference (m)', ylabel='Frequency', title='Difference Distribution'); ax.tick_params(colors='black')\n",
    "ax.set_yscale('log'); ax.legend(loc='upper left', fontsize=9)\n",
    "ax.grid(True, alpha=0.3, color='black', linewidth=0.5)\n",
    "for s in ax.spines.values(): s.set_edgecolor('black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 1) Prepare difference data\n",
    "# =============================================================================\n",
    "saocom_gdf['diff_tinitaly']   = saocom_gdf['HEIGHT_ABSOLUTE_TIN'] - saocom_gdf['tinitaly_height']\n",
    "saocom_gdf['diff_copernicus'] = saocom_gdf['HEIGHT_ABSOLUTE_COP'] - saocom_gdf['copernicus_height']\n",
    "\n",
    "if 'coherence_bin' not in saocom_gdf.columns:\n",
    "    cbins = [0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "    clabels = [f\"{cbins[i]:.1f}-{cbins[i+1]:.1f}\" for i in range(len(cbins)-1)]\n",
    "    saocom_gdf['coherence_bin'] = pd.cut(saocom_gdf['COHER'], bins=cbins, labels=clabels, include_lowest=True)\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Basic violin plots (filtered to 1st\u201399th percentile for display)\n",
    "# =============================================================================\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8), facecolor='white')\n",
    "plot_data = pd.DataFrame({'SAOCOM - TINITALY': saocom_gdf['diff_tinitaly'], 'SAOCOM - Copernicus': saocom_gdf['diff_copernicus']})\n",
    "p01, p99 = plot_data.quantile(0.01).min(), plot_data.quantile(0.99).max()\n",
    "filtered = [plot_data[c].dropna().clip(p01, p99) for c in plot_data.columns]\n",
    "\n",
    "parts = ax.violinplot(filtered, positions=[1,2], showmeans=True, showmedians=True, showextrema=True)\n",
    "for pc, col in zip(parts['bodies'], ['#4472C4', '#ED7D31']): pc.set_facecolor(col); pc.set_alpha(0.7); pc.set_edgecolor('black')\n",
    "for k in ('cbars','cmins','cmaxes','cmedians','cmeans'):\n",
    "    if k in parts: parts[k].set_edgecolor('black'); parts[k].set_linewidth(1.5)\n",
    "\n",
    "ax.axhline(0, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='Zero')\n",
    "ax.set_xticks([1,2]); ax.set_xticklabels(['SAOCOM -\\nTINITALY', 'SAOCOM -\\nCopernicus'], fontsize=11)\n",
    "ax.set_ylabel('Elevation Difference (m)', fontsize=12)\n",
    "ax.set_title('Distribution of Elevation Differences (1st\u201399th Percentile)', fontweight='bold', fontsize=14)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.3); ax.legend()\n",
    "\n",
    "for i, col in enumerate(plot_data.columns, 1):\n",
    "    data = plot_data[col].dropna()\n",
    "    ymax = filtered[i-1].max()\n",
    "    ax.text(i, ymax*0.9, f\"n={len(data):,}\\n\u03bc={data.mean():.2f}m\\n\u03c3={data.std():.2f}m\", ha='center', fontsize=12,\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 4) Summary statistics table\n",
    "# =============================================================================\n",
    "summary_df = pd.DataFrame([{\n",
    "    'Comparison': 'SAOCOM - TINITALY' if name=='diff_tinitaly' else 'SAOCOM - Copernicus',\n",
    "    'N Points': f\"{len(saocom_gdf[name].dropna()):,}\",\n",
    "    'Mean': f\"{saocom_gdf[name].mean():+.2f} m\",\n",
    "    'Median': f\"{saocom_gdf[name].median():+.2f} m\",\n",
    "    'Std Dev': f\"{saocom_gdf[name].std():.2f} m\",\n",
    "    'RMSE': f\"{np.sqrt((saocom_gdf[name].dropna()**2).mean()):.2f} m\",\n",
    "    'Range': f\"[{saocom_gdf[name].min():.1f}, {saocom_gdf[name].max():.1f}] m\"\n",
    "} for name in ('diff_tinitaly','diff_copernicus')])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\nVIOLIN PLOT SUMMARY STATISTICS\\n\" + \"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)\n"
   ],
   "id": "3f46f0c83d8ef692",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SAOCOM HEIGHT RESIDUAL OUTLIER DETECTION AND VISUALIZATION",
   "id": "50c4fc0b43a1e4fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def score_outliers_isolation_forest(\n",
    "    gdf: gpd.GeoDataFrame,\n",
    "    residual_col: str,\n",
    "    **kwargs\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Assigns an anomaly score to each point using the Isolation Forest algorithm.\n",
    "    This function is unchanged and remains the core of the detection process.\n",
    "\n",
    "    Args:\n",
    "        gdf (gpd.GeoDataFrame): The input GeoDataFrame.\n",
    "        residual_col (str): The name of the column containing the height residuals.\n",
    "        **kwargs: Additional keyword arguments to pass to the IsolationForest model.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: The original GeoDataFrame with a new 'outlier_score' column.\n",
    "    \"\"\"\n",
    "    if gdf.empty or residual_col not in gdf.columns:\n",
    "        print(\"Warning: Input GeoDataFrame is empty or residual column not found.\")\n",
    "        gdf['outlier_score'] = np.nan\n",
    "        return gdf\n",
    "\n",
    "    points_3d = np.c_[\n",
    "        gdf.geometry.x,\n",
    "        gdf.geometry.y,\n",
    "        gdf[residual_col].fillna(0)\n",
    "    ]\n",
    "    scaler = StandardScaler()\n",
    "    points_3d_scaled = scaler.fit_transform(points_3d)\n",
    "\n",
    "    model_params = {'n_estimators': 100, 'contamination': 'auto', 'random_state': 42, 'n_jobs': -1}\n",
    "    model_params.update(kwargs)\n",
    "\n",
    "    print(\"Fitting Isolation Forest model...\")\n",
    "    model = IsolationForest(**model_params)\n",
    "    model.fit(points_3d_scaled)\n",
    "\n",
    "    gdf_scored = gdf.copy()\n",
    "    gdf_scored['outlier_score'] = model.decision_function(points_3d_scaled)\n",
    "\n",
    "    print(\"Scoring complete. Added 'outlier_score' column.\")\n",
    "    return gdf_scored\n",
    "\n",
    "def filter_by_score_iqr(\n",
    "    gdf_scored: gpd.GeoDataFrame,\n",
    "    iqr_multiplier: float = 1\n",
    ") -> tuple[gpd.GeoDataFrame, gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Filters a scored GeoDataFrame using a statistically-driven threshold.\n",
    "\n",
    "    This function calculates the outlier threshold by applying the IQR method\n",
    "    to the 'outlier_score' column itself. This removes the need for an\n",
    "    arbitrary percentage cutoff.\n",
    "\n",
    "    Args:\n",
    "        gdf_scored (gpd.GeoDataFrame): GeoDataFrame with an 'outlier_score' column.\n",
    "        iqr_multiplier (float): The multiplier for the IQR to define the cutoff.\n",
    "                                A standard value is 1.5.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the cleaned GeoDataFrame and the outlier GeoDataFrame.\n",
    "    \"\"\"\n",
    "    if 'outlier_score' not in gdf_scored.columns:\n",
    "        raise ValueError(\"Input GeoDataFrame must have an 'outlier_score' column.\")\n",
    "\n",
    "    scores = gdf_scored['outlier_score']\n",
    "    q1 = scores.quantile(0.25)\n",
    "    q3 = scores.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # The threshold is statistically determined from the distribution of scores\n",
    "    score_threshold = q1 - (iqr_multiplier * iqr)\n",
    "    print(score_threshold)\n",
    "    is_outlier_mask = scores < score_threshold\n",
    "\n",
    "    outliers = gdf_scored[is_outlier_mask].copy()\n",
    "    gdf_cleaned = gdf_scored[~is_outlier_mask].copy()\n",
    "\n",
    "    total_points, n_outliers = len(gdf_scored), len(outliers)\n",
    "    pct_outliers = (n_outliers / total_points) * 100 if total_points > 0 else 0\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Statistically-Driven Filtering (IQR on Anomaly Scores)\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Score Q1: {q1:.4f}, Q3: {q3:.4f}, IQR: {iqr:.4f}\")\n",
    "    print(f\"IQR Multiplier: {iqr_multiplier}\")\n",
    "    print(f\"Calculated Score Threshold: < {score_threshold:.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total Points: {total_points:,}\")\n",
    "    print(f\"Outliers Removed: {n_outliers:,} ({pct_outliers:.2f}%)\")\n",
    "    print(f\"Remaining Points: {len(gdf_cleaned):,}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return gdf_cleaned, outliers\n",
    "\n",
    "def visualize_outlier_results(\n",
    "    gdf_original: gpd.GeoDataFrame,\n",
    "    gdf_cleaned: gpd.GeoDataFrame,\n",
    "    outliers: gpd.GeoDataFrame,\n",
    "    residual_col: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a two-panel plot to visualize the outlier removal results.\n",
    "    (This function is unchanged).\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 9), facecolor='white', gridspec_kw={'width_ratios': [1.2, 1]})\n",
    "\n",
    "    # --- Panel 1: Spatial Distribution ---\n",
    "    ax1 = axes[0]\n",
    "    vmin, vmax = np.nanpercentile(gdf_cleaned[residual_col], [2, 98])\n",
    "\n",
    "    scatter = ax1.scatter(\n",
    "        gdf_cleaned.geometry.x, gdf_cleaned.geometry.y,\n",
    "        c=gdf_cleaned[residual_col], cmap='RdBu_r', s=5,\n",
    "        vmin=vmin, vmax=vmax, alpha=0.8, label='Cleaned Data'\n",
    "    )\n",
    "    plt.colorbar(scatter, ax=ax1, label=f'Residual ({residual_col}) (m)', shrink=0.7)\n",
    "\n",
    "    if not outliers.empty:\n",
    "        outliers.plot(ax=ax1, markersize=25, color='yellow',\n",
    "                      edgecolors='black', linewidth=0.8,\n",
    "                      label=f'Outliers (n={len(outliers):,})', zorder=5)\n",
    "\n",
    "    ax1.set_title('Spatial Distribution of Cleaned Data and Outliers', fontweight='bold')\n",
    "    ax1.set_xlabel('UTM Easting (m)')\n",
    "    ax1.set_ylabel('UTM Northing (m)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, linestyle='--', alpha=0.4)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.set_aspect('equal', adjustable='box')\n",
    "\n",
    "    # --- Panel 2: Residual Distribution Histogram ---\n",
    "    ax2 = axes[1]\n",
    "    original_residuals = gdf_original[residual_col].dropna()\n",
    "    cleaned_residuals = gdf_cleaned[residual_col].dropna()\n",
    "\n",
    "    ax2.hist(original_residuals, bins=100, alpha=0.5, label=f'Before (n={len(original_residuals):,})', color='gray')\n",
    "    ax2.hist(cleaned_residuals, bins=50, alpha=1, label=f'After (n={len(cleaned_residuals):,})', color='#2E86AB')\n",
    "\n",
    "    ax2.set_title('Residual Distribution Before and After Cleaning', fontweight='bold')\n",
    "    ax2.set_xlabel(f'Residual ({residual_col}) (m)')\n",
    "    ax2.set_ylabel('Frequency (Log Scale)')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(RESULTS_DIR / \"difference_by_coherence.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# # =============================================================================\n",
    "# # EXAMPLE USAGE\n",
    "# # =============================================================================\n",
    "# if __name__ == '__main__':\n",
    "#     # 1. Create a sample GeoDataFrame for demonstration\n",
    "#     print(\"Creating sample GeoDataFrame for demonstration...\")\n",
    "#     np.random.seed(42)\n",
    "#     clean_points = np.random.rand(1000, 2) * 1000\n",
    "#     clean_residuals = np.random.normal(0, 2, 1000)\n",
    "#     outlier_points = np.array([[500, 500], [100, 900], [900, 100], [10, 10]])\n",
    "#     outlier_residuals = np.array([80, -65, 95, 5])\n",
    "#\n",
    "#     all_coords = np.vstack([clean_points, outlier_points])\n",
    "#     all_residuals = np.concatenate([clean_residuals, outlier_residuals])\n",
    "#     sample_gdf = gpd.GeoDataFrame(\n",
    "#         {'diff_tinitaly': all_residuals},\n",
    "#         geometry=[Point(x, y) for x, y in all_coords]\n",
    "#     )\n",
    "\n",
    "# --- Step 1: Score all the points ---\n",
    "print(len(saocom_gdf))\n",
    "saocom_gdf_scored = score_outliers_isolation_forest(saocom_gdf, 'diff_tinitaly')\n",
    "\n",
    "# --- Step 2: Filter using the STATISTICAL method ---\n",
    "# No more guessing a percentage! The function calculates the cutoff itself.\n",
    "saocom_gdf_cleaned, saocom_outliers = filter_by_score_iqr(saocom_gdf_scored)\n",
    "\n",
    "# --- Step 3: Visualize the results ---\n",
    "visualize_outlier_results(saocom_gdf, saocom_gdf_cleaned, saocom_outliers, 'diff_tinitaly')\n",
    "saocom_gdf = saocom_gdf_cleaned\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_height_statistics_summary(gdf, gdf_name=\"SAOCOM Data\"):\n",
    "    \"\"\"\n",
    "    Calculates and prints a comprehensive height statistics summary for a given\n",
    "    SAOCOM GeoDataFrame against the reference DEMs.\n",
    "\n",
    "    Args:\n",
    "        gdf (gpd.GeoDataFrame): The input GeoDataFrame containing SAOCOM points.\n",
    "                                Must include 'HEIGHT_RELATIVE', 'tinitaly_height',\n",
    "                                and 'copernicus_height' columns.\n",
    "        gdf_name (str): The name of the dataset for labeling the output.\n",
    "    \"\"\"\n",
    "    if not all(col in gdf.columns for col in ['HEIGHT_RELATIVE', 'tinitaly_height', 'copernicus_height']):\n",
    "        print(f\"Error: Input GeoDataFrame '{gdf_name}' is missing required height columns.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\"*95)\n",
    "    print(f\" STATISTICAL SUMMARY FOR: {gdf_name.upper()} ({len(gdf)} points)\")\n",
    "    print(\"=\"*95)\n",
    "\n",
    "    # =============================================================================\n",
    "    # HEIGHT STATISTICS COMPARISON\n",
    "    # =============================================================================\n",
    "    stats_list = []\n",
    "\n",
    "    # Function to calculate stats for a data series\n",
    "    def calculate_height_stats(data, name):\n",
    "        valid_data = data.dropna()\n",
    "        if len(valid_data) == 0: return None\n",
    "        return {\n",
    "            'Dataset': name,\n",
    "            'Count': len(valid_data), 'Min': np.min(valid_data),\n",
    "            'Max': np.max(valid_data), 'Mean': np.mean(valid_data),\n",
    "            'Median': np.median(valid_data), 'Std Dev': np.std(valid_data),\n",
    "            'Q25': np.percentile(valid_data, 25), 'Q75': np.percentile(valid_data, 75)\n",
    "        }\n",
    "\n",
    "    # Add stats for the input GDF\n",
    "    stats_list.append(calculate_height_stats(gdf['HEIGHT_RELATIVE'], f'{gdf_name} (Relative)'))\n",
    "    stats_list.append(calculate_height_stats(gdf['tinitaly_height'], f'TINITALY (at {gdf_name} pts)'))\n",
    "    stats_list.append(calculate_height_stats(gdf['copernicus_height'], f'Copernicus (at {gdf_name} pts)'))\n",
    "\n",
    "    # Create and display DataFrame\n",
    "    stats_df = pd.DataFrame([s for s in stats_list if s is not None])\n",
    "    print(\"\\nHEIGHT STATISTICS SUMMARY (all values in meters)\")\n",
    "    print(\"-\"*95)\n",
    "    print(stats_df.to_string(index=False, float_format=lambda x: f'{x:.2f}'))\n",
    "    print(\"-\"*95)\n",
    "\n",
    "\n",
    "    # =============================================================================\n",
    "    # DIFFERENCE STATISTICS (SAOCOM - REFERENCE)\n",
    "    # =============================================================================\n",
    "    print(\"\\nDIFFERENCE STATISTICS (SAOCOM Relative - Reference DEM):\")\n",
    "    print(\"-\"*95)\n",
    "\n",
    "    # SAOCOM - TINITALY\n",
    "    diff_tin_valid = (gdf['HEIGHT_RELATIVE'] - gdf['tinitaly_height']).dropna()\n",
    "    print(f\"\\n{gdf_name} - TINITALY:\")\n",
    "    print(f\"  Mean difference: {diff_tin_valid.mean():+.3f} m\")\n",
    "    print(f\"  Median difference: {diff_tin_valid.median():+.3f} m\")\n",
    "    print(f\"  Std deviation: {diff_tin_valid.std():.3f} m\")\n",
    "    print(f\"  RMSE: {np.sqrt((diff_tin_valid**2).mean()):.3f} m\")\n",
    "\n",
    "    # SAOCOM - Copernicus\n",
    "    diff_cop_valid = (gdf['HEIGHT_RELATIVE'] - gdf['copernicus_height']).dropna()\n",
    "    print(f\"\\n{gdf_name} - Copernicus:\")\n",
    "    print(f\"  Mean difference: {diff_cop_valid.mean():+.3f} m\")\n",
    "    print(f\"  Median difference: {diff_cop_valid.median():+.3f} m\")\n",
    "    print(f\"  Std deviation: {diff_cop_valid.std():.3f} m\")\n",
    "    print(f\"  RMSE: {np.sqrt((diff_cop_valid**2).mean()):.3f} m\")\n",
    "    print(\"=\"*95)\n",
    "\n",
    "# Run the summary for the main cleaned dataframe\n",
    "generate_height_statistics_summary(saocom_gdf_cleaned, gdf_name=\"SAOCOM Cleaned\")\n",
    "\n",
    "# Run the summary for the outliers that were removed\n",
    "generate_height_statistics_summary(saocom_outliers, gdf_name=\"SAOCOM Outliers\")\n",
    "\n",
    "# You can also run it on the original, unfiltered dataframe for comparison\n",
    "# generate_height_statistics_summary(saocom_gdf, gdf_name=\"SAOCOM Original\")\n"
   ],
   "id": "a3428b4a90d0d43a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. SPATIAL SAMPLE CORINE LAND COVER AT SAOCOM POINTS",
   "id": "b82cd7f2ce2fd9f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# PRE-REQUISITE: CALCULATE HEIGHT RESIDUALS\n",
    "# (This step was missing/unexecuted from the previous cell sequence)\n",
    "# =============================================================================\n",
    "# Residual = Calibrated SAOCOM Height - Reference DEM Height\n",
    "saocom_gdf['diff_tinitaly'] = saocom_gdf['HEIGHT_ABSOLUTE_TIN'] - saocom_gdf['tinitaly_height']\n",
    "# saocom_gdf['diff_copernicus'] = saocom_gdf['HEIGHT_ABSOLUTE_COP'] - saocom_gdf['copernicus_height'] # Not needed for this table\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SPATIAL SAMPLE CORINE LAND COVER AT SAOCOM POINTS\n",
    "# =============================================================================\n",
    "# The corine_10m raster is already loaded, reprojected, and masked (Cell 202).\n",
    "\n",
    "corine_codes = []\n",
    "# Assuming the global variables target_transform, grid_height, grid_width, corine_10m, and NODATA are defined in previous cells\n",
    "for idx, row in saocom_gdf.iterrows():\n",
    "    # Use the same transform and grid dimensions as defined in Cell 200\n",
    "    row_idx, col_idx = rowcol(target_transform, row.geometry.x, row.geometry.y)\n",
    "\n",
    "    if 0 <= row_idx < grid_height and 0 <= col_idx < grid_width:\n",
    "        code = corine_10m[row_idx, col_idx]\n",
    "        # Skip NoData/Masked value (255)\n",
    "        corine_codes.append(code if code != 255 else 0)\n",
    "    else:\n",
    "        corine_codes.append(0) # 0 represents NoData/outside study area\n",
    "\n",
    "saocom_gdf['corine_code'] = corine_codes\n",
    "\n",
    "# Filter out points outside the valid CORINE area (where code is 0)\n",
    "saocom_lc_analysis = saocom_gdf[saocom_gdf['corine_code'] != 0].copy()\n",
    "\n",
    "# =============================================================================\n",
    "# 2. CALCULATE ROBUST STATISTICS BY LAND COVER CLASS\n",
    "# =============================================================================\n",
    "\n",
    "def nmad(data):\n",
    "    \"\"\"Normalized Median Absolute Deviation (robust measure of spread)\"\"\"\n",
    "    return 1.4826 * np.median(np.abs(data - np.median(data)))\n",
    "\n",
    "# Calculate stats for the recommended residual: SAOCOM (calibrated to TINITALY) - TINITALY DEM\n",
    "lc_height_stats = saocom_lc_analysis.groupby('corine_code')['diff_tinitaly'].agg([\n",
    "    'count',\n",
    "    'median',\n",
    "    'mean',\n",
    "    'std',\n",
    "    nmad\n",
    "]).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "lc_height_stats.rename(columns={\n",
    "    'count': 'N_Points',\n",
    "    'median': 'Median_Diff_m',\n",
    "    'mean': 'Mean_Diff_m',\n",
    "    'std': 'Std_Dev_m',\n",
    "    'nmad': 'NMAD_m'\n",
    "}, inplace=True)\n",
    "\n",
    "# Add land cover label for interpretability\n",
    "lc_height_stats['LC_Label'] = lc_height_stats['corine_code'].map(CORINE_CLASSES)\n",
    "\n",
    "# Reorder and filter for classes with enough samples (N > 50)\n",
    "MIN_SAMPLES = 50\n",
    "lc_height_stats_filtered = lc_height_stats[lc_height_stats['N_Points'] >= MIN_SAMPLES]\n",
    "lc_height_stats_filtered = lc_height_stats_filtered.sort_values('LC_Label', ascending=True)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. DISPLAY RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"HEIGHT RESIDUAL STATISTICS by CORINE Land Cover (N > {MIN_SAMPLES})\")\n",
    "print(\"(Residual = Calibrated SAOCOM Height - TINITALY Reference DEM)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "display_cols = ['corine_code', 'LC_Label', 'N_Points', 'Median_Diff_m', 'NMAD_m', 'Mean_Diff_m', 'Std_Dev_m']\n",
    "\n",
    "# Print the filtered, robustly sorted table\n",
    "print(lc_height_stats_filtered[display_cols].to_string(\n",
    "    index=False,\n",
    "    float_format=lambda x: f'{x:+.2f}' if 'Diff' in lc_height_stats_filtered.columns.tolist() else f'{x:.2f}', # Generic float formatting\n",
    "    formatters={'N_Points': '{:,}'.format,\n",
    "                'Median_Diff_m': '{:+.2f} m'.format,\n",
    "                'NMAD_m': '{:.2f} m'.format,\n",
    "                'Mean_Diff_m': '{:+.2f} m'.format,\n",
    "                'Std_Dev_m': '{:.2f} m'.format}\n",
    "))\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Store the filtered results for later use in plotting/reporting\n",
    "lc_height_stats_final = lc_height_stats_filtered.copy()\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SETUP AND DEFINITIONS (from previous cells)\n",
    "# =============================================================================\n",
    "# Global variables assumed defined:\n",
    "# corine_10m (masked CLC raster), study_area_mask (valid area inside hull),\n",
    "# saocom_coverage (boolean array: True where SAOCOM data exists),\n",
    "# void_mask (study_area_mask & ~saocom_coverage), GRID_SIZE (10m)\n",
    "# CORINE_CLASSES (LC code lookup)\n",
    "\n",
    "# Recalculate global void stats for context\n",
    "n_total_cells = np.sum(study_area_mask)\n",
    "n_void_cells = np.sum(void_mask)\n",
    "void_percentage_global = 100 * n_void_cells / n_total_cells if n_total_cells > 0 else 0\n",
    "\n",
    "# Get unique, valid CORINE codes within the study area\n",
    "unique_lc_codes = np.unique(corine_10m[study_area_mask])\n",
    "unique_lc_codes = unique_lc_codes[unique_lc_codes != 0] # Filter out 0/NoData\n",
    "\n",
    "# =============================================================================\n",
    "# 2. VOID ANALYSIS BY LAND COVER CLASS\n",
    "# =============================================================================\n",
    "void_stats_by_lc = []\n",
    "cell_area_km2 = (GRID_SIZE / 1000.0) ** 2 # 0.0001 km^2\n",
    "\n",
    "for lc_code in unique_lc_codes:\n",
    "    # Mask for this land cover class within the study area\n",
    "    lc_mask = study_area_mask & (corine_10m == lc_code)\n",
    "\n",
    "    # Total cells of this land cover\n",
    "    total_lc_cells = np.sum(lc_mask)\n",
    "\n",
    "    if total_lc_cells == 0:\n",
    "        continue\n",
    "\n",
    "    # Void cells within this land cover\n",
    "    void_lc_cells = np.sum(lc_mask & void_mask)\n",
    "\n",
    "    # METRIC 1: What % of this land cover is void? (Key Metric for coverage performance)\n",
    "    pct_of_lc_that_is_void = 100 * void_lc_cells / total_lc_cells\n",
    "\n",
    "    # METRIC 2: What % of total voids is this land cover? (Key Metric for contribution)\n",
    "    pct_of_total_voids = 100 * void_lc_cells / n_void_cells if n_void_cells > 0 else 0\n",
    "\n",
    "    void_stats_by_lc.append({\n",
    "        'corine_code': lc_code,\n",
    "        'label': CORINE_CLASSES.get(lc_code, f'Unknown_{lc_code}'),\n",
    "        'total_cells': total_lc_cells,\n",
    "        'void_cells': void_lc_cells,\n",
    "        'Area_km2': total_lc_cells * cell_area_km2,\n",
    "        'Pct_LC_is_Void': pct_of_lc_that_is_void,\n",
    "        'Pct_of_Total_Voids': pct_of_total_voids,\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "void_stats_df = pd.DataFrame(void_stats_by_lc)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. DISPLAY RESULTS\n",
    "# =============================================================================\n",
    "# Filter for significant land cover classes (e.g., > 1 km2 area)\n",
    "MIN_AREA_KM2 = 1.0\n",
    "void_stats_filtered = void_stats_df[void_stats_df['Area_km2'] >= MIN_AREA_KM2].copy()\n",
    "\n",
    "# Sort by the primary metric: Percentage of the LC class that is void\n",
    "void_stats_filtered = void_stats_filtered.sort_values('Pct_LC_is_Void', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(f\"VOID ANALYSIS by CORINE Land Cover (Area > {MIN_AREA_KM2:.1f} km\u00b2)\")\n",
    "print(f\"Overall Void Percentage (Study Area): {void_percentage_global:.2f}%\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "display_cols = ['corine_code', 'label', 'Area_km2', 'void_cells', 'Pct_LC_is_Void', 'Pct_of_Total_Voids']\n",
    "\n",
    "# Print the filtered table\n",
    "print(void_stats_filtered[display_cols].to_string(\n",
    "    index=False,\n",
    "    float_format=lambda x: f'{x:.2f}',\n",
    "    formatters={'Area_km2': '{:.1f} km\u00b2'.format,\n",
    "                'void_cells': '{:,}'.format,\n",
    "                'Pct_LC_is_Void': '{:.2f} %'.format,\n",
    "                'Pct_of_Total_Voids': '{:.2f} %'.format}\n",
    "))\n",
    "\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Store the filtered results for later use in plotting/reporting\n",
    "lc_void_stats_final = void_stats_filtered.copy()"
   ],
   "id": "706690cf955e34aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  1. PREPARE DATA FOR PLOTTING",
   "id": "e4a7facf951d0d07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. PREPARE DATA FOR PLOTTING\n",
    "# =============================================================================\n",
    "# Use the full saocom_lc_analysis DataFrame which contains both the residuals\n",
    "# ('diff_tinitaly') and the sampled land cover codes ('corine_code').\n",
    "\n",
    "# Define major land cover groups for better visualization (CLC Level 1)\n",
    "def get_clc_level1(code):\n",
    "    \"\"\"Maps CLC Level 3 code to Level 1 category\"\"\"\n",
    "    if 100 <= code < 200: return '1. Artificial Surfaces'\n",
    "    if 200 <= code < 300: return '2. Agricultural Areas'\n",
    "    if 300 <= code < 400: return '3. Forest & Semi-Natural Areas'\n",
    "    if 400 <= code < 500: return '4. Wetlands'\n",
    "    if 500 <= code < 600: return '5. Water Bodies'\n",
    "    return 'Other'\n",
    "\n",
    "# Add Level 1 categories to the analysis DataFrame\n",
    "saocom_lc_analysis['LC_Level_1'] = saocom_lc_analysis['corine_code'].apply(get_clc_level1)\n",
    "saocom_lc_analysis['LC_Label'] = saocom_lc_analysis['corine_code'].map(CORINE_CLASSES)\n",
    "\n",
    "# Filter for the most common Level 3 classes (using the N_Points filter from Step 1)\n",
    "common_codes = lc_height_stats_final['corine_code'].unique()\n",
    "plot_df_L3 = saocom_lc_analysis[saocom_lc_analysis['corine_code'].isin(common_codes)].copy()\n",
    "\n",
    "# Filter extreme outliers for better plot scaling (e.g., 99th percentile)\n",
    "q_low = plot_df_L3['diff_tinitaly'].quantile(0.005)\n",
    "q_high = plot_df_L3['diff_tinitaly'].quantile(0.995)\n",
    "plot_df_L3_filtered = plot_df_L3[(plot_df_L3['diff_tinitaly'] >= q_low) &\n",
    "                                (plot_df_L3['diff_tinitaly'] <= q_high)]\n",
    "\n",
    "# Sort the categories by the NMAD metric (best to worst performance)\n",
    "nmad_order = lc_height_stats_final.sort_values('LC_Label', ascending=False)['LC_Label'].tolist()\n",
    "plot_df_L3_filtered['LC_Label'] = pd.Categorical(\n",
    "    plot_df_L3_filtered['LC_Label'],\n",
    "    categories=nmad_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "q_low = plot_df_L3['diff_copernicus'].quantile(0.005)\n",
    "q_high = plot_df_L3['diff_copernicus'].quantile(0.995)\n",
    "plot_df_cop_filtered = plot_df_L3[(plot_df_L3['diff_copernicus'] >= q_low) &\n",
    "                                (plot_df_L3['diff_copernicus'] <= q_high)]\n",
    "\n",
    "# Sort the categories by the NMAD metric (best to worst performance)\n",
    "nmad_order = lc_height_stats_final.sort_values('LC_Label', ascending=False)['LC_Label'].tolist()\n",
    "plot_df_cop_filtered['LC_Label'] = pd.Categorical(\n",
    "    plot_df_cop_filtered['LC_Label'],\n",
    "    categories=nmad_order,\n",
    "    ordered=True\n",
    ")\n"
   ],
   "id": "1d88b2f34cd41e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SENTINEL-2 RGB PREPARATION",
   "id": "f692fa9c5f4dc233"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# SENTINEL-2 RGB PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "# File discovery\n",
    "sentinel_files = list((DATA_DIR / \"sentinel_data\").glob(\"*.tif\"))\n",
    "if not sentinel_files:\n",
    "    raise FileNotFoundError(\"No Sentinel files found in sentinel_data directory\")\n",
    "\n",
    "# Load Sentinel bands (assuming separate R, G, B files or multi-band)\n",
    "with rasterio.open(sentinel_files[0]) as src:\n",
    "    sentinel_count = src.count\n",
    "\n",
    "    if sentinel_count >= 3:\n",
    "        # Multi-band file - read RGB bands\n",
    "        sentinel_r = src.read(1)  # Band 1 (Red)\n",
    "        sentinel_g = src.read(2)  # Band 2 (Green)\n",
    "        sentinel_b = src.read(3)  # Band 3 (Blue)\n",
    "        sentinel_transform_orig = src.transform\n",
    "        sentinel_crs = src.crs\n",
    "    else:\n",
    "        # Single band files - need to find R, G, B separately\n",
    "        r_file = next((f for f in sentinel_files if 'B04' in f.name or 'red' in f.name.lower()), None)\n",
    "        g_file = next((f for f in sentinel_files if 'B03' in f.name or 'green' in f.name.lower()), None)\n",
    "        b_file = next((f for f in sentinel_files if 'B02' in f.name or 'blue' in f.name.lower()), None)\n",
    "\n",
    "        if not all([r_file, g_file, b_file]):\n",
    "            raise FileNotFoundError(\"Could not find RGB bands in Sentinel files\")\n",
    "\n",
    "        with rasterio.open(r_file) as r_src:\n",
    "            sentinel_r = r_src.read(1)\n",
    "            sentinel_transform_orig = r_src.transform\n",
    "            sentinel_crs = r_src.crs\n",
    "        with rasterio.open(g_file) as g_src:\n",
    "            sentinel_g = g_src.read(1)\n",
    "        with rasterio.open(b_file) as b_src:\n",
    "            sentinel_b = b_src.read(1)\n",
    "\n",
    "# Resample each band to 10m grid\n",
    "sentinel_r_10m = np.zeros((grid_height, grid_width), dtype=np.float32)\n",
    "sentinel_g_10m = np.zeros((grid_height, grid_width), dtype=np.float32)\n",
    "sentinel_b_10m = np.zeros((grid_height, grid_width), dtype=np.float32)\n",
    "\n",
    "for band_src, band_dst in [(sentinel_r, sentinel_r_10m),\n",
    "                            (sentinel_g, sentinel_g_10m),\n",
    "                            (sentinel_b, sentinel_b_10m)]:\n",
    "    reproject(\n",
    "        source=band_src,\n",
    "        destination=band_dst,\n",
    "        src_transform=sentinel_transform_orig,\n",
    "        src_crs=sentinel_crs,\n",
    "        dst_transform=target_transform,\n",
    "        dst_crs=TARGET_CRS,\n",
    "        resampling=Resampling.bilinear\n",
    "    )\n",
    "\n",
    "# Stack into RGB array\n",
    "sentinel_rgb = np.stack([sentinel_r_10m, sentinel_g_10m, sentinel_b_10m], axis=2)\n",
    "\n",
    "# Mask to study area\n",
    "sentinel_rgb[~hull_mask] = 0\n",
    "\n",
    "# Normalize to 0-1 for display (using 2-98 percentile stretch for contrast)\n",
    "sentinel_rgb_norm = np.zeros_like(sentinel_rgb, dtype=np.float32)\n",
    "for i in range(3):\n",
    "    band = sentinel_rgb[:, :, i]\n",
    "    valid_pixels = band[hull_mask]\n",
    "\n",
    "    if len(valid_pixels) > 0:\n",
    "        p2, p98 = np.percentile(valid_pixels[valid_pixels > 0], [2, 98])\n",
    "        band_norm = np.clip((band - p2) / (p98 - p2), 0, 1)\n",
    "        sentinel_rgb_norm[:, :, i] = band_norm\n",
    "\n",
    "print(f\"\\nSentinel-2 RGB Prepared:\")\n",
    "print(f\"  Shape: {sentinel_rgb_norm.shape}\")\n",
    "print(f\"  Resolution: {GRID_SIZE}m\")\n",
    "print(f\"  Value range: [{sentinel_rgb_norm.min():.3f}, {sentinel_rgb_norm.max():.3f}]\")"
   ],
   "id": "c603c5472a10e4a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. GENERATE VIOLIN PLOT (Level 3 - Detailed Performance)",
   "id": "9fed879e4544c721"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# 1. GENERATE STATS WITHIN THE PLOT SCRIPT\n",
    "# =============================================================================\n",
    "print(\"Generating statistics just-in-time for the plot...\")\n",
    "\n",
    "def calculate_nmad(series):\n",
    "    \"\"\"Calculates the Normalized Median Absolute Deviation (NMAD).\"\"\"\n",
    "    return (series - series.median()).abs().median() * 1.4826\n",
    "\n",
    "# Create a new, temporary stats DataFrame by grouping the plotting data\n",
    "stats_for_plot = plot_df_L3_filtered.groupby('LC_Label')['diff_tinitaly'].agg(\n",
    "    Median_Diff_m='median',\n",
    "    NMAD_m=calculate_nmad,\n",
    "    Std_Dev_m='std',\n",
    "    Min_Diff_m='min',\n",
    "    Max_Diff_m='max'\n",
    ").reset_index()\n",
    "\n",
    "# Determine the plotting order based on the NMAD we just calculated\n",
    "nmad_order = stats_for_plot.sort_values('NMAD_m')['LC_Label'].tolist()\n",
    "\n",
    "# Define an anchor point for the text annotations\n",
    "q_high = plot_df_L3_filtered['diff_tinitaly'].quantile(0.99) + 5\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. GENERATE VIOLIN PLOT (Level 3 - Detailed Performance)\n",
    "# =============================================================================\n",
    "plt.figure(figsize=(16, 9), facecolor='white')\n",
    "\n",
    "# Use violin plot, ordering it with the 'nmad_order' list we just created\n",
    "sns.violinplot(\n",
    "    x='diff_tinitaly',\n",
    "    y='LC_Label',\n",
    "    data=plot_df_L3_filtered,\n",
    "    order=nmad_order,  # Use the calculated order here\n",
    "    inner='quartile',\n",
    "    palette='Spectral_r',\n",
    "    orient='h',\n",
    "    linewidth=1.0,\n",
    "    cut=0\n",
    ")\n",
    "\n",
    "# Add a vertical line at zero error\n",
    "plt.axvline(0, color='red', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "# Set labels and title\n",
    "plt.title('Distribution of SAOCOM Height Residuals by Land Cover (CLC Level 3)', fontweight='bold', fontsize=18)\n",
    "plt.xlabel('Height Residual (Calibrated SAOCOM - TINITALY DEM) [m]', fontsize=14)\n",
    "plt.ylabel('CORINE Land Cover Class (Ordered by NMAD)', fontsize=14)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# --- Add Annotations using the stats we just generated ---\n",
    "for i, label in enumerate(nmad_order):\n",
    "    # Query the 'stats_for_plot' DataFrame we created above\n",
    "    stats = stats_for_plot.query(\"LC_Label == @label\").iloc[0]\n",
    "\n",
    "    stats_text = (\n",
    "        f\"Median: {stats['Median_Diff_m']:+.2f} m\\n\"\n",
    "        f\"NMAD: {stats['NMAD_m']:.2f} m\\n\"\n",
    "        f\"Std Dev: {stats['Std_Dev_m']:.2f} m\\n\"\n",
    "        f\"Min/Max: [{stats['Min_Diff_m']:.1f}, {stats['Max_Diff_m']:.1f}] m\"\n",
    "    )\n",
    "\n",
    "    plt.text(q_high, i, stats_text,\n",
    "             verticalalignment='center',\n",
    "             horizontalalignment='left',\n",
    "             fontsize=10,\n",
    "             bbox=dict(boxstyle='round,pad=0.4', facecolor='white', alpha=0.8)\n",
    "            )\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.8, 1])\n",
    "plt.show()\n",
    "print(\"Violin Plot for Level 3 Land Cover classes generated successfully.\")\n",
    "# Add this line to save the figure\n",
    "plt.savefig(RESULTS_DIR / 'saocom_tinitaly_residuals_by_landcover.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# COPERNICUS STATISTICS AND VIOLIN PLOT (Level 3)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Generating statistics for Copernicus plot...\")\n",
    "\n",
    "def calculate_nmad(series):\n",
    "    \"\"\"Calculates the Normalized Median Absolute Deviation (NMAD).\"\"\"\n",
    "    return (series - series.median()).abs().median() * 1.4826\n",
    "\n",
    "# Create a new stats DataFrame by grouping the filtered Copernicus plotting data\n",
    "stats_for_cop_plot = plot_df_cop_filtered.groupby('LC_Label')['diff_copernicus'].agg(\n",
    "    Median_Diff_m='median',\n",
    "    NMAD_m=calculate_nmad,\n",
    "    Std_Dev_m='std',\n",
    "    Min_Diff_m='min',\n",
    "    Max_Diff_m='max'\n",
    ").reset_index()\n",
    "\n",
    "# Determine the plotting order based on the NMAD we just calculated\n",
    "nmad_order_cop = stats_for_cop_plot.sort_values('NMAD_m')['LC_Label'].tolist()\n",
    "\n",
    "# Define an anchor point for the text annotations\n",
    "q_high_cop = plot_df_cop_filtered['diff_copernicus'].quantile(0.995) + 5\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. GENERATE COPERNICUS VIOLIN PLOT (Level 3)\n",
    "# =============================================================================\n",
    "plt.figure(figsize=(16, 9), facecolor='white')\n",
    "\n",
    "# Generate the violin plot, ordering it with the 'nmad_order_cop' list\n",
    "sns.violinplot(\n",
    "    x='diff_copernicus',\n",
    "    y='LC_Label',\n",
    "    data=plot_df_cop_filtered,\n",
    "    order=nmad_order_cop,  # Use the calculated order here\n",
    "    inner='quartile',\n",
    "    palette='Spectral_r',\n",
    "    orient='h',\n",
    "    linewidth=1.0,\n",
    "    cut=0\n",
    ")\n",
    "\n",
    "# Add a vertical line at zero error\n",
    "plt.axvline(0, color='red', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "# Set labels and title\n",
    "plt.title('Distribution of SAOCOM Height Residuals by Land Cover (vs. Copernicus)', fontweight='bold', fontsize=18)\n",
    "plt.xlabel('Height Residual (Calibrated SAOCOM - Copernicus DEM) [m]', fontsize=14)\n",
    "plt.ylabel('CORINE Land Cover Class (Ordered by NMAD)', fontsize=14)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# --- Add Annotations using the stats we just generated ---\n",
    "for i, label in enumerate(nmad_order_cop):\n",
    "    # Query the 'stats_for_cop_plot' DataFrame\n",
    "    stats = stats_for_cop_plot.query(\"LC_Label == @label\").iloc[0]\n",
    "\n",
    "    stats_text = (\n",
    "        f\"Median: {stats['Median_Diff_m']:+.2f} m\\n\"\n",
    "        f\"NMAD: {stats['NMAD_m']:.2f} m\\n\"\n",
    "        f\"Std Dev: {stats['Std_Dev_m']:.2f} m\\n\"\n",
    "        f\"Min/Max: [{stats['Min_Diff_m']:.1f}, {stats['Max_Diff_m']:.1f}] m\"\n",
    "    )\n",
    "\n",
    "    plt.text(q_high_cop, i, stats_text,\n",
    "             verticalalignment='center',\n",
    "             horizontalalignment='left',\n",
    "             fontsize=10,\n",
    "             bbox=dict(boxstyle='round,pad=0.4', facecolor='white', alpha=0.8)\n",
    "            )\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.8, 1])\n",
    "plt.show()\n",
    "print(\"Violin Plot for Copernicus comparison generated successfully.\")\n",
    "# Add this line to save the figure\n",
    "plt.savefig(RESULTS_DIR / 'saocom_copernicus_residuals_by_landcover.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "# =============================================================================\n",
    "# 3. GENERATE BOX PLOT (Level 1 - Broad Comparison)\n",
    "# =============================================================================\n",
    "plt.figure(figsize=(10, 6), facecolor='white')\n",
    "\n",
    "# Use box plot for a cleaner Level 1 aggregation\n",
    "sns.boxplot(\n",
    "    x='LC_Level_1',\n",
    "    y='diff_tinitaly',\n",
    "    data=plot_df_L3_filtered,\n",
    "    palette='Set2',\n",
    "    linewidth=1.0,\n",
    "    showfliers=False # Do not show outliers already filtered\n",
    ")\n",
    "\n",
    "# Add a horizontal line at zero error\n",
    "plt.axhline(0, color='red', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "# Set labels and title\n",
    "plt.title(\n",
    "    'SAOCOM Height Residuals by Land Cover (CLC Level 1)',\n",
    "    fontweight='bold',\n",
    "    fontsize=14\n",
    ")\n",
    "plt.xlabel('CORINE Land Cover Category (Level 1)', fontsize=11)\n",
    "plt.ylabel('Height Residual (m)', fontsize=11)\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Box Plot for Level 1 Land Cover categories generated successfully.\")\n",
    "\n",
    "# =============================================================================\n",
    "# CORINE LAND COVER VISUALIZATION\n",
    "# =============================================================================\n",
    "fig, ax = plt.subplots(figsize=(14, 10), facecolor='white')\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "# Get extent\n",
    "extent = [xmin_grid, xmax_grid, ymin_grid, ymax_grid]\n",
    "\n",
    "# Mask NoData/zero values for transparency\n",
    "corine_display = np.ma.masked_where((corine_10m == 0) | (corine_10m == 255), corine_10m)\n",
    "\n",
    "# Get unique classes in study area\n",
    "unique_codes = np.unique(corine_display.compressed())\n",
    "\n",
    "# Create colormap for present classes only\n",
    "colors_list = [CORINE_COLORS_MPL.get(code, (0.5, 0.5, 0.5)) for code in unique_codes]\n",
    "cmap = ListedColormap(colors_list)\n",
    "norm = BoundaryNorm(boundaries=np.append(unique_codes, unique_codes[-1]+1) - 0.5,\n",
    "                    ncolors=len(unique_codes))\n",
    "\n",
    "# Plot CORINE\n",
    "im = ax.imshow(corine_display, cmap=cmap, norm=norm, origin='upper', extent=extent)\n",
    "\n",
    "# Add study area boundary\n",
    "hull_gdf.boundary.plot(ax=ax, color='black', linewidth=2, label='Study Area')\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('UTM Easting (m)', fontsize=14, color='black')\n",
    "ax.set_ylabel('UTM Northing (m)', fontsize=14, color='black')\n",
    "ax.set_title('CORINE Land Cover 2018', fontweight='bold', fontsize=18, color='black')\n",
    "ax.tick_params(colors='black', labelsize=12)\n",
    "ax.grid(True, alpha=0.3, linewidth=0.5, color='black')\n",
    "\n",
    "# Create legend with only present classes\n",
    "legend_elements = [plt.Rectangle((0,0),1,1, facecolor=CORINE_COLORS_MPL[code],\n",
    "                                 edgecolor='black', linewidth=0.5,\n",
    "                                 label=f\"{code}: {CORINE_CLASSES.get(code, 'Unknown')}\")\n",
    "                   for code in sorted(unique_codes)]\n",
    "\n",
    "ax.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5),\n",
    "          fontsize=13, frameon=True, fancybox=False, edgecolor='black')\n",
    "\n",
    "# Add scale bar\n",
    "scalebar = ScaleBar(1, location='lower right', box_alpha=0.8, color='black')\n",
    "ax.add_artist(scalebar)\n",
    "\n",
    "# Add statistics box\n",
    "total_area_km2 = np.sum(study_area_mask) * 0.0001\n",
    "stats_text = f\"Study Area: {total_area_km2:.2f} km\u00b2\\nClasses: {len(unique_codes)}\"\n",
    "ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=12,\n",
    "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white',\n",
    "        alpha=0.9, edgecolor='black'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCORINE Land Cover Map:\")\n",
    "print(f\"  Total classes present: {len(unique_codes)}\")\n",
    "print(f\"  Study area: {total_area_km2:.2f} km\u00b2\")\n",
    "## =============================================================================\n",
    "# SAOCOM HEIGHT RESIDUALS - INTERPOLATED HEAT MAPS\n",
    "# =============================================================================\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(22, 10), facecolor='white')\n",
    "\n",
    "extent = [xmin_grid, xmax_grid, ymin_grid, ymax_grid]\n",
    "\n",
    "# Filter valid points\n",
    "valid_tin = saocom_gdf[saocom_gdf['diff_tinitaly'].notna()].copy()\n",
    "valid_cop = saocom_gdf[saocom_gdf['diff_copernicus'].notna()].copy()\n",
    "\n",
    "# Calculate symmetric color limits (95th percentile)\n",
    "tin_limit = np.percentile(np.abs(valid_tin['diff_tinitaly']), 95)\n",
    "cop_limit = np.percentile(np.abs(valid_cop['diff_copernicus']), 95)\n",
    "common_limit = max(tin_limit, cop_limit)\n",
    "\n",
    "# Create interpolation grid (matching the 10m grid)\n",
    "xi = np.linspace(xmin_grid, xmax_grid, grid_width)\n",
    "yi = np.linspace(ymax_grid, ymin_grid, grid_height)\n",
    "xi_grid, yi_grid = np.meshgrid(xi, yi)\n",
    "\n",
    "# =============================================================================\n",
    "# Plot 1: SAOCOM - TINITALY Heat Map\n",
    "# =============================================================================\n",
    "ax = axes[0]\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "# Background: Sentinel RGB (very faint)\n",
    "ax.imshow(sentinel_rgb_norm, extent=extent, origin='upper', alpha=0.2)\n",
    "\n",
    "# Extract coordinates and values\n",
    "x_tin = valid_tin.geometry.x.values\n",
    "y_tin = valid_tin.geometry.y.values\n",
    "z_tin = valid_tin['diff_tinitaly'].values\n",
    "\n",
    "# Interpolate to grid using cubic method\n",
    "print(\"Interpolating TINITALY differences...\")\n",
    "zi_tin = griddata((x_tin, y_tin), z_tin, (xi_grid, yi_grid),\n",
    "                  method='cubic', fill_value=np.nan)\n",
    "\n",
    "# Apply Gaussian smoothing for smoother heat map\n",
    "zi_tin_smooth = gaussian_filter(np.nan_to_num(zi_tin, nan=0), sigma=2)\n",
    "zi_tin_smooth[~hull_mask] = np.nan  # Mask to study area\n",
    "\n",
    "# Plot heat map\n",
    "im1 = ax.imshow(zi_tin_smooth, extent=extent, origin='upper',\n",
    "                cmap='RdBu_r', alpha=0.8,\n",
    "                vmin=-common_limit, vmax=common_limit,\n",
    "                interpolation='bilinear')\n",
    "\n",
    "# Overlay original points (small, for reference)\n",
    "ax.scatter(x_tin, y_tin, c=z_tin, cmap='RdBu_r',\n",
    "           s=0.5, alpha=0.3, edgecolors='none',\n",
    "           vmin=-common_limit, vmax=common_limit)\n",
    "\n",
    "# Study area boundary\n",
    "hull_gdf.boundary.plot(ax=ax, color='black', linewidth=2.5, linestyle='--')\n",
    "\n",
    "# Colorbar\n",
    "cbar1 = plt.colorbar(im1, ax=ax, label='Height Difference (m)',\n",
    "                     shrink=0.8, pad=0.02)\n",
    "cbar1.ax.tick_params(labelsize=10, colors='black')\n",
    "cbar1.ax.yaxis.label.set_color('black')\n",
    "\n",
    "ax.set_xlabel('UTM Easting (m)', fontsize=12, color='black')\n",
    "ax.set_ylabel('UTM Northing (m)', fontsize=12, color='black')\n",
    "ax.set_title('SAOCOM - TINITALY\\nInterpolated Height Residual Heat Map',\n",
    "             fontweight='bold', fontsize=14, color='black')\n",
    "ax.grid(True, alpha=0.3, color='black', linewidth=0.5)\n",
    "ax.tick_params(colors='black')\n",
    "\n",
    "# Statistics box\n",
    "stats_text = f\"\"\"Points: {len(valid_tin):,}\n",
    "Mean: {valid_tin['diff_tinitaly'].mean():+.2f} m\n",
    "Median: {valid_tin['diff_tinitaly'].median():+.2f} m\n",
    "RMSE: {np.sqrt((valid_tin['diff_tinitaly']**2).mean()):.2f} m\n",
    "NMAD: {1.4826 * np.median(np.abs(valid_tin['diff_tinitaly'] - valid_tin['diff_tinitaly'].median())):.2f} m\n",
    "Std: {valid_tin['diff_tinitaly'].std():.2f} m\n",
    "\n",
    "Interpolation: Cubic + Gaussian\n",
    "Color Scale: \u00b1{common_limit:.1f} m\n",
    "Red = SAOCOM Higher\n",
    "Blue = TINITALY Higher\"\"\"\n",
    "\n",
    "ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=9,\n",
    "        verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.9,\n",
    "                 edgecolor='black'))\n",
    "\n",
    "# =============================================================================\n",
    "# Plot 2: SAOCOM - Copernicus Heat Map\n",
    "# =============================================================================\n",
    "ax = axes[1]\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "# Background: Sentinel RGB (very faint)\n",
    "ax.imshow(sentinel_rgb_norm, extent=extent, origin='upper', alpha=0.2)\n",
    "\n",
    "# Extract coordinates and values\n",
    "x_cop = valid_cop.geometry.x.values\n",
    "y_cop = valid_cop.geometry.y.values\n",
    "z_cop = valid_cop['diff_copernicus'].values\n",
    "\n",
    "# Interpolate to grid using cubic method\n",
    "print(\"Interpolating Copernicus differences...\")\n",
    "zi_cop = griddata((x_cop, y_cop), z_cop, (xi_grid, yi_grid),\n",
    "                  method='cubic', fill_value=np.nan)\n",
    "\n",
    "# Apply Gaussian smoothing\n",
    "zi_cop_smooth = gaussian_filter(np.nan_to_num(zi_cop, nan=0), sigma=2)\n",
    "zi_cop_smooth[~hull_mask] = np.nan  # Mask to study area\n",
    "\n",
    "# Plot heat map\n",
    "im2 = ax.imshow(zi_cop_smooth, extent=extent, origin='upper',\n",
    "                cmap='RdBu_r', alpha=0.8,\n",
    "                vmin=-common_limit, vmax=common_limit,\n",
    "                interpolation='bilinear')\n",
    "\n",
    "# Overlay original points (small, for reference)\n",
    "ax.scatter(x_cop, y_cop, c=z_cop, cmap='RdBu_r',\n",
    "           s=0.5, alpha=0.3, edgecolors='none',\n",
    "           vmin=-common_limit, vmax=common_limit)\n",
    "\n",
    "# Study area boundary\n",
    "hull_gdf.boundary.plot(ax=ax, color='black', linewidth=2.5, linestyle='--')\n",
    "\n",
    "# Colorbar\n",
    "cbar2 = plt.colorbar(im2, ax=ax, label='Height Difference (m)',\n",
    "                     shrink=0.8, pad=0.02)\n",
    "cbar2.ax.tick_params(labelsize=10, colors='black')\n",
    "cbar2.ax.yaxis.label.set_color('black')\n",
    "\n",
    "ax.set_xlabel('UTM Easting (m)', fontsize=12, color='black')\n",
    "ax.set_ylabel('UTM Northing (m)', fontsize=12, color='black')\n",
    "ax.set_title('SAOCOM - Copernicus\\nInterpolated Height Residual Heat Map',\n",
    "             fontweight='bold', fontsize=14, color='black')\n",
    "ax.grid(True, alpha=0.3, color='black', linewidth=0.5)\n",
    "ax.tick_params(colors='black')\n",
    "\n",
    "# Statistics box\n",
    "stats_text = f\"\"\"Points: {len(valid_cop):,}\n",
    "Mean: {valid_cop['diff_copernicus'].mean():+.2f} m\n",
    "Median: {valid_cop['diff_copernicus'].median():+.2f} m\n",
    "RMSE: {np.sqrt((valid_cop['diff_copernicus']**2).mean()):.2f} m\n",
    "NMAD: {1.4826 * np.median(np.abs(valid_cop['diff_copernicus'] - valid_cop['diff_copernicus'].median())):.2f} m\n",
    "Std: {valid_cop['diff_copernicus'].std():.2f} m\n",
    "\n",
    "Interpolation: Cubic + Gaussian\n",
    "Color Scale: \u00b1{common_limit:.1f} m\n",
    "Red = SAOCOM Higher\n",
    "Blue = Copernicus Higher\"\"\"\n",
    "\n",
    "ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=9,\n",
    "        verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.9,\n",
    "                 edgecolor='black'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'saocom_residual_heatmaps_interpolated.png',\n",
    "            dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved: saocom_residual_heatmaps_interpolated.png\")\n",
    "print(f\"Interpolation method: Cubic spline + Gaussian smoothing (sigma=2)\")\n",
    "print(f\"Color scale range: \u00b1{common_limit:.2f} m\")\n",
    "\n"
   ],
   "id": "b889084bf560beb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Class Overlays Basic\n",
   "id": "a3baa4dd4b2b89f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### INDIVIDUAL CLASS OVERLAY MAPS (COLORBLIND-FRIENDLY)",
   "id": "616b3bb301efccf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# INDIVIDUAL CLASS OVERLAY MAPS (COLORBLIND-FRIENDLY)\n",
    "# =============================================================================\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Get unique classes present in data\n",
    "unique_classes = np.unique(corine_10m[corine_10m > 0])\n",
    "\n",
    "# Create one map per class\n",
    "for lc_code in sorted(unique_classes):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 10), facecolor='white')\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    # Display Sentinel RGB as background\n",
    "    ax.imshow(sentinel_rgb_norm, extent=[xmin_grid, xmax_grid, ymin_grid, ymax_grid],\n",
    "              origin='upper', alpha=0.7)  # Slight transparency to help overlay show\n",
    "\n",
    "    # Get color for this land cover class\n",
    "    fill_color = tuple(c/255 for c in CORINE_COLORS.get(lc_code, (128, 128, 128)))\n",
    "\n",
    "    # Create mask for this land cover class\n",
    "    lc_mask = (corine_10m == lc_code)\n",
    "\n",
    "    # Vectorize to get boundaries\n",
    "    mask_shapes = shapes(lc_mask.astype(np.uint8), mask=lc_mask, transform=target_transform)\n",
    "\n",
    "    # Convert to polygons and plot\n",
    "    polys = [shape(geom) for geom, val in mask_shapes if val == 1]\n",
    "\n",
    "    if polys:\n",
    "        for poly in polys:\n",
    "            if poly.is_valid:\n",
    "                x, y = poly.exterior.xy\n",
    "\n",
    "                # Fill with class-specific color + hatching for visibility\n",
    "                ax.fill(x, y, color=fill_color, alpha=0.4,\n",
    "                       edgecolor='none', hatch='///', linewidth=0)\n",
    "\n",
    "                # Bold black outline for definition\n",
    "                ax.plot(x, y, color='black', linewidth=2.5, alpha=0.9)\n",
    "\n",
    "                # Colored inner outline\n",
    "                ax.plot(x, y, color=fill_color, linewidth=1.5, alpha=1.0)\n",
    "\n",
    "    # Add study area boundary\n",
    "    hull_gdf.boundary.plot(ax=ax, color='black', linewidth=3, linestyle='--', alpha=0.8)\n",
    "    hull_gdf.boundary.plot(ax=ax, color='red', linewidth=1.5, linestyle='--', alpha=1.0)\n",
    "\n",
    "    # Calculate statistics\n",
    "    lc_count = np.sum(lc_mask)\n",
    "    area_km2 = lc_count * (GRID_SIZE**2) / 1e6\n",
    "    pct_area = 100 * lc_count / np.sum(corine_10m > 0)\n",
    "\n",
    "    # Title with statistics\n",
    "    class_name = CORINE_CLASSES.get(lc_code, f'Class {lc_code}')\n",
    "    ax.set_title(f'Land Cover: {class_name}\\n'\n",
    "                 f'Code {lc_code} | Area: {area_km2:.1f} km\u00b2 ({pct_area:.1f}%)',\n",
    "                 fontweight='bold', fontsize=13, pad=15)\n",
    "\n",
    "    ax.set_xlabel('UTM Easting (m)', fontsize=11)\n",
    "    ax.set_ylabel('UTM Northing (m)', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, color='gray', linewidth=0.5)\n",
    "\n",
    "    # Legend with hatching\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=fill_color, edgecolor='black', linewidth=2,\n",
    "              alpha=0.4, hatch='///', label=class_name),\n",
    "        Patch(facecolor='none', edgecolor='red', linestyle='--',\n",
    "              linewidth=2, label='Study Area')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right', fontsize=10,\n",
    "              frameon=True, fancybox=False, edgecolor='black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save\n",
    "    safe_name = class_name.replace(' ', '_').replace(',', '').replace('/', '_')\n",
    "    filename = f'landcover_{lc_code}_{safe_name}.png'\n",
    "    plt.savefig(RESULTS_DIR / filename, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "print(f\"\\nGenerated {len(unique_classes)} individual land cover overlay maps\")"
   ],
   "id": "4df0f162885db27d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SAOCOM VS TINITALY COMPARISON",
   "id": "2c164913486822f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# SAOCOM VS TINITALY COMPARISON\n",
    "# =============================================================================\n",
    "# Filter for valid comparisons with elevation range check\n",
    "valid_elevation_range = (50, 850)\n",
    "\n",
    "saocom_tinitaly_mask = (\n",
    "    (saocom_gdf['HEIGHT_ABSOLUTE_TIN'].notna()) &\n",
    "    (saocom_gdf['tinitaly_height'].notna()) &\n",
    "    (saocom_gdf['HEIGHT_ABSOLUTE_TIN'] >= valid_elevation_range[0]) &\n",
    "    (saocom_gdf['HEIGHT_ABSOLUTE_TIN'] <= valid_elevation_range[1]) &\n",
    "    (saocom_gdf['COHER'] >= 0.5)\n",
    ")\n",
    "saocom_tinitaly_valid = saocom_gdf[saocom_tinitaly_mask]\n",
    "\n",
    "saocom_tinitaly_diff = (saocom_tinitaly_valid['HEIGHT_ABSOLUTE_TIN'] -\n",
    "                        saocom_tinitaly_valid['tinitaly_height']).values\n",
    "\n",
    "saocom_tinitaly_metrics = {\n",
    "    'n_points': int(len(saocom_tinitaly_diff)),\n",
    "    'mean_diff': float(np.mean(saocom_tinitaly_diff)),\n",
    "    'median_diff': float(np.median(saocom_tinitaly_diff)),\n",
    "    'std_diff': float(np.std(saocom_tinitaly_diff)),\n",
    "    'rmse': float(np.sqrt(np.mean(saocom_tinitaly_diff**2))),\n",
    "    'mae': float(np.mean(np.abs(saocom_tinitaly_diff))),\n",
    "    'nmad': float(1.4826 * np.median(np.abs(saocom_tinitaly_diff - np.median(saocom_tinitaly_diff)))),\n",
    "    'min_diff': float(np.min(saocom_tinitaly_diff)),\n",
    "    'max_diff': float(np.max(saocom_tinitaly_diff)),\n",
    "    'correlation': float(np.corrcoef(saocom_tinitaly_valid['HEIGHT_ABSOLUTE_TIN'].values,\n",
    "                                     saocom_tinitaly_valid['tinitaly_height'].values)[0, 1])\n",
    "}\n",
    "\n",
    "saocom_tinitaly_tolerance = float(saocom_tinitaly_metrics['nmad'])\n",
    "saocom_tinitaly_higher_mask = saocom_tinitaly_diff > saocom_tinitaly_tolerance\n",
    "saocom_tinitaly_lower_mask = saocom_tinitaly_diff < -saocom_tinitaly_tolerance\n",
    "saocom_tinitaly_equal_mask = np.abs(saocom_tinitaly_diff) <= saocom_tinitaly_tolerance\n",
    "\n",
    "saocom_tinitaly_higher_count = np.sum(saocom_tinitaly_higher_mask)\n",
    "saocom_tinitaly_lower_count = np.sum(saocom_tinitaly_lower_mask)\n",
    "saocom_tinitaly_equal_count = np.sum(saocom_tinitaly_equal_mask)\n",
    "\n",
    "saocom_tinitaly_pct_higher = 100 * saocom_tinitaly_higher_count / len(saocom_tinitaly_diff)\n",
    "saocom_tinitaly_pct_lower = 100 * saocom_tinitaly_lower_count / len(saocom_tinitaly_diff)\n",
    "saocom_tinitaly_pct_equal = 100 * saocom_tinitaly_equal_count / len(saocom_tinitaly_diff)\n",
    "\n",
    "# =============================================================================\n",
    "# SAOCOM VS COPERNICUS COMPARISON\n",
    "# =============================================================================\n",
    "saocom_copernicus_mask = (\n",
    "    (saocom_gdf['HEIGHT_ABSOLUTE_COP'].notna()) &\n",
    "    (saocom_gdf['copernicus_height'].notna()) &\n",
    "    (saocom_gdf['HEIGHT_ABSOLUTE_COP'] >= valid_elevation_range[0]) &\n",
    "    (saocom_gdf['HEIGHT_ABSOLUTE_COP'] <= valid_elevation_range[1]) &\n",
    "    (saocom_gdf['COHER'] >= 0.5)\n",
    ")\n",
    "saocom_copernicus_valid = saocom_gdf[saocom_copernicus_mask]\n",
    "\n",
    "saocom_copernicus_diff = (saocom_copernicus_valid['HEIGHT_ABSOLUTE_COP'] -\n",
    "                          saocom_copernicus_valid['copernicus_height']).values\n",
    "\n",
    "saocom_copernicus_metrics = {\n",
    "    'n_points': int(len(saocom_copernicus_diff)),\n",
    "    'mean_diff': float(np.mean(saocom_copernicus_diff)),\n",
    "    'median_diff': float(np.median(saocom_copernicus_diff)),\n",
    "    'std_diff': float(np.std(saocom_copernicus_diff)),\n",
    "    'rmse': float(np.sqrt(np.mean(saocom_copernicus_diff**2))),\n",
    "    'mae': float(np.mean(np.abs(saocom_copernicus_diff))),\n",
    "    'nmad': float(1.4826 * np.median(np.abs(saocom_copernicus_diff - np.median(saocom_copernicus_diff)))),\n",
    "    'min_diff': float(np.min(saocom_copernicus_diff)),\n",
    "    'max_diff': float(np.max(saocom_copernicus_diff)),\n",
    "    'correlation': float(np.corrcoef(saocom_copernicus_valid['HEIGHT_ABSOLUTE_COP'].values,\n",
    "                                     saocom_copernicus_valid['copernicus_height'].values)[0, 1])\n",
    "}\n",
    "\n",
    "saocom_copernicus_tolerance = float(saocom_copernicus_metrics['nmad'])\n",
    "saocom_copernicus_higher_mask = saocom_copernicus_diff > saocom_copernicus_tolerance\n",
    "saocom_copernicus_lower_mask = saocom_copernicus_diff < -saocom_copernicus_tolerance\n",
    "saocom_copernicus_equal_mask = np.abs(saocom_copernicus_diff) <= saocom_copernicus_tolerance\n",
    "\n",
    "saocom_copernicus_higher_count = int(np.sum(saocom_copernicus_higher_mask))\n",
    "saocom_copernicus_lower_count = int(np.sum(saocom_copernicus_lower_mask))\n",
    "saocom_copernicus_equal_count = int(np.sum(saocom_copernicus_equal_mask))\n",
    "\n",
    "saocom_copernicus_pct_higher = float(100 * saocom_copernicus_higher_count / len(saocom_copernicus_diff))\n",
    "saocom_copernicus_pct_lower = float(100 * saocom_copernicus_lower_count / len(saocom_copernicus_diff))\n",
    "saocom_copernicus_pct_equal = float(100 * saocom_copernicus_equal_count / len(saocom_copernicus_diff))\n",
    "\n"
   ],
   "id": "3590ef9aef1cf80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    ### 3d model\n",
   "id": "4f0559fad1cd3db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Use the cleaned dataset and outliers from Isolation Forest\n",
    "normal_sample_3d = saocom_gdf.sample(n=min(5000, len(saocom_gdf)), random_state=42)\n",
    "outlier_sample_3d = saocom_outliers.sample(n=min(500, len(saocom_outliers)), random_state=42) if len(saocom_outliers) > 0 else saocom_outliers\n",
    "\n",
    "# Create SAOCOM interpolated surface\n",
    "saocom_valid = saocom_gdf[saocom_gdf['HEIGHT_ABSOLUTE_TIN'].notna()].copy()\n",
    "x_saocom = saocom_valid.geometry.x.values\n",
    "y_saocom = saocom_valid.geometry.y.values\n",
    "z_saocom = saocom_valid['HEIGHT_ABSOLUTE_TIN'].values\n",
    "\n",
    "# Create grid for surface\n",
    "xi = np.linspace(xmin_grid, xmax_grid, 100)\n",
    "yi = np.linspace(ymin_grid, ymax_grid, 100)\n",
    "xi_grid, yi_grid = np.meshgrid(xi, yi)\n",
    "\n",
    "# Interpolate SAOCOM surface\n",
    "zi_saocom = griddata((x_saocom, y_saocom), z_saocom, (xi_grid, yi_grid),\n",
    "                     method='linear', fill_value=np.nan)\n",
    "\n",
    "# Downsample TINITALY surface\n",
    "tinitaly_downsampled = tinitaly_10m[::10, ::10]\n",
    "x_tin = np.linspace(xmin_grid, xmax_grid, tinitaly_downsampled.shape[1])\n",
    "y_tin = np.linspace(ymax_grid, ymin_grid, tinitaly_downsampled.shape[0])\n",
    "x_tin_grid, y_tin_grid = np.meshgrid(x_tin, y_tin)\n",
    "tinitaly_downsampled = np.where(tinitaly_downsampled == NODATA, np.nan, tinitaly_downsampled)\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# TINITALY Surface\n",
    "fig.add_trace(go.Surface(\n",
    "    x=x_tin_grid, y=y_tin_grid, z=tinitaly_downsampled,\n",
    "    colorscale='Earth', name='TINITALY DEM', showscale=True,\n",
    "    colorbar=dict(x=1.15, title='Elevation (m)'),\n",
    "    visible=True, opacity=0.8,\n",
    "    hovertemplate='X: %{x:.0f}<br>Y: %{y:.0f}<br>TINITALY: %{z:.1f}m<extra></extra>'\n",
    "))\n",
    "\n",
    "# SAOCOM Interpolated Surface\n",
    "fig.add_trace(go.Surface(\n",
    "    x=xi_grid, y=yi_grid, z=zi_saocom,\n",
    "    colorscale='Viridis', name='SAOCOM Surface',\n",
    "    showscale=False, visible=False, opacity=0.7,\n",
    "    hovertemplate='X: %{x:.0f}<br>Y: %{y:.0f}<br>SAOCOM: %{z:.1f}m<extra></extra>'\n",
    "))\n",
    "\n",
    "# Normal SAOCOM Points\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=normal_sample_3d.geometry.x, y=normal_sample_3d.geometry.y,\n",
    "    z=normal_sample_3d['HEIGHT_ABSOLUTE_TIN'],\n",
    "    mode='markers', name='SAOCOM Points',\n",
    "    marker=dict(size=2, color=normal_sample_3d['diff_tinitaly'],\n",
    "                colorscale='RdBu_r', cmin=-10, cmax=10,\n",
    "                colorbar=dict(x=1.0, title='Residual (m)', len=0.5, y=0.25),\n",
    "                showscale=True, line=dict(width=0)),\n",
    "    text=[f\"Residual: {r:+.2f}m<br>Height: {h:.1f}m<br>Coherence: {c:.2f}\"\n",
    "          for r, h, c in zip(normal_sample_3d['diff_tinitaly'],\n",
    "                            normal_sample_3d['HEIGHT_ABSOLUTE_TIN'],\n",
    "                            normal_sample_3d['COHER'])],\n",
    "    hovertemplate='%{text}<extra></extra>', visible=True\n",
    "))\n",
    "\n",
    "# Outlier Points\n",
    "if len(outlier_sample_3d) > 0:\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=outlier_sample_3d.geometry.x, y=outlier_sample_3d.geometry.y,\n",
    "        z=outlier_sample_3d['HEIGHT_ABSOLUTE_TIN'],\n",
    "        mode='markers', name='Outliers',\n",
    "        marker=dict(size=6, color='red', symbol='diamond',\n",
    "                   line=dict(color='black', width=1)),\n",
    "        text=[f\"<b>OUTLIER</b><br>Residual: {r:+.2f}m<br>Height: {h:.1f}m<br>Coherence: {c:.2f}\"\n",
    "              for r, h, c in zip(outlier_sample_3d['diff_tinitaly'],\n",
    "                                outlier_sample_3d['HEIGHT_ABSOLUTE_TIN'],\n",
    "                                outlier_sample_3d['COHER'])],\n",
    "        hovertemplate='%{text}<extra></extra>', visible=True\n",
    "    ))\n",
    "\n",
    "# Layout\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(type=\"buttons\", direction=\"down\", x=0.02, xanchor=\"left\",\n",
    "             y=0.98, yanchor=\"top\",\n",
    "             buttons=[\n",
    "                 dict(label=\"All\", method=\"update\",\n",
    "                      args=[{\"visible\": [True, True, True, True]}]),\n",
    "                 dict(label=\"Surfaces Only\", method=\"update\",\n",
    "                      args=[{\"visible\": [True, True, False, False]}]),\n",
    "                 dict(label=\"Points Only\", method=\"update\",\n",
    "                      args=[{\"visible\": [False, False, True, True]}]),\n",
    "                 dict(label=\"TINITALY + Points\", method=\"update\",\n",
    "                      args=[{\"visible\": [True, False, True, True]}]),\n",
    "                 dict(label=\"SAOCOM Surface + Outliers\", method=\"update\",\n",
    "                      args=[{\"visible\": [False, True, False, True]}]),\n",
    "             ]),\n",
    "        dict(type=\"buttons\", direction=\"right\", x=0.02, xanchor=\"left\",\n",
    "             y=0.02, yanchor=\"bottom\",\n",
    "             buttons=[\n",
    "                 dict(label=\"Toggle TINITALY\", method=\"restyle\",\n",
    "                      args=[\"visible\", \"toggle\"], args2=[{\"visible\": [True]}, [0]]),\n",
    "                 dict(label=\"Toggle SAOCOM Surface\", method=\"restyle\",\n",
    "                      args=[\"visible\", \"toggle\"], args2=[{\"visible\": [True]}, [1]]),\n",
    "                 dict(label=\"Toggle Points\", method=\"restyle\",\n",
    "                      args=[\"visible\", \"toggle\"], args2=[{\"visible\": [True]}, [2]]),\n",
    "                 dict(label=\"Toggle Outliers\", method=\"restyle\",\n",
    "                      args=[\"visible\", \"toggle\"], args2=[{\"visible\": [True]}, [3]]),\n",
    "             ])\n",
    "    ],\n",
    "    scene=dict(xaxis_title='UTM Easting (m)', yaxis_title='UTM Northing (m)',\n",
    "               zaxis_title='Elevation (m)', aspectmode='manual',\n",
    "               aspectratio=dict(x=1, y=1, z=0.3),\n",
    "               camera=dict(eye=dict(x=1.5, y=1.5, z=1.2))),\n",
    "    title=dict(text='3D SAOCOM vs TINITALY Analysis<br><sub>Top: Presets | Bottom: Toggle layers</sub>',\n",
    "               x=0.5, xanchor='center'),\n",
    "    width=1400, height=900, showlegend=True,\n",
    "    legend=dict(x=0.02, y=0.5),\n",
    "    paper_bgcolor='white', plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "fig.write_html(RESULTS_DIR / 'saocom_3d_interactive.html')\n",
    "print(f\"Saved: {RESULTS_DIR / 'saocom_3d_interactive.html'}\")\n",
    "fig.show()"
   ],
   "id": "da913839ec3aed88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Note: This script assumes the following variables are pre-defined from previous cells:\n",
    "# saocom_gdf, COHERENCE_THRESHOLD, hull_mask, hull_gdf, grid_height, grid_width,\n",
    "# target_transform, xmin_grid, xmax_grid, ymin_grid, ymax_grid,\n",
    "# saocom_tinitaly_metrics, saocom_copernicus_metrics, RESULTS_DIR\n",
    "\n",
    "def create_difference_grid(gdf, height_col, ref_col, grid_shape, transform, hull_mask):\n",
    "    \"\"\"Grids point data differences onto a raster grid using nearest neighbor interpolation.\"\"\"\n",
    "    query_str = f\"`{height_col}`.notna() & `{ref_col}`.notna() & COHER >= @COHERENCE_THRESHOLD\"\n",
    "    valid_points = gdf.query(query_str).copy()\n",
    "    valid_points['diff'] = valid_points[height_col] - valid_points[ref_col]\n",
    "\n",
    "    if valid_points.empty:\n",
    "        return np.full(grid_shape, np.nan), valid_points\n",
    "\n",
    "    # Create grid coordinates for interpolation\n",
    "    grid_height, grid_width = grid_shape\n",
    "    x_coords = np.linspace(transform.c, transform.c + transform.a * grid_width, grid_width)\n",
    "    y_coords = np.linspace(transform.f, transform.f + transform.e * grid_height, grid_height)\n",
    "    xi_grid, yi_grid = np.meshgrid(x_coords, y_coords)\n",
    "\n",
    "    # Interpolate and mask the grid\n",
    "    diff_grid = griddata(\n",
    "        (valid_points.geometry.x, valid_points.geometry.y),\n",
    "        valid_points['diff'],\n",
    "        (xi_grid, yi_grid),\n",
    "        method='nearest'\n",
    "    )\n",
    "    diff_grid[~hull_mask] = np.nan\n",
    "    return diff_grid, valid_points\n",
    "\n",
    "def plot_panel(ax, data, title, cmap, vmin=None, vmax=None, stats_text=None):\n",
    "    \"\"\"Helper function to plot a single map panel.\"\"\"\n",
    "    ax.set_facecolor('white')\n",
    "    cmap.set_bad(color='white', alpha=0)\n",
    "    extent = [xmin_grid, xmax_grid, ymin_grid, ymax_grid]\n",
    "    im = ax.imshow(data, cmap=cmap, origin='upper', extent=extent, vmin=vmin, vmax=vmax)\n",
    "    hull_gdf.boundary.plot(ax=ax, color='black', linewidth=1.5)\n",
    "    ax.set_title(title, fontweight='bold', fontsize=12)\n",
    "    ax.set_xlabel('UTM Easting (m)')\n",
    "    ax.set_ylabel('UTM Northing (m)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.colorbar(im, ax=ax, label='Difference (m)', shrink=0.8)\n",
    "    if stats_text:\n",
    "        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=9,\n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "\n",
    "# --- Main Execution ---\n",
    "print(\"Gridding SAOCOM differences (using cleaned data)...\")\n",
    "diff_grid_tin, points_tin = create_difference_grid(saocom_gdf, 'HEIGHT_ABSOLUTE_TIN', 'tinitaly_height', (grid_height, grid_width), target_transform, hull_mask)\n",
    "diff_grid_cop, points_cop = create_difference_grid(saocom_gdf, 'HEIGHT_ABSOLUTE_COP', 'copernicus_height', (grid_height, grid_width), target_transform, hull_mask)\n",
    "\n",
    "# --- Visualization ---\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 14), facecolor='white')\n",
    "fig.suptitle('SAOCOM vs. Reference DEMs - Gridded Difference Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plotting parameters\n",
    "v_tin = np.nanpercentile(np.abs(points_tin['diff']), 98)\n",
    "v_cop = np.nanpercentile(np.abs(points_cop['diff']), 98)\n",
    "stats_tin = f\"n = {len(points_tin):,}\\nMean = {points_tin['diff'].mean():+.2f} m\\nRMSE = {np.sqrt(np.mean(points_tin['diff']**2)):.2f} m\"\n",
    "stats_cop = f\"n = {len(points_cop):,}\\nMean = {points_cop['diff'].mean():+.2f} m\\nRMSE = {np.sqrt(np.mean(points_cop['diff']**2)):.2f} m\"\n",
    "\n",
    "# Row 1: SAOCOM vs TINITALY\n",
    "plot_panel(axes[0, 0], diff_grid_tin, 'SAOCOM - TINITALY\\nFull Difference', plt.cm.coolwarm, -v_tin, v_tin, stats_tin)\n",
    "plot_panel(axes[0, 1], np.where(diff_grid_tin > 0, diff_grid_tin, np.nan), 'SAOCOM > TINITALY', plt.cm.Reds, 0, v_tin)\n",
    "plot_panel(axes[0, 2], np.where(diff_grid_tin < 0, diff_grid_tin, np.nan), 'TINITALY > SAOCOM', plt.cm.Blues_r, -v_tin, 0)\n",
    "\n",
    "# Row 2: SAOCOM vs Copernicus\n",
    "plot_panel(axes[1, 0], diff_grid_cop, 'SAOCOM - Copernicus\\nFull Difference', plt.cm.coolwarm, -v_cop, v_cop, stats_cop)\n",
    "plot_panel(axes[1, 1], np.where(diff_grid_cop > 0, diff_grid_cop, np.nan), 'SAOCOM > Copernicus', plt.cm.Reds, 0, v_cop)\n",
    "plot_panel(axes[1, 2], np.where(diff_grid_cop < 0, diff_grid_cop, np.nan), 'Copernicus > SAOCOM', plt.cm.Blues_r, -v_cop, 0)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(RESULTS_DIR / 'saocom_gridded_comparison.png', dpi=300, facecolor='white')\n",
    "plt.show()"
   ],
   "id": "d7f0ff39558e2e35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Note: This script assumes the following variables are pre-defined from previous cells:\n",
    "# saocom_gdf, COHERENCE_THRESHOLD, hull_gdf, saocom_tinitaly_metrics,\n",
    "# saocom_copernicus_metrics, RESULTS_DIR, xmin_grid, xmax_grid, ymin_grid, ymax_grid\n",
    "\n",
    "# =============================================================================\n",
    "# Helper function for plotting points (Unchanged)\n",
    "# =============================================================================\n",
    "def plot_points_panel(ax, data, title, cmap, vmin=None, vmax=None, stats_text=None):\n",
    "    \"\"\"Helper function to plot a single map panel using a scatter plot.\"\"\"\n",
    "    ax.set_facecolor('gainsboro')\n",
    "\n",
    "    im = ax.scatter(data.geometry.x, data.geometry.y, c=data['diff'],\n",
    "                    s=1, alpha=0.7, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "\n",
    "    hull_gdf.boundary.plot(ax=ax, color='black', linewidth=1.5)\n",
    "    ax.set_title(title, fontweight='bold', fontsize=12)\n",
    "    ax.set_xlabel('UTM Easting (m)')\n",
    "    ax.set_ylabel('UTM Northing (m)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.colorbar(im, ax=ax, label='Difference (m)', shrink=0.8)\n",
    "    if stats_text:\n",
    "        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=9,\n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n",
    "\n",
    "# =============================================================================\n",
    "# Data preparation (Unchanged)\n",
    "# =============================================================================\n",
    "print(\"Filtering SAOCOM differences for plotting...\")\n",
    "\n",
    "query_str_tin = f\"`HEIGHT_ABSOLUTE_TIN`.notna() & `tinitaly_height`.notna() & COHER >= @COHERENCE_THRESHOLD\"\n",
    "points_tin = saocom_gdf.query(query_str_tin).copy()\n",
    "points_tin['diff'] = points_tin['HEIGHT_ABSOLUTE_TIN'] - points_tin['tinitaly_height']\n",
    "\n",
    "query_str_cop = f\"`HEIGHT_ABSOLUTE_COP`.notna() & `copernicus_height`.notna() & COHER >= @COHERENCE_THRESHOLD\"\n",
    "points_cop = saocom_gdf.query(query_str_cop).copy()\n",
    "points_cop['diff'] = points_cop['HEIGHT_ABSOLUTE_COP'] - points_cop['copernicus_height']\n",
    "\n",
    "\n",
    "# --- Visualization ---\n",
    "# =============================================================================\n",
    "# MODIFIED: Change subplot layout from (2, 3) to (3, 2) and adjust figsize\n",
    "# =============================================================================\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 20), facecolor='white')\n",
    "fig.suptitle('SAOCOM vs. Reference DEMs - Point-Based Difference Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plotting parameters (Unchanged)\n",
    "v_tin = np.nanpercentile(np.abs(points_tin['diff']), 98)\n",
    "v_cop = np.nanpercentile(np.abs(points_cop['diff']), 98)\n",
    "stats_tin = f\"n = {len(points_tin):,}\\nMean = {points_tin['diff'].mean():+.2f} m\\nRMSE = {np.sqrt(np.mean(points_tin['diff']**2)):.2f} m\"\n",
    "stats_cop = f\"n = {len(points_cop):,}\\nMean = {points_cop['diff'].mean():+.2f} m\\nRMSE = {np.sqrt(np.mean(points_cop['diff']**2)):.2f} m\"\n",
    "\n",
    "# =============================================================================\n",
    "# MODIFIED: Update axes indexing for the new 3x2 layout\n",
    "# =============================================================================\n",
    "# Row 1: Full Difference Maps\n",
    "plot_points_panel(axes[0, 0], points_tin, 'SAOCOM - TINITALY\\nFull Difference', plt.cm.coolwarm, -v_tin, v_tin, stats_tin)\n",
    "plot_points_panel(axes[0, 1], points_cop, 'SAOCOM - Copernicus\\nFull Difference', plt.cm.coolwarm, -v_cop, v_cop, stats_cop)\n",
    "\n",
    "# Row 2: SAOCOM > Reference DEM\n",
    "plot_points_panel(axes[1, 0], points_tin[points_tin['diff'] > 0], 'SAOCOM > TINITALY', plt.cm.Reds, 0, v_tin)\n",
    "plot_points_panel(axes[1, 1], points_cop[points_cop['diff'] > 0], 'SAOCOM > Copernicus', plt.cm.Reds, 0, v_cop)\n",
    "\n",
    "# Row 3: Reference DEM > SAOCOM\n",
    "plot_points_panel(axes[2, 0], points_tin[points_tin['diff'] < 0], 'TINITALY > SAOCOM', plt.cm.Blues_r, -v_tin, 0)\n",
    "plot_points_panel(axes[2, 1], points_cop[points_cop['diff'] < 0], 'Copernicus > SAOCOM', plt.cm.Blues_r, -v_cop, 0)\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(RESULTS_DIR / 'saocom_point_comparison_3x2.png', dpi=300, facecolor='white')\n",
    "plt.show()"
   ],
   "id": "9d13b5663bafe95b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Edited Histograms\n",
    "\n"
   ],
   "id": "986658038cb36f24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_distribution(ax, diff_series, title, metrics):\n",
    "    \"\"\"Helper function to plot a single residual distribution histogram.\"\"\"\n",
    "    ax.set_facecolor('white')\n",
    "    ax.hist(diff_series, bins=50, alpha=0.75, color='steelblue', edgecolor='black')\n",
    "    ax.axvline(0, color='red', linestyle='--', label='Zero')\n",
    "    ax.axvline(metrics['mean_diff'], color='green', linestyle='-', label=f\"Mean: {metrics['mean_diff']:+.2f}m\")\n",
    "\n",
    "    stats_text = (f\"n = {metrics['n_points']:,}\\n\"\n",
    "                  f\"RMSE = {metrics['rmse']:.2f} m\\n\"\n",
    "                  f\"NMAD = {metrics['nmad']:.2f} m\\n\"\n",
    "                  f\"Std Dev = {metrics['std_diff']:.2f} m\")\n",
    "\n",
    "    ax.text(0.98, 0.97, stats_text, transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='top', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "    ax.set_title(title, fontweight='bold', fontsize=14)\n",
    "    ax.set_xlabel('Elevation Difference (m)', fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Visualization ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7), facecolor='white')\n",
    "fig.suptitle('Residual Distributions (Cleaned Data)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Use the 'diff' column from the points DataFrames created in the previous cell\n",
    "plot_distribution(axes[0], points_tin['diff'], 'SAOCOM - TINITALY', saocom_tinitaly_metrics)\n",
    "plot_distribution(axes[1], points_cop['diff'], 'SAOCOM - Copernicus', saocom_copernicus_metrics)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(RESULTS_DIR / 'saocom_residual_distributions.png', dpi=300, facecolor='white')\n",
    "plt.show()"
   ],
   "id": "78e87c7a7d043613",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_scatter_comparison(ax, x_data, y_data, x_label, y_label, title, stats):\n",
    "    \"\"\"\n",
    "    Creates a simple 1:1 scatter plot to compare two sets of height data.\n",
    "    \"\"\"\n",
    "    ax.set_facecolor('white') # Set background to white\n",
    "\n",
    "    # Plot the individual data points\n",
    "    ax.scatter(x_data, y_data, s=1, alpha=0.3, c='steelblue', label='Data Points')\n",
    "\n",
    "    # Determine plot limits and draw the 1:1 line\n",
    "    lims = [\n",
    "        np.min([x_data.min(), y_data.min()]),\n",
    "        np.max([x_data.max(), y_data.max()]),\n",
    "    ]\n",
    "    ax.plot(lims, lims, 'r--', linewidth=2, label='1:1 Line', zorder=10)\n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims)\n",
    "\n",
    "    # Add statistics box\n",
    "    stats_text = (f\"n = {stats.get('n_points', stats.get('n_pixels')):,}\\n\"\n",
    "                  f\"Bias = {stats['mean_diff']:.2f} m\\n\"\n",
    "                  f\"RMSE = {stats['rmse']:.2f} m\\n\"\n",
    "                  f\"Corr (r) = {stats['correlation']:.3f}\")\n",
    "\n",
    "    ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=9,\n",
    "            verticalalignment='top', color='black', # Text color to black\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.7, edgecolor='black'))\n",
    "\n",
    "    # Style the plot\n",
    "    ax.set_title(title, fontweight='bold', fontsize=12, color='black')\n",
    "    ax.set_xlabel(x_label, fontsize=11, color='black')\n",
    "    ax.set_ylabel(y_label, fontsize=11, color='black')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "\n",
    "    # Style ticks and spines\n",
    "    ax.tick_params(colors='black')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('black')\n",
    "\n",
    "\n",
    "# --- Visualization ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 7), facecolor='white')\n",
    "fig.suptitle('1:1 Height Comparison Scatter Plots (Cleaned Data)', fontsize=16, fontweight='bold', color='black')\n",
    "\n",
    "# Note: Assumes variables from previous cells are available\n",
    "# (points_tin, points_cop, valid_copernicus, valid_tinitaly, and all metrics dicts)\n",
    "\n",
    "plot_scatter_comparison(axes[0], points_tin['tinitaly_height'], points_tin['HEIGHT_ABSOLUTE_TIN'],\n",
    "                        'TINITALY Height (m)', 'SAOCOM Height (m)', 'SAOCOM vs TINITALY', saocom_tinitaly_metrics)\n",
    "\n",
    "plot_scatter_comparison(axes[1], points_cop['copernicus_height'], points_cop['HEIGHT_ABSOLUTE_COP'],\n",
    "                        'Copernicus Height (m)', 'SAOCOM Height (m)', 'SAOCOM vs Copernicus', saocom_copernicus_metrics)\n",
    "\n",
    "plot_scatter_comparison(axes[2], valid_copernicus, valid_tinitaly,\n",
    "                        'Copernicus Height (m)', 'TINITALY Height (m)', 'TINITALY vs Copernicus', ref_metrics)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(RESULTS_DIR / 'saocom_scatter_comparisons.png', dpi=300, facecolor='white')\n",
    "plt.show()"
   ],
   "id": "bc045bb09b35cc7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Density Plots Color\n",
   "id": "fbf5d5ba2b937348"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# --- Visualization ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 7), facecolor='white')\n",
    "fig.suptitle('1:1 Height Comparison - High-Contrast Hexbin Density Plots', fontsize=16, fontweight='bold')\n",
    "\n",
    "def plot_hexbin(ax, x_data, y_data, x_label, y_label, title):\n",
    "    \"\"\"Helper function to create a hexbin plot with improved contrast.\"\"\"\n",
    "    ax.set_facecolor('gainsboro')\n",
    "\n",
    "    # Create the hexbin plot\n",
    "    # CHANGE 1: Switched to 'inferno' colormap for a high-contrast, \"hotter\" look.\n",
    "    # Other good options are 'plasma' or 'magma'.\n",
    "    hb = ax.hexbin(x_data, y_data, gridsize=150, cmap='inferno',\n",
    "                   norm=colors.LogNorm(), mincnt=1) # Log scale is essential\n",
    "\n",
    "    # Add a color bar\n",
    "    fig.colorbar(hb, ax=ax, label='Point Count')\n",
    "\n",
    "    # CHANGE 2: Switched 1:1 line to red for better visibility against the colormap.\n",
    "    lims = [min(ax.get_xlim()[0], ax.get_ylim()[0]), max(ax.get_xlim()[1], ax.get_ylim()[1])]\n",
    "    ax.plot(lims, lims, 'r--', linewidth=2, label='1:1 line', alpha=0.9)\n",
    "\n",
    "    ax.set_title(title, fontweight='bold', fontsize=12)\n",
    "    ax.set_xlabel(x_label, fontsize=11)\n",
    "    ax.set_ylabel(y_label, fontsize=11)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    ax.legend()\n",
    "    ax.set_aspect('equal', 'box')\n",
    "\n",
    "# This is an alternative function you can use instead of plot_hexbin for a different style.\n",
    "def plot_hist2d(ax, x_data, y_data, x_label, y_label, title):\n",
    "    \"\"\"Alternative helper function to create a 2D histogram (square bins).\"\"\"\n",
    "    ax.set_facecolor('gainsboro')\n",
    "\n",
    "    # Create the 2D histogram\n",
    "    # Bins determines the resolution. Cmin=1 ensures empty bins are not colored.\n",
    "    h = ax.hist2d(x_data, y_data, bins=150, cmap='inferno',\n",
    "                  norm=colors.LogNorm(), cmin=1)\n",
    "\n",
    "    # Add a color bar\n",
    "    fig.colorbar(h[3], ax=ax, label='Point Count')\n",
    "\n",
    "    # Add 1:1 line\n",
    "    lims = [min(ax.get_xlim()[0], ax.get_ylim()[0]), max(ax.get_xlim()[1], ax.get_ylim()[1])]\n",
    "    ax.plot(lims, lims, 'w--', linewidth=1.5, label='1:1 line', alpha=0.7) # White line works well here\n",
    "\n",
    "    ax.set_title(title, fontweight='bold', fontsize=12)\n",
    "    ax.set_xlabel(x_label, fontsize=11)\n",
    "    ax.set_ylabel(y_label, fontsize=11)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    ax.legend()\n",
    "    ax.set_aspect('equal', 'box')\n",
    "\n",
    "# Plot 1: SAOCOM vs TINITALY\n",
    "plot_hexbin(axes[0], points_tin['tinitaly_height'], points_tin['HEIGHT_ABSOLUTE_TIN'],\n",
    "            'TINITALY Height (m)', 'SAOCOM Height (m)', 'SAOCOM vs TINITALY')\n",
    "\n",
    "# Plot 2: SAOCOM vs Copernicus\n",
    "plot_hexbin(axes[1], points_cop['copernicus_height'], points_cop['HEIGHT_ABSOLUTE_COP'],\n",
    "            'Copernicus Height (m)', 'SAOCOM Height (m)', 'SAOCOM vs Copernicus')\n",
    "\n",
    "# Plot 3: TINITALY vs Copernicus\n",
    "plot_hexbin(axes[2], valid_copernicus, valid_tinitaly,\n",
    "            'Copernicus Height (m)', 'TINITALY Height (m)', 'TINITALY vs Copernicus')\n",
    "\n",
    "# To use the 2D histogram instead, you would replace the calls above with:\n",
    "# plot_hist2d(axes[0], ...)\n",
    "# plot_hist2d(axes[1], ...)\n",
    "# plot_hist2d(axes[2], ...)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(RESULTS_DIR / 'saocom_hexbin_comparisons_high_contrast.png', dpi=300, facecolor='white')\n",
    "plt.show()"
   ],
   "id": "d805ab78a400b357",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "\n",
    "# --- Visualization ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 7), facecolor='white')\n",
    "fig.suptitle('1:1 Height Comparison - Bland-Altman Density Plots', fontsize=16, fontweight='bold')\n",
    "\n",
    "def plot_bland_altman(ax, x_data, y_data, x_label, y_label, title):\n",
    "    \"\"\"Helper function to create a Bland-Altman hexbin plot.\"\"\"\n",
    "    # --- Data Transformation ---\n",
    "    # Calculate the average and difference\n",
    "    average = (x_data + y_data) / 2\n",
    "    difference = y_data - x_data\n",
    "\n",
    "    # Calculate key statistics for the plot\n",
    "    mean_diff = np.mean(difference)\n",
    "    std_diff = np.std(difference)\n",
    "    limit_of_agreement = 1.96 * std_diff\n",
    "\n",
    "    ax.set_facecolor('gainsboro')\n",
    "    # Create the hexbin plot using the transformed data\n",
    "    hb = ax.hexbin(average, difference, gridsize=150, cmap='viridis',\n",
    "                   norm=colors.LogNorm(), mincnt=1)\n",
    "\n",
    "    # Add a color bar\n",
    "    fig.colorbar(hb, ax=ax, label='Point Count')\n",
    "\n",
    "    # --- Add Statistical Lines ---\n",
    "    # Line of perfect agreement (zero difference)\n",
    "    ax.axhline(0, color='white', linestyle='--', linewidth=1.5, label='Zero (Perfect Agreement)')\n",
    "    # Mean difference line\n",
    "    ax.axhline(mean_diff, color='red', linestyle='-', linewidth=2, label=f'Mean Diff: {mean_diff:.2f} m')\n",
    "    # Limits of agreement lines (+/- 1.96 * SD)\n",
    "    ax.axhline(mean_diff + limit_of_agreement, color='red', linestyle='--', linewidth=1.5, label=f'Limits of Agreement (\u00b1{limit_of_agreement:.2f} m)')\n",
    "    ax.axhline(mean_diff - limit_of_agreement, color='red', linestyle='--', linewidth=1.5)\n",
    "\n",
    "    ax.set_title(title, fontweight='bold', fontsize=12)\n",
    "    # Update axis labels for the new plot type\n",
    "    ax.set_xlabel(f'Average of ({x_label} and {y_label})', fontsize=11)\n",
    "    ax.set_ylabel(f'Difference ({y_label} - {x_label})', fontsize=11)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    ax.legend()\n",
    "\n",
    "# Plot 1: SAOCOM vs TINITALY\n",
    "plot_bland_altman(axes[0], points_tin['tinitaly_height'], points_tin['HEIGHT_ABSOLUTE_TIN'],\n",
    "                  'SAOCOM', 'TINITALY', 'SAOCOM vs TINITALY')\n",
    "\n",
    "# Plot 2: SAOCOM vs Copernicus\n",
    "plot_bland_altman(axes[1], points_cop['copernicus_height'], points_cop['HEIGHT_ABSOLUTE_COP'],\n",
    "                  'SAOCOM', 'Copernicus', 'SAOCOM vs Copernicus')\n",
    "\n",
    "# Plot 3: TINITALY vs Copernicus\n",
    "plot_bland_altman(axes[2], valid_copernicus, valid_tinitaly,\n",
    "                  'TINITALY', 'Copernicus', 'TINITALY vs Copernicus')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(RESULTS_DIR / 'saocom_bland_altman_comparisons.png', dpi=300, facecolor='white')\n",
    "plt.show()"
   ],
   "id": "4c9d7beeabd1d942",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SAOCOM VS REFERENCE DEMs - GRIDDED COMPARISON ANALYSIS",
   "id": "d7d002b4cf5aed97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.interpolate import griddata\n",
    "# from scipy import stats\n",
    "#\n",
    "# # =============================================================================\n",
    "# # 1. GRIDDED DEM COMPARISON\n",
    "# # =============================================================================\n",
    "#\n",
    "# def process_dem_comparison(gdf, height_col, ref_col, grid_shape, transform, hull_mask, metrics):\n",
    "#     \"\"\"Grids SAOCOM point differences and classifies them against a tolerance.\"\"\"\n",
    "#     # Filter for valid, coherent points and calculate difference\n",
    "#     valid_mask = gdf[height_col].notna() & gdf[ref_col].notna() & (gdf['COHER'] >= COHERENCE_THRESHOLD)\n",
    "#     points = gdf[valid_mask].copy()\n",
    "#     points['diff'] = points[height_col] - points[ref_col]\n",
    "#\n",
    "#     # Interpolate difference onto a grid\n",
    "#     grid_height, grid_width = grid_shape\n",
    "#     xi, yi = np.meshgrid(\n",
    "#         np.linspace(transform.c, transform.c + transform.a * grid_width, grid_width),\n",
    "#         np.linspace(transform.f, transform.f + transform.e * grid_height, grid_height)\n",
    "#     )\n",
    "#     diff_grid = griddata(\n",
    "#         (points.geometry.x, points.geometry.y), points['diff'],\n",
    "#         (xi, yi), method='nearest'\n",
    "#     )\n",
    "#     diff_grid[~hull_mask] = np.nan # Apply hull mask\n",
    "#\n",
    "#     # Classify differences based on NMAD tolerance\n",
    "#     tolerance = metrics['nmad']\n",
    "#     with np.errstate(invalid='ignore'): # Ignore warnings from comparing with NaN\n",
    "#         higher_mask = diff_grid > tolerance\n",
    "#         lower_mask = diff_grid < -tolerance\n",
    "#\n",
    "#     # Generate statistics\n",
    "#     n_valid = np.count_nonzero(~np.isnan(diff_grid))\n",
    "#     stats_dict = {\n",
    "#         'n_total': n_valid,\n",
    "#         'n_higher': np.sum(higher_mask), 'pct_higher': 100 * np.sum(higher_mask) / n_valid,\n",
    "#         'n_lower': np.sum(lower_mask), 'pct_lower': 100 * np.sum(lower_mask) / n_valid,\n",
    "#     }\n",
    "#     return diff_grid, higher_mask, lower_mask, stats_dict, points\n",
    "#\n",
    "# def plot_gridded_comparison(axes, data, title, cmap, vmin=None, vmax=None, stats_text=None):\n",
    "#     \"\"\"Helper function to plot a single difference map.\"\"\"\n",
    "#     ax = axes\n",
    "#     ax.set_facecolor('white')\n",
    "#     cmap.set_bad(color='white', alpha=0)\n",
    "#     im = ax.imshow(data, cmap=cmap, origin='upper', extent=[xmin_grid, xmax_grid, ymin_grid, ymax_grid], vmin=vmin, vmax=vmax)\n",
    "#     hull_gdf.boundary.plot(ax=ax, color='black', linewidth=1.5)\n",
    "#     plt.colorbar(im, ax=ax, label='Difference (m)', shrink=0.8)\n",
    "#     ax.set_title(title, fontweight='bold', fontsize=12)\n",
    "#     ax.set_xlabel('UTM Easting (m)')\n",
    "#     ax.set_ylabel('UTM Northing (m)')\n",
    "#     ax.grid(True, alpha=0.3)\n",
    "#     if stats_text:\n",
    "#         ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=9, va='top',\n",
    "#                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, ec='black'))\n",
    "#\n",
    "# # --- Main Plotting Logic for Gridded Comparison ---\n",
    "# fig, axes = plt.subplots(2, 3, figsize=(20, 14), facecolor='white')\n",
    "#\n",
    "# # Define datasets to compare\n",
    "# comparisons = [\n",
    "#     {'name': 'TINITALY', 'h_col': 'HEIGHT_ABSOLUTE_TIN', 'r_col': 'tinitaly_height', 'metrics': saocom_tinitaly_metrics},\n",
    "#     {'name': 'Copernicus', 'h_col': 'HEIGHT_ABSOLUTE_COP', 'r_col': 'copernicus_height', 'metrics': saocom_copernicus_metrics}\n",
    "# ]\n",
    "#\n",
    "# for i, p in enumerate(comparisons):\n",
    "#     diff_grid, higher, lower, stats, points = process_dem_comparison(\n",
    "#         saocom_gdf, p['h_col'], p['r_col'], (grid_height, grid_width), target_transform, hull_mask, p['metrics']\n",
    "#     )\n",
    "#\n",
    "#     # Plot 1: Full Difference\n",
    "#     diff_limit = np.percentile(np.abs(points['diff']), 95)\n",
    "#     stats1 = f\"Points: {stats['n_total']:,}\\nMean: {p['metrics']['mean_diff']:+.2f}m\\nRMSE: {p['metrics']['rmse']:.2f}m\\nNMAD: {p['metrics']['nmad']:.2f}m\"\n",
    "#     plot_gridded_comparison(axes[i, 0], diff_grid, f\"SAOCOM - {p['name']}\\nFull Difference\", plt.cm.coolwarm.copy(), -diff_limit, diff_limit, stats1)\n",
    "#\n",
    "#     # Plot 2: SAOCOM Higher\n",
    "#     higher_grid = np.where(higher, diff_grid, np.nan)\n",
    "#     stats2 = f\"Points: {stats['n_higher']:,}\\nMean: {np.nanmean(higher_grid):.2f}m\\nMax: {np.nanmax(higher_grid):.2f}m\"\n",
    "#     plot_gridded_comparison(axes[i, 1], higher_grid, f\"SAOCOM > {p['name']}\\n({stats['pct_higher']:.1f}%)\", plt.cm.YlOrRd.copy(), 0, np.nanmax(higher_grid), stats2)\n",
    "#\n",
    "#     # Plot 3: SAOCOM Lower\n",
    "#     lower_grid = np.where(lower, diff_grid, np.nan)\n",
    "#     stats3 = f\"Points: {stats['n_lower']:,}\\nMean: {np.nanmean(lower_grid):.2f}m\\nMin: {np.nanmin(lower_grid):.2f}m\"\n",
    "#     plot_gridded_comparison(axes[i, 2], lower_grid, f\"{p['name']} > SAOCOM\\n({stats['pct_lower']:.1f}%)\", plt.cm.Blues_r.copy(), np.nanmin(lower_grid), 0, stats3)\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(RESULTS_DIR / 'saocom_comparison_directional.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "#\n",
    "#\n",
    "# # =============================================================================\n",
    "# # 2. RESIDUAL DISTRIBUTION HISTOGRAMS\n",
    "# # =============================================================================\n",
    "#\n",
    "# def plot_residual_histogram(ax, diff_series, metrics, tolerance, title):\n",
    "#     \"\"\"Helper function to plot a single residual distribution histogram.\"\"\"\n",
    "#     ax.hist(diff_series, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "#     ax.axvline(0, color='red', ls='--', lw=2, label='Zero')\n",
    "#     ax.axvline(metrics['mean_diff'], color='green', ls='-', lw=2, label=f\"Mean: {metrics['mean_diff']:+.2f}m\")\n",
    "#     ax.axvline(tolerance, color='orange', ls='--', lw=1.5, label=f\"\u00b1NMAD: {tolerance:.2f}m\")\n",
    "#     ax.axvline(-tolerance, color='orange', ls='--', lw=1.5)\n",
    "#\n",
    "#     stats_text = f\"n = {metrics['n_points']:,}\\nRMSE = {metrics['rmse']:.2f}m\\nNMAD = {metrics['nmad']:.2f}m\\nStd Dev = {metrics['std_diff']:.2f}m\"\n",
    "#     ax.text(0.98, 0.97, stats_text, transform=ax.transAxes, fontsize=9, va='top', ha='right',\n",
    "#             bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, ec='black'))\n",
    "#\n",
    "#     ax.set(xlabel='Elevation Difference (m)', ylabel='Frequency (Log Scale)',\n",
    "#            title=title, yscale='log')\n",
    "#     ax.legend(loc='upper right', fontsize=10)\n",
    "#     ax.grid(True, alpha=0.3)\n",
    "#\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(18, 7), facecolor='white')\n",
    "# plot_residual_histogram(axes[0], saocom_tinitaly_diff, saocom_tinitaly_metrics, saocom_tinitaly_tolerance, 'SAOCOM - TINITALY Distribution')\n",
    "# plot_residual_histogram(axes[1], saocom_copernicus_diff, saocom_copernicus_metrics, saocom_copernicus_tolerance, 'SAOCOM - Copernicus Distribution')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(RESULTS_DIR / 'saocom_residual_distributions.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "#\n",
    "#\n",
    "# # =============================================================================\n",
    "# # 3. ENSEMBLE OUTLIER DETECTION\n",
    "# # =============================================================================\n",
    "#\n",
    "# # Filter for valid heights and run detection methods\n",
    "# valid_saocom = saocom_gdf[saocom_gdf['HEIGHT_ABSOLUTE_TIN'].notna()].copy()\n",
    "# heights = valid_saocom['HEIGHT_ABSOLUTE_TIN']\n",
    "#\n",
    "# # Method 1: IQR (3x multiplier for extreme outliers)\n",
    "# q1, q3 = heights.quantile(0.25), heights.quantile(0.75)\n",
    "# iqr = q3 - q1\n",
    "# outliers_iqr = (heights < (q1 - 3 * iqr)) | (heights > (q3 + 3 * iqr))\n",
    "#\n",
    "# # Method 2: Z-score\n",
    "# outliers_zscore = np.abs(stats.zscore(heights)) > 3\n",
    "#\n",
    "# # Method 3: Modified Z-score (NMAD-based)\n",
    "# median_abs_dev = np.median(np.abs(heights - heights.median()))\n",
    "# mod_z_scores = 0.6745 * (heights - heights.median()) / median_abs_dev\n",
    "# outliers_nmad = np.abs(mod_z_scores) > 3.5\n",
    "#\n",
    "# # Combine methods: outlier if flagged by at least 2\n",
    "# is_outlier = np.sum([outliers_iqr, outliers_zscore, outliers_nmad], axis=0) >= 2\n",
    "# valid_saocom['is_outlier'] = is_outlier\n",
    "# normal_points = valid_saocom[~is_outlier]\n",
    "# outlier_points = valid_saocom[is_outlier]\n",
    "#\n",
    "# print(f\"Ensemble outlier detection complete. Found {len(outlier_points)} outliers.\")\n",
    "#\n",
    "#\n",
    "# # =============================================================================\n",
    "# # 4. OUTLIER VISUALIZATION & ANALYSIS\n",
    "# # =============================================================================\n",
    "#\n",
    "# fig = plt.figure(figsize=(18, 16), facecolor='white')\n",
    "# gs = fig.add_gridspec(2, 2)\n",
    "#\n",
    "# # Plot 1: Spatial Distribution\n",
    "# ax1 = fig.add_subplot(gs[0, 0])\n",
    "# normal_points.plot(ax=ax1, markersize=0.5, color='#2E86AB', alpha=0.4, label=f'Normal ({len(normal_points):,})')\n",
    "# if not outlier_points.empty:\n",
    "#     outlier_points.plot(ax=ax1, markersize=8, color='#E63946', alpha=0.9, ec='black', lw=0.3, label=f'Outliers ({len(outlier_points):,})')\n",
    "# hull_gdf.boundary.plot(ax=ax1, color='black', lw=2, ls='--')\n",
    "# ax1.set(title='SAOCOM Height Outliers - Spatial Distribution', xlabel='UTM Easting (m)', ylabel='UTM Northing (m)')\n",
    "# ax1.legend(); ax1.grid(True, alpha=0.3)\n",
    "#\n",
    "# # Plot 2: Height Distribution\n",
    "# ax2 = fig.add_subplot(gs[0, 1])\n",
    "# ax2.hist(normal_points['HEIGHT_ABSOLUTE_TIN'], bins=50, alpha=0.6, color='#2E86AB', label='Normal')\n",
    "# if not outlier_points.empty:\n",
    "#     ax2.hist(outlier_points['HEIGHT_ABSOLUTE_TIN'], bins=20, alpha=0.8, color='#E63946', label='Outliers')\n",
    "# ax2.axvline(q1 - 3 * iqr, color='orange', ls='--', label=f'IQR Bound')\n",
    "# ax2.axvline(q3 + 3 * iqr, color='orange', ls='--')\n",
    "# ax2.set(title='Height Distribution with Outlier Thresholds', xlabel='Height (m)', ylabel='Frequency')\n",
    "# ax2.legend(); ax2.grid(True, alpha=0.3)\n",
    "#\n",
    "# # Plot 3: Box Plot\n",
    "# ax3 = fig.add_subplot(gs[1, 0])\n",
    "# box_data = [normal_points['HEIGHT_ABSOLUTE_TIN'].dropna()]\n",
    "# if not outlier_points.empty:\n",
    "#     box_data.append(outlier_points['HEIGHT_ABSOLUTE_TIN'].dropna())\n",
    "# bp = ax3.boxplot(box_data, patch_artist=True, showmeans=True, meanline=True,\n",
    "#                  labels=['Normal', 'Outliers'] if not outlier_points.empty else ['Normal'])\n",
    "# colors = ['#2E86AB', '#E63946']\n",
    "# for patch, color in zip(bp['boxes'], colors):\n",
    "#     patch.set_facecolor(color)\n",
    "# ax3.set(title='Height Distribution Comparison', ylabel='Height (m)')\n",
    "# ax3.grid(True, axis='y', alpha=0.3)\n",
    "#\n",
    "# # Plot 4: Summary Text\n",
    "# ax4 = fig.add_subplot(gs[1, 1])\n",
    "# ax4.axis('off')\n",
    "# summary_text = f\"\"\"OUTLIER ANALYSIS SUMMARY\n",
    "# -----------------------------------\n",
    "# Flagged by IQR Method: {np.sum(outliers_iqr):,}\n",
    "# Flagged by Z-score: {np.sum(outliers_zscore):,}\n",
    "# Flagged by Mod. Z-score: {np.sum(outliers_nmad):,}\n",
    "# -----------------------------------\n",
    "# Final Outliers (\u22652 methods): {len(outlier_points):,} ({len(outlier_points)/len(valid_saocom):.2%})\n",
    "# Normal Points: {len(normal_points):,}\n",
    "# -----------------------------------\n",
    "# Normal Mean Height: {normal_points['HEIGHT_ABSOLUTE_TIN'].mean():.2f} m\n",
    "# Outlier Mean Height: {outlier_points['HEIGHT_ABSOLUTE_TIN'].mean():.2f} m\n",
    "# \"\"\"\n",
    "# ax4.text(0.05, 0.5, summary_text, va='center', fontfamily='monospace',\n",
    "#          bbox=dict(boxstyle='round', facecolor='white', ec='black'))\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(RESULTS_DIR / 'saocom_height_outliers_ensemble.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "#\n",
    "# # =============================================================================\n",
    "# # 5. SCATTER PLOT COMPARISONS\n",
    "# # =============================================================================\n",
    "#\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(18, 16), facecolor='white')\n",
    "# axes[1, 1].axis('off') # Hide unused subplot\n",
    "#\n",
    "# # Define scatter plot configurations\n",
    "# scatter_plots = [\n",
    "#     {'ax': axes[0, 0], 'x': saocom_tinitaly_valid['tinitaly_height'], 'y': saocom_tinitaly_valid['HEIGHT_ABSOLUTE_TIN'],\n",
    "#      'title': 'SAOCOM vs TINITALY', 'xlabel': 'TINITALY Height (m)', 'metrics': saocom_tinitaly_metrics},\n",
    "#     {'ax': axes[0, 1], 'x': saocom_copernicus_valid['copernicus_height'], 'y': saocom_copernicus_valid['HEIGHT_ABSOLUTE_COP'],\n",
    "#      'title': 'SAOCOM vs Copernicus', 'xlabel': 'Copernicus Height (m)', 'metrics': saocom_copernicus_metrics},\n",
    "#     {'ax': axes[1, 0], 'x': valid_copernicus, 'y': valid_tinitaly,\n",
    "#      'title': 'TINITALY vs Copernicus', 'xlabel': 'Copernicus Height (m)', 'ylabel': 'TINITALY Height (m)', 'metrics': ref_metrics},\n",
    "# ]\n",
    "#\n",
    "# for p in scatter_plots:\n",
    "#     ax = p['ax']\n",
    "#     ax.scatter(p['x'], p['y'], s=1, alpha=0.3, c='steelblue', ec='none')\n",
    "#     min_val, max_val = min(p['x'].min(), p['y'].min()), max(p['x'].max(), p['y'].max())\n",
    "#     ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='1:1 line')\n",
    "#\n",
    "#     stats_text = (f\"n = {p['metrics'].get('n_points', p['metrics'].get('n_pixels')):,}\\n\"\n",
    "#                   f\"Bias = {p['metrics']['mean_diff']:.2f}m\\nRMSE = {p['metrics']['rmse']:.2f}m\\n\"\n",
    "#                   f\"NMAD = {p['metrics']['nmad']:.2f}m\\nCorr (r) = {p['metrics']['correlation']:.3f}\")\n",
    "#\n",
    "#     ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=9, va='top', fontfamily='monospace',\n",
    "#             bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, ec='black'))\n",
    "#\n",
    "#     ax.set(title=p['title'], xlabel=p['xlabel'], ylabel=p.get('ylabel', 'SAOCOM Height (m)'))\n",
    "#     ax.legend(); ax.grid(True, alpha=0.3)\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "id": "f8ffbcb8ce5b6d3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### VOID ZONES vs LAND COVER ANALYSIS",
   "id": "957d6fce3ae6ea1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# VOID ZONES vs LAND COVER ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CALCULATE VOID STATISTICS BY LAND COVER\n",
    "# =============================================================================\n",
    "void_lc_stats = []\n",
    "\n",
    "for lc_code in np.unique(corine_10m[corine_10m > 0]):\n",
    "    lc_mask = (corine_10m == lc_code) & study_area_mask\n",
    "\n",
    "    total_lc_cells = np.sum(lc_mask)\n",
    "    void_lc_cells = np.sum(lc_mask & void_mask)\n",
    "\n",
    "    if total_lc_cells == 0:\n",
    "        continue\n",
    "\n",
    "    pct_lc_is_void = 100 * void_lc_cells / total_lc_cells\n",
    "    pct_of_total_voids = 100 * void_lc_cells / np.sum(void_mask)\n",
    "\n",
    "    void_lc_stats.append({\n",
    "        'LC_Code': lc_code,\n",
    "        'LC_Name': CORINE_CLASSES.get(lc_code, f'Unknown_{lc_code}'),\n",
    "        'Total_Cells': total_lc_cells,\n",
    "        'Void_Cells': void_lc_cells,\n",
    "        'Area_km2': total_lc_cells * (GRID_SIZE**2) / 1e6,\n",
    "        'Void_Area_km2': void_lc_cells * (GRID_SIZE**2) / 1e6,\n",
    "        'Pct_LC_is_Void': pct_lc_is_void,\n",
    "        'Pct_of_Total_Voids': pct_of_total_voids\n",
    "    })\n",
    "\n",
    "void_lc_df = pd.DataFrame(void_lc_stats).sort_values('Pct_LC_is_Void', ascending=False)\n",
    "\n",
    "# Display table\n",
    "print(f\"\\n{'='*120}\")\n",
    "print(\"VOID ZONES BY LAND COVER CLASS\")\n",
    "print(f\"{'='*120}\")\n",
    "print(void_lc_df[['LC_Code', 'LC_Name', 'Area_km2', 'Void_Area_km2', 'Pct_LC_is_Void', 'Pct_of_Total_Voids']].to_string(index=False))\n",
    "print(f\"{'='*120}\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. MAP: VOID ZONES WITH LAND COVER OVERLAY\n",
    "# =============================================================================\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 12), facecolor='white')\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "extent = [xmin_grid, xmax_grid, ymin_grid, ymax_grid]\n",
    "\n",
    "# Sentinel background\n",
    "ax.imshow(sentinel_rgb_norm, extent=extent, origin='upper', alpha=0.6)\n",
    "\n",
    "# Land cover in void zones only\n",
    "lc_in_voids = corine_10m.copy()\n",
    "lc_in_voids[~void_mask] = 0\n",
    "lc_display = np.ma.masked_where(lc_in_voids == 0, lc_in_voids)\n",
    "\n",
    "void_codes = np.unique(lc_display.compressed())\n",
    "\n",
    "if len(void_codes) > 0:\n",
    "    colors_list = [CORINE_COLORS_MPL.get(code, (0.5, 0.5, 0.5)) for code in void_codes]\n",
    "    cmap = ListedColormap(colors_list)\n",
    "    norm = BoundaryNorm(boundaries=np.append(void_codes, void_codes[-1]+1) - 0.5,\n",
    "                        ncolors=len(void_codes))\n",
    "\n",
    "    im = ax.imshow(lc_display, cmap=cmap, norm=norm, origin='upper',\n",
    "                  extent=extent, alpha=0.7)\n",
    "\n",
    "# Study area boundary\n",
    "hull_gdf.boundary.plot(ax=ax, color='black', linewidth=2.5, linestyle='--')\n",
    "\n",
    "# Statistics box\n",
    "void_area = np.sum(void_mask) * (GRID_SIZE**2) / 1e6\n",
    "total_area = np.sum(study_area_mask) * (GRID_SIZE**2) / 1e6\n",
    "pct_void = 100 * np.sum(void_mask) / np.sum(study_area_mask)\n",
    "\n",
    "stats_text = f\"\"\"Void Area: {void_area:.2f} km\u00b2\n",
    "Total Area: {total_area:.2f} km\u00b2\n",
    "Void %: {pct_void:.1f}%\n",
    "LC Classes: {len(void_codes)}\"\"\"\n",
    "\n",
    "ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=11,\n",
    "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white',\n",
    "        alpha=0.9, edgecolor='black'))\n",
    "\n",
    "# Legend\n",
    "if len(void_codes) > 0:\n",
    "    legend_elements = [mpatches.Rectangle((0,0),1,1,\n",
    "                                         facecolor=CORINE_COLORS_MPL[code],\n",
    "                                         edgecolor='black', linewidth=0.5,\n",
    "                                         label=f\"{code}: {CORINE_CLASSES.get(code, 'Unknown')}\")\n",
    "                      for code in sorted(void_codes)]\n",
    "\n",
    "    ax.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5),\n",
    "             fontsize=9, frameon=True, fancybox=False, edgecolor='black',\n",
    "             title='Land Cover in Void Zones')\n",
    "\n",
    "ax.set_title('Land Cover Distribution in SAOCOM Void Zones',\n",
    "             fontweight='bold', fontsize=14, pad=15)\n",
    "ax.set_xlabel('UTM Easting (m)', fontsize=11)\n",
    "ax.set_ylabel('UTM Northing (m)', fontsize=11)\n",
    "ax.grid(True, alpha=0.3, color='gray', linewidth=0.5)\n",
    "saocom_gdf.plot(ax=ax, markersize=1, color='black', alpha=0.8, label='SAOCOM Points')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'voids_by_landcover_map.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: voids_by_landcover_map.png\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. BAR CHART: WHICH LAND COVERS HAVE THE MOST VOIDS\n",
    "# =============================================================================\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8), facecolor='white')\n",
    "\n",
    "# Chart 1: % of each land cover that is void\n",
    "top_pct = void_lc_df.nlargest(15, 'Pct_LC_is_Void')\n",
    "bars1 = ax1.barh(range(len(top_pct)), top_pct['Pct_LC_is_Void'])\n",
    "\n",
    "for i, (_, row) in enumerate(top_pct.iterrows()):\n",
    "    bars1[i].set_color(CORINE_COLORS_MPL.get(row['LC_Code'], (0.5, 0.5, 0.5)))\n",
    "    bars1[i].set_edgecolor('black')\n",
    "    bars1[i].set_linewidth(0.5)\n",
    "\n",
    "ax1.set_yticks(range(len(top_pct)))\n",
    "ax1.set_yticklabels([f\"{row['LC_Code']}: {row['LC_Name'][:35]}\"\n",
    "                      for _, row in top_pct.iterrows()], fontsize=9)\n",
    "ax1.set_xlabel('% of Land Cover Class that is Void', fontsize=11)\n",
    "ax1.set_title('Land Covers with Highest Void Percentage\\n(Worst Coverage Performance)',\n",
    "              fontweight='bold', fontsize=12)\n",
    "ax1.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "ax1.axvline(pct_void, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Overall Void Rate: {pct_void:.1f}%')\n",
    "ax1.legend()\n",
    "\n",
    "# Chart 2: Contribution to total voids\n",
    "top_contrib = void_lc_df.nlargest(15, 'Pct_of_Total_Voids')\n",
    "bars2 = ax2.barh(range(len(top_contrib)), top_contrib['Pct_of_Total_Voids'])\n",
    "\n",
    "for i, (_, row) in enumerate(top_contrib.iterrows()):\n",
    "    bars2[i].set_color(CORINE_COLORS_MPL.get(row['LC_Code'], (0.5, 0.5, 0.5)))\n",
    "    bars2[i].set_edgecolor('black')\n",
    "    bars2[i].set_linewidth(0.5)\n",
    "\n",
    "ax2.set_yticks(range(len(top_contrib)))\n",
    "ax2.set_yticklabels([f\"{row['LC_Code']}: {row['LC_Name'][:35]}\"\n",
    "                      for _, row in top_contrib.iterrows()], fontsize=9)\n",
    "ax2.set_xlabel('% of Total Void Area', fontsize=11)\n",
    "ax2.set_title('Land Covers Contributing Most to Total Voids\\n(Largest Void Areas)',\n",
    "              fontweight='bold', fontsize=12)\n",
    "ax2.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'voids_by_landcover_charts.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved: voids_by_landcover_charts.png\")\n",
    "print(\"\\nVoid analysis complete!\")"
   ],
   "id": "25fa4553895dcd59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### VOID ZONES vs LAND COVER - \"SWISS CHEESE\" VISUALIZATION",
   "id": "4b2d47e7bbe85ff9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # =============================================================================\n",
    "# # VOID ZONES vs LAND COVER - \"SWISS CHEESE\" VISUALIZATION\n",
    "# # =============================================================================\n",
    "#\n",
    "# # =============================================================================\n",
    "# # LAND COVER WITH VOIDS - OUTLINED POLYGONS VERSION\n",
    "# # =============================================================================\n",
    "#\n",
    "#\n",
    "#\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(16, 12), facecolor='white')\n",
    "# ax.set_facecolor('white')\n",
    "#\n",
    "# extent = [xmin_grid, xmax_grid, ymin_grid, ymax_grid]\n",
    "#\n",
    "# # Get unique land cover classes\n",
    "# display_codes = np.unique(corine_10m[(corine_10m > 0) & ~void_mask])\n",
    "#\n",
    "# print(\"Vectorizing land cover polygons (this may take a moment)...\")\n",
    "# im = ax.imshow(corine_display, cmap=cmap, norm=norm, origin='upper', extent=extent, alpha = 0.2)\n",
    "#\n",
    "# # Process each land cover class\n",
    "# for lc_code in sorted(display_codes):\n",
    "#     # Create mask: this land cover AND has SAOCOM coverage (not void)\n",
    "#     lc_with_coverage = (corine_10m == lc_code) & (~void_mask)\n",
    "#\n",
    "#     # Vectorize to get boundaries\n",
    "#     mask_shapes = shapes(lc_with_coverage.astype(np.uint8),\n",
    "#                         mask=lc_with_coverage,\n",
    "#                         transform=target_transform)\n",
    "#\n",
    "#     # Convert to shapely polygons\n",
    "#     polys = [shape(geom) for geom, val in mask_shapes if val == 1]\n",
    "#\n",
    "#     # Get color for this land cover\n",
    "#     fill_color = CORINE_COLORS_MPL.get(lc_code, (0.5, 0.5, 0.5))\n",
    "#\n",
    "#     # Draw each polygon\n",
    "#     for poly in polys:\n",
    "#         if poly.is_valid:\n",
    "#             x, y = poly.exterior.xy\n",
    "#\n",
    "#             # Very faint fill\n",
    "#             ax.fill(x, y, color=fill_color, alpha=0.15, edgecolor='none', zorder=1)\n",
    "#\n",
    "#             # Colored outline\n",
    "#             ax.plot(x, y, color=fill_color, linewidth=1.5, alpha=0.9, zorder=2)\n",
    "#\n",
    "# # Study area boundary (on top)\n",
    "# hull_gdf.boundary.plot(ax=ax, color='black', linewidth=2.5, linestyle='--', zorder=3)\n",
    "#\n",
    "# # Statistics\n",
    "# void_area = np.sum(void_mask) * (GRID_SIZE**2) / 1e6\n",
    "# total_area = np.sum(study_area_mask) * (GRID_SIZE**2) / 1e6\n",
    "# pct_void = 100 * np.sum(void_mask) / np.sum(study_area_mask)\n",
    "#\n",
    "# stats_text = f\"\"\"Void Area: {void_area:.2f} km\u00b2\n",
    "# Coverage Area: {total_area - void_area:.2f} km\u00b2\n",
    "# Void %: {pct_void:.1f}%\n",
    "# Coverage %: {100 - pct_void:.1f}%\"\"\"\n",
    "#\n",
    "# ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=13,\n",
    "#         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white',\n",
    "#         alpha=0.9, edgecolor='black'), zorder=4)\n",
    "# # Legend\n",
    "# legend_elements = []\n",
    "# for code in sorted(display_codes):\n",
    "#     color = CORINE_COLORS_MPL.get(code, (0.5, 0.5, 0.5))\n",
    "#     legend_elements.append(mpatches.Rectangle((0,0),1,1,\n",
    "#                                              facecolor=color,\n",
    "#                                              edgecolor=color,\n",
    "#                                              alpha=0.3,\n",
    "#                                              linewidth=2,\n",
    "#                                              label=f\"{code}: {CORINE_CLASSES.get(code, 'Unknown')}\"))\n",
    "#\n",
    "# legend_elements.append(mpatches.Rectangle((0,0),1,1,\n",
    "#                                          facecolor='white',\n",
    "#                                          edgecolor='gray',\n",
    "#                                          linewidth=1,\n",
    "#                                          label='VOID (No SAOCOM Data)'))\n",
    "#\n",
    "# ax.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5),\n",
    "#          fontsize=13, frameon=True, fancybox=False, edgecolor='black',\n",
    "#          title='Land Cover Classes')\n",
    "#\n",
    "# ax.set_title('CORINE Land Cover with SAOCOM Void Zones\\n(White areas = No coverage)',\n",
    "#              fontweight='bold', fontsize=14, pad=15)\n",
    "# ax.set_xlabel('UTM Easting (m)', fontsize=11)\n",
    "# ax.set_ylabel('UTM Northing (m)', fontsize=11)\n",
    "# ax.set_xlim(extent[0], extent[1])\n",
    "# ax.set_ylim(extent[2], extent[3])\n",
    "# ax.grid(True, alpha=0.3, color='gray', linewidth=0.5)\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(RESULTS_DIR / 'landcover_with_voids_outlined.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "# plt.show()\n",
    "#\n",
    "# print(\"Saved: landcover_with_voids_outlined.png\")\n",
    "#\n",
    "# # =============================================================================\n",
    "# # 3. BAR CHART WITH REFERENCE LINES\n",
    "# # =============================================================================\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8), facecolor='white')\n",
    "#\n",
    "# # Chart 1: % of each land cover that is void\n",
    "# top_pct = void_lc_df.nlargest(15, 'Pct_LC_is_Void')\n",
    "# bars1 = ax1.barh(range(len(top_pct)), top_pct['Pct_LC_is_Void'])\n",
    "#\n",
    "# for i, (_, row) in enumerate(top_pct.iterrows()):\n",
    "#     bars1[i].set_color(CORINE_COLORS_MPL.get(row['LC_Code'], (0.5, 0.5, 0.5)))\n",
    "#     bars1[i].set_edgecolor('black')\n",
    "#     bars1[i].set_linewidth(0.5)\n",
    "#\n",
    "# ax1.set_yticks(range(len(top_pct)))\n",
    "# ax1.set_yticklabels([f\"{row['LC_Code']}: {row['LC_Name'][:35]}\"\n",
    "#                       for _, row in top_pct.iterrows()], fontsize=9)\n",
    "# ax1.set_xlabel('% of Land Cover Class that is Void', fontsize=11)\n",
    "# ax1.set_title('Land Covers with Highest Void Percentage\\n(Worst Coverage Performance)',\n",
    "#               fontweight='bold', fontsize=12)\n",
    "# ax1.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "#\n",
    "# # Add reference lines\n",
    "# ax1.axvline(100, color='black', linestyle='-', linewidth=2, label='100% (Total Void)', zorder=3)\n",
    "# for pct in [20, 40, 60, 80]:\n",
    "#     ax1.axvline(pct, color='gray', linestyle='--', linewidth=1.5, alpha=0.7, zorder=3)\n",
    "#     ax1.text(pct, -0.5, f'{pct}%', ha='center', fontsize=9, color='gray')\n",
    "#\n",
    "# ax1.set_xlim(0, 105)\n",
    "# # ax1.legend(loc='lower right')\n",
    "#\n",
    "# # Chart 2: Contribution to total voids\n",
    "# top_contrib = void_lc_df.nlargest(15, 'Pct_of_Total_Voids')\n",
    "# bars2 = ax2.barh(range(len(top_contrib)), top_contrib['Pct_of_Total_Voids'])\n",
    "#\n",
    "# for i, (_, row) in enumerate(top_contrib.iterrows()):\n",
    "#     bars2[i].set_color(CORINE_COLORS_MPL.get(row['LC_Code'], (0.5, 0.5, 0.5)))\n",
    "#     bars2[i].set_edgecolor('black')\n",
    "#     bars2[i].set_linewidth(0.5)\n",
    "#\n",
    "# ax2.set_yticks(range(len(top_contrib)))\n",
    "# ax2.set_yticklabels([f\"{row['LC_Code']}: {row['LC_Name'][:35]}\"\n",
    "#                       for _, row in top_contrib.iterrows()], fontsize=9)\n",
    "# ax2.set_xlabel('% of Total Void Area', fontsize=11)\n",
    "# ax2.set_title('Land Covers Contributing Most to Total Voids\\n(Largest Void Areas)',\n",
    "#               fontweight='bold', fontsize=12)\n",
    "# ax2.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(RESULTS_DIR / 'voids_by_landcover_charts.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "# plt.show()\n",
    "#\n",
    "# print(\"Saved: voids_by_landcover_charts.png\")\n",
    "# print(\"\\nVoid analysis complete!\")"
   ],
   "id": "cfc36e8c939c80c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### INDIVIDUAL LAND COVER MAPS WITH VOID VISUALIZATION",
   "id": "fb0645c338d3dfdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # =============================================================================\n",
    "# # 1. PRE-CALCULATE ALL STATISTICS (DERIVED FROM \"SWISS CHEESE\" DATA)\n",
    "# # =============================================================================\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "#\n",
    "# print(\"Calculating statistics for all land cover classes...\")\n",
    "#\n",
    "# # Get unique classes present in the data\n",
    "# unique_classes = np.unique(corine_10m[corine_10m > 0])\n",
    "#\n",
    "# summary_data = []\n",
    "# for lc_code in sorted(unique_classes):\n",
    "#     # Create masks from the main \"swiss cheese\" arrays\n",
    "#     lc_mask = (corine_10m == lc_code)\n",
    "#     lc_total_pixels = np.sum(lc_mask)\n",
    "#\n",
    "#     if lc_total_pixels > 0:\n",
    "#         lc_void_pixels = np.sum(lc_mask & void_mask)\n",
    "#         lc_coverage_pixels = lc_total_pixels - lc_void_pixels\n",
    "#\n",
    "#         # Calculate statistics\n",
    "#         total_area_km2 = lc_total_pixels * (GRID_SIZE**2) / 1e6\n",
    "#         coverage_area_km2 = lc_coverage_pixels * (GRID_SIZE**2) / 1e6\n",
    "#         void_area_km2 = lc_void_pixels * (GRID_SIZE**2) / 1e6\n",
    "#\n",
    "#         pct_coverage = 100 * lc_coverage_pixels / lc_total_pixels\n",
    "#         pct_void = 100 * lc_void_pixels / lc_total_pixels\n",
    "#         pct_of_study_area = 100 * lc_total_pixels / np.sum(corine_10m > 0)\n",
    "#\n",
    "#         summary_data.append({\n",
    "#             'LC_Code': lc_code,\n",
    "#             'LC_Name': CORINE_CLASSES.get(lc_code, f'Class {lc_code}'),\n",
    "#             'Total_km2': total_area_km2,\n",
    "#             'Coverage_km2': coverage_area_km2,\n",
    "#             'Void_km2': void_area_km2,\n",
    "#             'Pct_Coverage': pct_coverage,\n",
    "#             'Pct_Void': pct_void,\n",
    "#             'Pct_of_Study_Area': pct_of_study_area\n",
    "#         })\n",
    "#\n",
    "# # Create the master DataFrame with all stats\n",
    "# stats_df = pd.DataFrame(summary_data)\n",
    "# stats_df = stats_df.sort_values(by='Pct_Void', ascending=False) # Sort by worst coverage\n",
    "#\n",
    "# print(\"\u2713 Statistics DataFrame created.\")\n",
    "# print(stats_df.to_string(index=False, float_format=\"%.2f\"))\n",
    "#\n",
    "# print(f\"\\nGenerating {len(stats_df)} land cover maps using pre-calculated stats...\")\n",
    "#\n",
    "# # Loop through the DataFrame, one row per land cover class\n",
    "# for index, row in stats_df.iterrows():\n",
    "#     lc_code = row['LC_Code']\n",
    "#     class_name = row['LC_Name']\n",
    "#\n",
    "#     fig, ax = plt.subplots(1, 1, figsize=(14, 10), facecolor='white')\n",
    "#     ax.set_facecolor('white')\n",
    "#\n",
    "#     # Display Sentinel RGB as background\n",
    "#     ax.imshow(sentinel_rgb_norm, extent=[xmin_grid, xmax_grid, ymin_grid, ymax_grid],\n",
    "#               origin='upper', alpha=0.4)\n",
    "#\n",
    "#     fill_color = tuple(c/255 for c in CORINE_COLORS.get(lc_code, (128, 128, 128)))\n",
    "#\n",
    "#     # --- Visualization (Masking and Vectorizing is still required here) ---\n",
    "#     lc_mask = (corine_10m == lc_code)\n",
    "#\n",
    "#     # 1. Draw COVERAGE areas (with data)\n",
    "#     lc_with_coverage = lc_mask & (~void_mask)\n",
    "#     if np.any(lc_with_coverage):\n",
    "#         coverage_shapes = shapes(lc_with_coverage.astype(np.uint8), mask=lc_with_coverage, transform=target_transform)\n",
    "#         for geom, val in coverage_shapes:\n",
    "#             if val == 1:\n",
    "#                 poly = shape(geom)\n",
    "#                 if poly.is_valid:\n",
    "#                     x, y = poly.exterior.xy\n",
    "#                     ax.fill(x, y, color=fill_color, alpha=0.35, edgecolor='none', hatch='///')\n",
    "#                     ax.plot(x, y, color='black', linewidth=2.0)\n",
    "#                     ax.plot(x, y, color=fill_color, linewidth=1.2)\n",
    "#\n",
    "#     # 2. Draw VOID areas (no data)\n",
    "#     lc_void = lc_mask & void_mask\n",
    "#     if np.any(lc_void):\n",
    "#         void_shapes = shapes(lc_void.astype(np.uint8), mask=lc_void, transform=target_transform)\n",
    "#         for geom, val in void_shapes:\n",
    "#             if val == 1:\n",
    "#                 poly = shape(geom)\n",
    "#                 if poly.is_valid:\n",
    "#                     x, y = poly.exterior.xy\n",
    "#                     ax.fill(x, y, color='white', alpha=0.8, edgecolor='none', hatch='....')\n",
    "#                     ax.plot(x, y, color='red', linewidth=1.5, linestyle='--')\n",
    "#                     ax.plot(x, y, color='gray', linewidth=0.8, linestyle='--')\n",
    "#\n",
    "#     hull_gdf.boundary.plot(ax=ax, color='black', linewidth=3, linestyle='-')\n",
    "#\n",
    "#     # --- Use Pre-Calculated Stats for Title and Text ---\n",
    "#     ax.set_title(\n",
    "#         f\"Land Cover: {class_name} (Code {lc_code})\\n\"\n",
    "#         f\"Total: {row['Total_km2']:.1f} km\u00b2 ({row['Pct_of_Study_Area']:.1f}% of study area) | \"\n",
    "#         f\"Coverage: {row['Coverage_km2']:.1f} km\u00b2 ({row['Pct_Coverage']:.1f}%) | \"\n",
    "#         f\"Void: {row['Void_km2']:.1f} km\u00b2 ({row['Pct_Void']:.1f}%)\",\n",
    "#         fontweight='bold', fontsize=12, pad=15\n",
    "#     )\n",
    "#\n",
    "#     stats_text = (\n",
    "#         f\"Coverage: {row['Pct_Coverage']:.1f}%\\n\"\n",
    "#         f\"Void: {row['Pct_Void']:.1f}%\\n\"\n",
    "#         f\"Area w/ data: {row['Coverage_km2']:.1f} km\u00b2\\n\"\n",
    "#         f\"Area w/o data: {row['Void_km2']:.1f} km\u00b2\"\n",
    "#     )\n",
    "#     ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=10,\n",
    "#             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='black'))\n",
    "#\n",
    "#     # --- Legend ---\n",
    "#     legend_elements = [\n",
    "#         Patch(facecolor=fill_color, edgecolor='black', linewidth=2, alpha=0.35, hatch='///', label=f'{class_name} (Coverage)'),\n",
    "#         Patch(facecolor='white', edgecolor='red', linewidth=1.5, alpha=0.8, hatch='....', linestyle='--', label=f'{class_name} (Void/No Data)'),\n",
    "#         Patch(facecolor='none', edgecolor='black', linewidth=3, label='Study Area Boundary')\n",
    "#     ]\n",
    "#     ax.legend(handles=legend_elements, loc='upper right', fontsize=10, frameon=True, fancybox=False, edgecolor='black', title='Legend')\n",
    "#\n",
    "#     ax.set_xlabel('UTM Easting (m)')\n",
    "#     ax.set_ylabel('UTM Northing (m)')\n",
    "#     ax.grid(True, alpha=0.3, color='gray', linewidth=0.5)\n",
    "#     plt.tight_layout()\n",
    "#\n",
    "#     # --- Save File ---\n",
    "#     safe_name = class_name.replace(' ', '_').replace(',', '').replace('/', '_')\n",
    "#     filename = f'landcover_{lc_code}_{safe_name}_coverage_void.png'\n",
    "#     plt.savefig(RESULTS_DIR / filename, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "#     plt.show()\n",
    "#     plt.close()\n",
    "#\n",
    "#     print(f\"Saved: {filename}\")\n",
    "#\n",
    "# print(f\"\\n\u2713 Generated {len(stats_df)} land cover maps.\")\n",
    "# print(f\"All maps saved to: {RESULTS_DIR}\")"
   ],
   "id": "5d96be91dab3fa96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Land cover histograms",
   "id": "f0eec84a041ef2aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.transform import rowcol\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Re-define necessary variables and functions from your notebook\n",
    "# =============================================================================\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "NODATA = -9999\n",
    "CORINE_CLASSES = {\n",
    "    111: 'Continuous urban fabric', 112: 'Discontinuous urban fabric',\n",
    "    121: 'Industrial or commercial units', 122: 'Road and rail networks and associated land',\n",
    "    123: 'Port areas', 124: 'Airports', 131: 'Mineral extraction sites',\n",
    "    132: 'Dump sites', 133: 'Construction sites', 141: 'Green urban areas',\n",
    "    142: 'Sport and leisure facilities', 211: 'Non-irrigated arable land',\n",
    "    212: 'Permanently irrigated land', 213: 'Rice fields', 221: 'Vineyards',\n",
    "    222: 'Fruit trees and berry plantations', 223: 'Olive groves',\n",
    "    231: 'Pastures', 241: 'Annual crops associated with permanent crops',\n",
    "    242: 'Complex cultivation patterns', 243: 'Agriculture/natural vegetation mix',\n",
    "    244: 'Agro-forestry areas', 311: 'Broad-leaved forest',\n",
    "    312: 'Coniferous forest', 313: 'Mixed forest', 321: 'Natural grasslands',\n",
    "    322: 'Moors and heathland', 323: 'Sclerophyllous vegetation',\n",
    "    324: 'Transitional woodland-shrub', 331: 'Beaches, dunes, sands',\n",
    "    332: 'Bare rocks', 333: 'Sparsely vegetated areas', 334: 'Burnt areas',\n",
    "    335: 'Glaciers and perpetual snow', 411: 'Inland marshes',\n",
    "    412: 'Peat bogs', 421: 'Salt marshes', 422: 'Salines',\n",
    "    423: 'Intertidal flats', 511: 'Water courses', 512: 'Water bodies',\n",
    "    521: 'Coastal lagoons', 522: 'Estuaries', 523: 'Sea and ocean'\n",
    "}\n",
    "\n",
    "# Assuming 'corine_10m_masked.tif' is in the RESULTS_DIR from previous cells\n",
    "corine_masked_path = RESULTS_DIR / \"corine_10m_masked.tif\"\n",
    "with rasterio.open(corine_masked_path) as src:\n",
    "    corine_10m = src.read(1)\n",
    "    target_transform = src.transform\n",
    "    grid_height, grid_width = src.height, src.width\n",
    "\n",
    "# Helper function from your notebook\n",
    "def plot_distribution(ax, diff_series, title, metrics):\n",
    "    \"\"\"Helper function to plot a single residual distribution histogram.\"\"\"\n",
    "    ax.set_facecolor('white')\n",
    "    ax.hist(diff_series, bins=20, alpha=0.75, color='steelblue', edgecolor='black')\n",
    "    ax.axvline(0, color='red', linestyle='--', label='Zero')\n",
    "    ax.axvline(metrics['mean_diff'], color='green', linestyle='-', label=f\"Mean: {metrics['mean_diff']:+.2f}m\")\n",
    "\n",
    "    stats_text = (f\"n = {metrics['n_points']:,}\\n\"\n",
    "                  f\"RMSE = {metrics['rmse']:.2f} m\\n\"\n",
    "                  f\"NMAD = {metrics['nmad']:.2f} m\\n\"\n",
    "                  f\"Std Dev = {metrics['std_diff']:.2f} m\")\n",
    "\n",
    "    ax.text(0.98, 0.97, stats_text, transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='top', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "    ax.set_title(title, fontweight='bold', fontsize=14)\n",
    "    ax.set_xlabel('Elevation Difference (m)', fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Sample CORINE Land Cover at SAOCOM point locations\n",
    "# =============================================================================\n",
    "landcover_values = []\n",
    "for idx, row in saocom_gdf.iterrows():\n",
    "    row_idx, col_idx = rowcol(target_transform, row.geometry.x, row.geometry.y)\n",
    "    if 0 <= row_idx < grid_height and 0 <= col_idx < grid_width:\n",
    "        lc_val = corine_10m[row_idx, col_idx]\n",
    "        landcover_values.append(lc_val if lc_val != 0 else np.nan)\n",
    "    else:\n",
    "        landcover_values.append(np.nan)\n",
    "\n",
    "saocom_gdf['landcover'] = landcover_values\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Create points dataframes with landcover information\n",
    "# =============================================================================\n",
    "\n",
    "# SAOCOM - TINITALY\n",
    "points_tin = saocom_gdf[['HEIGHT_ABSOLUTE_TIN', 'tinitaly_height', 'landcover']].copy()\n",
    "points_tin.dropna(inplace=True)\n",
    "points_tin['diff'] = points_tin['HEIGHT_ABSOLUTE_TIN'] - points_tin['tinitaly_height']\n",
    "\n",
    "# SAOCOM - Copernicus\n",
    "points_cop = saocom_gdf[['HEIGHT_ABSOLUTE_COP', 'copernicus_height', 'landcover']].copy()\n",
    "points_cop.dropna(inplace=True)\n",
    "points_cop['diff'] = points_cop['HEIGHT_ABSOLUTE_COP'] - points_cop['copernicus_height']\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Group by landcover and calculate metrics\n",
    "# =============================================================================\n",
    "\n",
    "landcover_groups_tin = points_tin.groupby('landcover')\n",
    "landcover_groups_cop = points_cop.groupby('landcover')\n",
    "\n",
    "metrics_by_landcover = {}\n",
    "\n",
    "unique_landcovers = sorted(points_tin['landcover'].unique())\n",
    "\n",
    "for lc_code in unique_landcovers:\n",
    "    if np.isnan(lc_code):\n",
    "        continue\n",
    "    lc_name = CORINE_CLASSES.get(lc_code, f'Unknown ({lc_code})')\n",
    "    metrics_by_landcover[lc_name] = {}\n",
    "\n",
    "    # TINITALY metrics\n",
    "    if lc_code in landcover_groups_tin.groups:\n",
    "        group_tin = landcover_groups_tin.get_group(lc_code)\n",
    "        diff_tin = group_tin['diff']\n",
    "        metrics_by_landcover[lc_name]['tin'] = {\n",
    "            'n_points': len(diff_tin),\n",
    "            'rmse': np.sqrt(np.mean(diff_tin**2)),\n",
    "            'nmad': stats.median_abs_deviation(diff_tin, scale='normal'),\n",
    "            'mean_diff': np.mean(diff_tin),\n",
    "            'std_diff': np.std(diff_tin),\n",
    "            'diff_series': diff_tin\n",
    "        }\n",
    "\n",
    "    # Copernicus metrics\n",
    "    if lc_code in landcover_groups_cop.groups:\n",
    "        group_cop = landcover_groups_cop.get_group(lc_code)\n",
    "        diff_cop = group_cop['diff']\n",
    "        metrics_by_landcover[lc_name]['cop'] = {\n",
    "            'n_points': len(diff_cop),\n",
    "            'rmse': np.sqrt(np.mean(diff_cop**2)),\n",
    "            'nmad': stats.median_abs_deviation(diff_cop, scale='normal'),\n",
    "            'mean_diff': np.mean(diff_cop),\n",
    "            'std_diff': np.std(diff_cop),\n",
    "            'diff_series': diff_cop\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Generate and save histograms for each landcover\n",
    "# =============================================================================\n",
    "\n",
    "for lc_name, metrics in metrics_by_landcover.items():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 7), facecolor='white')\n",
    "    fig.suptitle(f'Residual Distributions for Landcover: {lc_name}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    if 'tin' in metrics:\n",
    "        plot_distribution(axes[0], metrics['tin']['diff_series'], 'SAOCOM - TINITALY', metrics['tin'])\n",
    "    else:\n",
    "        axes[0].set_title('SAOCOM - TINITALY\\n(No Data)', fontweight='bold', fontsize=14)\n",
    "        axes[0].set_facecolor('lightgray')\n",
    "\n",
    "\n",
    "    if 'cop' in metrics:\n",
    "        plot_distribution(axes[1], metrics['cop']['diff_series'], 'SAOCOM - Copernicus', metrics['cop'])\n",
    "    else:\n",
    "        axes[1].set_title('SAOCOM - Copernicus\\n(No Data)', fontweight='bold', fontsize=14)\n",
    "        axes[1].set_facecolor('lightgray')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "    # Sanitize filename\n",
    "    safe_lc_name = lc_name.replace('/', '_').replace(' ', '_').lower()\n",
    "    plt.savefig(RESULTS_DIR / f'saocom_residuals_{safe_lc_name}.png', dpi=300, facecolor='white')\n",
    "    plt.show()"
   ],
   "id": "fc12c828c6c60f3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# --- Assuming 'saocom_gdf_lc', 'RESULTS_DIR', etc., are pre-loaded ---\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DEFINE A ROBUST SCATTER PLOTTING FUNCTION\n",
    "# =============================================================================\n",
    "def plot_scatter_comparison(ax, x_data, y_data, x_label, y_label, title, global_lims):\n",
    "    \"\"\"\n",
    "    Creates a 1:1 scatter plot with consistent axes and statistics.\n",
    "    \"\"\"\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    # --- Clean data and calculate stats ---\n",
    "    valid_mask = ~np.isnan(x_data) & ~np.isnan(y_data)\n",
    "    x_val, y_val = x_data[valid_mask], y_data[valid_mask]\n",
    "\n",
    "    if len(x_val) > 1:\n",
    "        n_points = len(x_val)\n",
    "        bias = np.mean(y_val - x_val)\n",
    "        rmse = np.sqrt(mean_squared_error(x_val, y_val))\n",
    "        correlation = np.corrcoef(x_val, y_val)[0, 1]\n",
    "        stats_text = (f\"n = {n_points:,}\\n\"\n",
    "                      f\"Bias = {bias:.2f} m\\n\"\n",
    "                      f\"RMSE = {rmse:.2f} m\\n\"\n",
    "                      f\"Corr (r) = {correlation:.3f}\")\n",
    "    else:\n",
    "        stats_text = \"Not enough data\"\n",
    "\n",
    "    # --- Plotting ---\n",
    "    ax.scatter(x_val, y_val, s=2, alpha=0.4, c='steelblue')\n",
    "    ax.plot(global_lims, global_lims, 'r--', linewidth=2, label='1:1 Line', zorder=10)\n",
    "    ax.set_xlim(global_lims)\n",
    "    ax.set_ylim(global_lims)\n",
    "\n",
    "    # --- Annotation and Styling ---\n",
    "    ax.text(0.04, 0.96, stats_text, transform=ax.transAxes, fontsize=9,\n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor='black'))\n",
    "    ax.set_title(title, fontweight='bold', fontsize=12)\n",
    "    ax.set_xlabel(x_label, fontsize=11)\n",
    "    ax.set_ylabel(y_label, fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "\n",
    "# =============================================================================\n",
    "# 2. CALCULATE GLOBAL AXIS LIMITS FOR CONSISTENCY\n",
    "# =============================================================================\n",
    "# Combine all height data to find the absolute min and max\n",
    "all_heights = pd.concat([\n",
    "    saocom_gdf_lc['HEIGHT_ABSOLUTE_TIN'],\n",
    "    saocom_gdf_lc['tinitaly_height'],\n",
    "    saocom_gdf_lc['copernicus_height']\n",
    "]).dropna()\n",
    "all_heights = np.array([i for i in all_heights if i != -9999])\n",
    "print(sorted(all_heights)[0])\n",
    "min_lim = all_heights.min()\n",
    "max_lim = all_heights.max()\n",
    "print(min_lim, max_lim)\n",
    "buffer = (max_lim - min_lim) * 0.05  # 5% buffer\n",
    "\n",
    "# Define the global limits to be used in all plots\n",
    "global_axis_lims = [min_lim - buffer, max_lim + buffer]\n",
    "\n",
    "# =============================================================================\n",
    "# 3. LOOP THROUGH EACH LAND COVER TYPE AND CREATE PLOTS\n",
    "# =============================================================================\n",
    "# Get a sorted list of unique land cover types\n",
    "unique_land_covers = sorted(saocom_gdf_lc['land_cover'].unique())\n",
    "\n",
    "for lc_name in unique_land_covers:\n",
    "    # Create a new figure for each land cover type\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7.5), facecolor='white')\n",
    "    fig.suptitle(f'Height Comparison for Land Cover: {lc_name}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Filter the data for the current land cover\n",
    "    subset_gdf = saocom_gdf_lc[saocom_gdf_lc['land_cover'] == lc_name]\n",
    "\n",
    "    # --- Plot 1: SAOCOM vs TINITALY ---\n",
    "    if not subset_gdf['tinitaly_height'].dropna().empty:\n",
    "        plot_scatter_comparison(axes[0],\n",
    "                                x_data=subset_gdf['tinitaly_height'],\n",
    "                                y_data=subset_gdf['HEIGHT_ABSOLUTE_TIN'],\n",
    "                                x_label='TINITALY Height (m)',\n",
    "                                y_label='SAOCOM Height (m)',\n",
    "                                title='SAOCOM vs TINITALY',\n",
    "                                global_lims=global_axis_lims)\n",
    "    else:\n",
    "        axes[0].set_title('SAOCOM vs TINITALY\\n(No Data)', fontweight='bold')\n",
    "        axes[0].set_facecolor('lightgray')\n",
    "        axes[0].set_aspect('equal', 'box')\n",
    "\n",
    "\n",
    "    # --- Plot 2: SAOCOM vs Copernicus ---\n",
    "    if not subset_gdf['copernicus_height'].dropna().empty:\n",
    "        plot_scatter_comparison(axes[1],\n",
    "                                x_data=subset_gdf['copernicus_height'],\n",
    "                                y_data=subset_gdf['HEIGHT_ABSOLUTE_TIN'],\n",
    "                                x_label='Copernicus Height (m)',\n",
    "                                y_label='SAOCOM Height (m)',\n",
    "                                title='SAOCOM vs Copernicus',\n",
    "                                global_lims=global_axis_lims)\n",
    "    else:\n",
    "        axes[1].set_title('SAOCOM vs Copernicus\\n(No Data)', fontweight='bold')\n",
    "        axes[1].set_facecolor('lightgray')\n",
    "        axes[1].set_aspect('equal', 'box')\n",
    "\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.94])\n",
    "\n",
    "    # Sanitize filename and save the figure\n",
    "    safe_lc_name = lc_name.replace('/', '_').replace(' ', '_').lower()\n",
    "    plt.savefig(RESULTS_DIR / f'saocom_scatter_comparison_{safe_lc_name}.png', dpi=300, facecolor='white')\n",
    "    plt.show()"
   ],
   "id": "bce9c16a9dae7ef8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2371ed16e956627b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === Topographic Factors Analysis for InSAR Residuals ===\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_global_ns = globals()\n",
    "\n",
    "\n",
    "def _resolve_dem_path(ns):\n",
    "    dem_candidates = ['dem_path', 'reference_dem_path', 'ref_dem_path']\n",
    "    for key in dem_candidates:\n",
    "        value = ns.get(key)\n",
    "        if value:\n",
    "            return Path(value)\n",
    "    reference_dems = ns.get('reference_dems')\n",
    "    if isinstance(reference_dems, dict):\n",
    "        for dem_key in ('tinitaly_crop', 'copernicus'):\n",
    "            entry = reference_dems.get(dem_key)\n",
    "            if isinstance(entry, dict):\n",
    "                for path_key in ('masked_path', 'resampled_path', 'path'):\n",
    "                    path_val = entry.get(path_key)\n",
    "                    if path_val:\n",
    "                        return Path(path_val)\n",
    "    for fallback_key in (\n",
    "        'tinitaly_masked_path',\n",
    "        'copernicus_masked_path',\n",
    "        'tinitaly_10m_path',\n",
    "        'copernicus_10m_path',\n",
    "        'tinitaly_path',\n",
    "        'copernicus_path',\n",
    "    ):\n",
    "        fallback_val = ns.get(fallback_key)\n",
    "        if fallback_val:\n",
    "            return Path(fallback_val)\n",
    "    raise ValueError(\n",
    "        \"Could not resolve DEM path from expected variables (dem_path, reference_dem_path, ref_dem_path).\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _resolve_points_gdf(ns):\n",
    "    candidate_names = [\n",
    "        'insar_gdf',\n",
    "        'gdf_points',\n",
    "        'points_gdf',\n",
    "        'saocom_gdf_cleaned',\n",
    "        'saocom_gdf',\n",
    "        'saocom_gdf_lc',\n",
    "    ]\n",
    "    for name in candidate_names:\n",
    "        obj = ns.get(name)\n",
    "        if isinstance(obj, gpd.GeoDataFrame) and not obj.empty:\n",
    "            return obj.copy(), name\n",
    "    raise ValueError(\n",
    "        \"Could not find a non-empty GeoDataFrame among expected variables (insar_gdf, gdf_points, points_gdf).\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _resolve_residual_column(gdf):\n",
    "    preferred = [\n",
    "        'residual',\n",
    "        'height_residual',\n",
    "        'height_residual_m',\n",
    "        'residual_m',\n",
    "        'err',\n",
    "        'error',\n",
    "        'diff_tinitaly',\n",
    "        'diff_copernicus',\n",
    "    ]\n",
    "    for col in preferred:\n",
    "        if col in gdf.columns:\n",
    "            return col\n",
    "    for col in gdf.columns:\n",
    "        lower = col.lower()\n",
    "        if any(token in lower for token in ('resid', 'error', 'diff')) and 'coher' not in lower:\n",
    "            return col\n",
    "    raise ValueError('Could not locate a residual column in the points GeoDataFrame.')\n",
    "\n",
    "\n",
    "def _resolve_time_column(gdf):\n",
    "    preferred = ['time_str', 'date_str', 'tbin', 'time_bin']\n",
    "    for col in preferred:\n",
    "        if col in gdf.columns:\n",
    "            return col\n",
    "    for derived in ('SVET', 'LVET'):\n",
    "        if derived in gdf.columns:\n",
    "            return derived\n",
    "    return None\n",
    "\n",
    "\n",
    "def _pearson_corr(x, y):\n",
    "    x_arr = np.asarray(x, dtype='float64')\n",
    "    y_arr = np.asarray(y, dtype='float64')\n",
    "    mask = np.isfinite(x_arr) & np.isfinite(y_arr)\n",
    "    if mask.sum() < 2:\n",
    "        return np.nan\n",
    "    x_sel, y_sel = x_arr[mask], y_arr[mask]\n",
    "    if np.nanstd(x_sel) == 0 or np.nanstd(y_sel) == 0:\n",
    "        return np.nan\n",
    "    return float(np.corrcoef(x_sel, y_sel)[0, 1])\n",
    "\n",
    "\n",
    "def _summarize_bins(df, bin_col_name):\n",
    "    overall = (\n",
    "        df.groupby(bin_col_name)['abs_residual']\n",
    "        .agg(count='count', mean='mean', median='median')\n",
    "        .reset_index()\n",
    "        .assign(time_str='ALL')\n",
    "    )\n",
    "    per_time = (\n",
    "        df.groupby(['time_str', bin_col_name])['abs_residual']\n",
    "        .agg(count='count', mean='mean', median='median')\n",
    "        .reset_index()\n",
    "    )\n",
    "    combined = pd.concat([overall, per_time], ignore_index=True, sort=False)\n",
    "    combined['bin_start'] = combined[bin_col_name].apply(\n",
    "        lambda val: val.left if isinstance(val, pd.Interval) else np.nan\n",
    "    )\n",
    "    combined['bin_end'] = combined[bin_col_name].apply(\n",
    "        lambda val: val.right if isinstance(val, pd.Interval) else np.nan\n",
    "    )\n",
    "    combined['bin_center'] = (combined['bin_start'] + combined['bin_end']) / 2.0\n",
    "    combined['bin_label'] = combined[bin_col_name].astype(str)\n",
    "    return combined.drop(columns=[bin_col_name])\n",
    "\n",
    "\n",
    "# Resolve inputs\n",
    "_dem_path = _resolve_dem_path(_global_ns)\n",
    "points_gdf, _points_name = _resolve_points_gdf(_global_ns)\n",
    "_residual_col = _resolve_residual_column(points_gdf)\n",
    "_time_col = _resolve_time_column(points_gdf)\n",
    "points_gdf = points_gdf.copy()\n",
    "if _time_col is None:\n",
    "    points_gdf['time_str'] = 'ALL'\n",
    "    _time_col = 'time_str'\n",
    "else:\n",
    "    if _time_col != 'time_str':\n",
    "        points_gdf['time_str'] = points_gdf[_time_col].astype(str)\n",
    "        _time_col = 'time_str'\n",
    "    else:\n",
    "        points_gdf['time_str'] = points_gdf[_time_col].astype(str)\n",
    "if points_gdf.crs is None:\n",
    "    raise ValueError('The points GeoDataFrame must have a valid CRS for sampling.')\n",
    "\n",
    "# Determine output directory\n",
    "_output_dir = _global_ns.get('output_dir')\n",
    "if _output_dir is not None:\n",
    "    output_base = Path(_output_dir)\n",
    "else:\n",
    "    base_parent = _global_ns.get('local_path')\n",
    "    if base_parent is None:\n",
    "        base_parent = Path.cwd()\n",
    "    output_base = Path(base_parent) / 'topography_outputs'\n",
    "topography_dir = output_base / 'topography'\n",
    "topography_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Read DEM and compute terrain derivatives\n",
    "with rasterio.open(_dem_path) as src:\n",
    "    dem = src.read(1).astype('float64')\n",
    "    transform = src.transform\n",
    "    dem_crs = src.crs\n",
    "    dem_nodata = src.nodata\n",
    "\n",
    "if dem_nodata is not None:\n",
    "    dem = np.where(dem == dem_nodata, np.nan, dem)\n",
    "\n",
    "dx = abs(transform.a)\n",
    "dy = abs(transform.e)\n",
    "slope = np.full_like(dem, np.nan, dtype='float64')\n",
    "aspect = np.full_like(dem, np.nan, dtype='float64')\n",
    "\n",
    "if dem.size > 0:\n",
    "    z1 = dem[:-2, :-2]\n",
    "    z2 = dem[:-2, 1:-1]\n",
    "    z3 = dem[:-2, 2:]\n",
    "    z4 = dem[1:-1, :-2]\n",
    "    z5 = dem[1:-1, 1:-1]\n",
    "    z6 = dem[1:-1, 2:]\n",
    "    z7 = dem[2:, :-2]\n",
    "    z8 = dem[2:, 1:-1]\n",
    "    z9 = dem[2:, 2:]\n",
    "    window_valid = ~np.isnan(np.stack([z1, z2, z3, z4, z5, z6, z7, z8, z9], axis=0)).any(axis=0)\n",
    "    dzdx = ((z3 + 2 * z6 + z9) - (z1 + 2 * z4 + z7)) / (8.0 * dx)\n",
    "    dzdy = ((z7 + 2 * z8 + z9) - (z1 + 2 * z2 + z3)) / (8.0 * dy)\n",
    "    slope_core = np.degrees(np.arctan(np.hypot(dzdx, dzdy)))\n",
    "    aspect_core = np.degrees(np.arctan2(dzdy, -dzdx))\n",
    "    aspect_core = np.where(aspect_core < 0, 360.0 + aspect_core, aspect_core)\n",
    "    slope_core = np.where(window_valid, slope_core, np.nan)\n",
    "    aspect_core = np.where(window_valid, aspect_core, np.nan)\n",
    "    slope[1:-1, 1:-1] = slope_core\n",
    "    aspect[1:-1, 1:-1] = aspect_core\n",
    "\n",
    "# Prepare points and sample terrain values\n",
    "if dem_crs is not None and points_gdf.crs != dem_crs:\n",
    "    points_in_dem = points_gdf.to_crs(dem_crs)\n",
    "else:\n",
    "    points_in_dem = points_gdf.copy()\n",
    "points_in_dem = points_in_dem[points_in_dem[_residual_col].notna()].copy()\n",
    "\n",
    "if points_in_dem.empty:\n",
    "    raise ValueError('No points available with valid residuals for topographic sampling.')\n",
    "\n",
    "coords_x = points_in_dem.geometry.x.to_numpy()\n",
    "coords_y = points_in_dem.geometry.y.to_numpy()\n",
    "row_indices, col_indices = rasterio.transform.rowcol(\n",
    "    transform, coords_x, coords_y, op=np.floor\n",
    ")\n",
    "row_indices = np.asarray(row_indices)\n",
    "col_indices = np.asarray(col_indices)\n",
    "valid_mask = (\n",
    "    (row_indices >= 0)\n",
    "    & (row_indices < dem.shape[0])\n",
    "    & (col_indices >= 0)\n",
    "    & (col_indices < dem.shape[1])\n",
    ")\n",
    "\n",
    "if not np.any(valid_mask):\n",
    "    raise ValueError('No InSAR points overlap the DEM extent for sampling.')\n",
    "\n",
    "sampled_points = points_in_dem.iloc[valid_mask].copy()\n",
    "row_indices = row_indices[valid_mask].astype(int)\n",
    "col_indices = col_indices[valid_mask].astype(int)\n",
    "\n",
    "elev_values = dem[row_indices, col_indices]\n",
    "slope_values = slope[row_indices, col_indices]\n",
    "aspect_values = np.mod(aspect[row_indices, col_indices], 360.0)\n",
    "\n",
    "sampled_points['elevation_m'] = elev_values\n",
    "sampled_points['slope_deg'] = slope_values\n",
    "sampled_points['aspect_deg'] = aspect_values\n",
    "\n",
    "sampled_points = sampled_points.replace([np.inf, -np.inf], np.nan)\n",
    "sampled_points = sampled_points.dropna(\n",
    "    subset=['elevation_m', 'slope_deg', 'aspect_deg', _residual_col, 'time_str']\n",
    ")\n",
    "\n",
    "if sampled_points.empty:\n",
    "    raise ValueError('All sampled terrain values are invalid (nodata). Cannot continue analysis.')\n",
    "\n",
    "sampled_points['residual'] = sampled_points[_residual_col].astype('float64')\n",
    "sampled_points['abs_residual'] = sampled_points['residual'].abs()\n",
    "analysis_df = sampled_points[\n",
    "    ['time_str', 'residual', 'abs_residual', 'elevation_m', 'slope_deg', 'aspect_deg']\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Compute correlations\n",
    "slope_r = _pearson_corr(analysis_df['slope_deg'], analysis_df['abs_residual'])\n",
    "elev_r = _pearson_corr(analysis_df['elevation_m'], analysis_df['abs_residual'])\n",
    "aspect_rad = np.deg2rad(analysis_df['aspect_deg'])\n",
    "sin_r = _pearson_corr(np.sin(aspect_rad), analysis_df['abs_residual'])\n",
    "cos_r = _pearson_corr(np.cos(aspect_rad), analysis_df['abs_residual'])\n",
    "\n",
    "# Bin definitions\n",
    "num_slope_bins = 12\n",
    "num_aspect_bins = 18\n",
    "num_elev_bins = 10\n",
    "\n",
    "slope_min, slope_max = analysis_df['slope_deg'].min(), analysis_df['slope_deg'].max()\n",
    "if np.isfinite(slope_min) and np.isfinite(slope_max) and slope_max > slope_min:\n",
    "    slope_edges = np.linspace(slope_min, slope_max, num_slope_bins + 1)\n",
    "else:\n",
    "    slope_edges = np.linspace(0, max(1.0, slope_max if np.isfinite(slope_max) else 1.0), num_slope_bins + 1)\n",
    "analysis_df['slope_bin'] = pd.cut(\n",
    "    analysis_df['slope_deg'], bins=slope_edges, include_lowest=True, duplicates='drop'\n",
    ")\n",
    "\n",
    "analysis_df['aspect_wrapped'] = np.mod(analysis_df['aspect_deg'], 360.0)\n",
    "aspect_edges = np.linspace(0.0, 360.0, num_aspect_bins + 1)\n",
    "aspect_edges[-1] = 360.0\n",
    "analysis_df['aspect_bin'] = pd.cut(\n",
    "    analysis_df['aspect_wrapped'],\n",
    "    bins=aspect_edges,\n",
    "    include_lowest=True,\n",
    "    right=False,\n",
    "    duplicates='drop',\n",
    ")\n",
    "\n",
    "elev_series = analysis_df['elevation_m']\n",
    "quantile_edges = elev_series.quantile(np.linspace(0, 1, num_elev_bins + 1)).to_numpy()\n",
    "if np.unique(quantile_edges).size < 2:\n",
    "    elev_edges = np.linspace(\n",
    "        float(elev_series.min()), float(elev_series.max() + 1e-6), num_elev_bins + 1\n",
    "    )\n",
    "else:\n",
    "    elev_edges = np.unique(quantile_edges)\n",
    "    if elev_edges.size < 2:\n",
    "        elev_edges = np.linspace(\n",
    "            float(elev_series.min()), float(elev_series.max() + 1e-6), num_elev_bins + 1\n",
    "        )\n",
    "analysis_df['elev_bin'] = pd.cut(\n",
    "    analysis_df['elevation_m'], bins=elev_edges, include_lowest=True, duplicates='drop'\n",
    ")\n",
    "\n",
    "# Summaries\n",
    "slope_binned_stats = (\n",
    "    _summarize_bins(analysis_df.dropna(subset=['slope_bin']), 'slope_bin')\n",
    "    if 'slope_bin' in analysis_df\n",
    "    else pd.DataFrame()\n",
    ")\n",
    "aspect_binned_stats = (\n",
    "    _summarize_bins(analysis_df.dropna(subset=['aspect_bin']), 'aspect_bin')\n",
    "    if 'aspect_bin' in analysis_df\n",
    "    else pd.DataFrame()\n",
    ")\n",
    "elev_binned_stats = (\n",
    "    _summarize_bins(analysis_df.dropna(subset=['elev_bin']), 'elev_bin')\n",
    "    if 'elev_bin' in analysis_df\n",
    "    else pd.DataFrame()\n",
    ")\n",
    "\n",
    "per_time_records = []\n",
    "for time_value, group in analysis_df.groupby('time_str'):\n",
    "    per_time_records.append(\n",
    "        {\n",
    "            'time_str': time_value,\n",
    "            'count': int(len(group)),\n",
    "            'mean_abs_residual': group['abs_residual'].mean(),\n",
    "            'median_abs_residual': group['abs_residual'].median(),\n",
    "            'r_slope_abs_residual': _pearson_corr(group['slope_deg'], group['abs_residual']),\n",
    "            'r_elevation_abs_residual': _pearson_corr(group['elevation_m'], group['abs_residual']),\n",
    "            'r_sin_aspect_abs_residual': _pearson_corr(\n",
    "                np.sin(np.deg2rad(group['aspect_deg'])), group['abs_residual']\n",
    "            ),\n",
    "            'r_cos_aspect_abs_residual': _pearson_corr(\n",
    "                np.cos(np.deg2rad(group['aspect_deg'])), group['abs_residual']\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "per_time_summary_df = pd.DataFrame(per_time_records)\n",
    "overall_summary_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'count': int(len(analysis_df)),\n",
    "            'mean_abs_residual': analysis_df['abs_residual'].mean(),\n",
    "            'median_abs_residual': analysis_df['abs_residual'].median(),\n",
    "            'r_slope_abs_residual': slope_r,\n",
    "            'r_elevation_abs_residual': elev_r,\n",
    "            'r_sin_aspect_abs_residual': sin_r,\n",
    "            'r_cos_aspect_abs_residual': cos_r,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Plotting helpers\n",
    "slope_plot_path = topography_dir / 'plot_slope_vs_abs_residual.png'\n",
    "aspect_plot_path = topography_dir / 'plot_aspect_polar_mean_abs_residual.png'\n",
    "elev_plot_path = topography_dir / 'plot_elevation_vs_abs_residual.png'\n",
    "\n",
    "plt.figure(figsize=(8, 6), facecolor='white')\n",
    "ax = plt.gca()\n",
    "ax.scatter(\n",
    "    analysis_df['slope_deg'], analysis_df['abs_residual'], s=8, alpha=0.25, edgecolors='none'\n",
    ")\n",
    "if not slope_binned_stats.empty:\n",
    "    slope_overall = slope_binned_stats[slope_binned_stats['time_str'] == 'ALL'].dropna(\n",
    "        subset=['bin_center', 'mean']\n",
    "    )\n",
    "    ax.plot(slope_overall['bin_center'], slope_overall['mean'], linewidth=2)\n",
    "ax.set_xlabel('Slope (degrees)')\n",
    "ax.set_ylabel('|Residual| (m)')\n",
    "ax.set_title('Slope vs |Residual|')\n",
    "ax.grid(True, alpha=0.3)\n",
    "text_r = 'Pearson r = {:.3f}'.format(slope_r) if np.isfinite(slope_r) else 'Pearson r = N/A'\n",
    "ax.text(\n",
    "    0.02,\n",
    "    0.98,\n",
    "    text_r,\n",
    "    transform=ax.transAxes,\n",
    "    va='top',\n",
    "    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(slope_plot_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(7, 7), facecolor='white')\n",
    "ax = plt.subplot(111, projection='polar')\n",
    "r_vals = np.array([])\n",
    "if not aspect_binned_stats.empty:\n",
    "    aspect_overall = aspect_binned_stats[\n",
    "        aspect_binned_stats['time_str'] == 'ALL'\n",
    "    ].dropna(subset=['bin_center', 'mean'])\n",
    "    theta = np.deg2rad(aspect_overall['bin_center'].to_numpy())\n",
    "    r_vals = aspect_overall['mean'].to_numpy()\n",
    "    if theta.size > 0:\n",
    "        ax.plot(theta, r_vals, linewidth=2)\n",
    "        ax.fill(theta, r_vals, alpha=0.25)\n",
    "ax.set_theta_zero_location('N')\n",
    "ax.set_theta_direction(-1)\n",
    "ax.set_title('Aspect vs |Residual| (Polar mean)')\n",
    "radius_text = r_vals.max() * 0.9 if r_vals.size else max(analysis_df['abs_residual'].median(), 0.1)\n",
    "annotation = 'r(sin) = {0:.3f}\\nr(cos) = {1:.3f}'.format(\n",
    "    sin_r if np.isfinite(sin_r) else float('nan'),\n",
    "    cos_r if np.isfinite(cos_r) else float('nan'),\n",
    ")\n",
    "ax.text(\n",
    "    np.deg2rad(5),\n",
    "    radius_text,\n",
    "    annotation,\n",
    "    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(aspect_plot_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(9, 6), facecolor='white')\n",
    "ax = plt.gca()\n",
    "if not elev_binned_stats.empty:\n",
    "    elev_overall = elev_binned_stats[elev_binned_stats['time_str'] == 'ALL'].dropna(\n",
    "        subset=['bin_center', 'mean']\n",
    "    )\n",
    "    ax.plot(elev_overall['bin_center'], elev_overall['mean'], marker='o', linewidth=2)\n",
    "    box_positions = []\n",
    "    box_widths = []\n",
    "    box_data = []\n",
    "    if hasattr(analysis_df['elev_bin'], 'cat'):\n",
    "        for interval in analysis_df['elev_bin'].cat.categories:\n",
    "            subset = analysis_df.loc[analysis_df['elev_bin'] == interval, 'abs_residual']\n",
    "            if subset.empty:\n",
    "                continue\n",
    "            center = (interval.left + interval.right) / 2.0\n",
    "            width = (interval.right - interval.left) * 0.6 if np.isfinite(interval.right - interval.left) else 1.0\n",
    "            box_positions.append(center)\n",
    "            box_widths.append(width)\n",
    "            box_data.append(subset)\n",
    "    if box_data:\n",
    "        bp = ax.boxplot(\n",
    "            box_data,\n",
    "            positions=box_positions,\n",
    "            widths=box_widths,\n",
    "            patch_artist=True,\n",
    "            manage_ticks=False,\n",
    "        )\n",
    "        for patch in bp['boxes']:\n",
    "            patch.set_facecolor('lightgray')\n",
    "            patch.set_alpha(0.5)\n",
    "ax.set_xlabel('Elevation (m)')\n",
    "ax.set_ylabel('|Residual| (m)')\n",
    "ax.set_title('Elevation vs |Residual|')\n",
    "ax.grid(True, alpha=0.3)\n",
    "text_e = 'Pearson r = {:.3f}'.format(elev_r) if np.isfinite(elev_r) else 'Pearson r = N/A'\n",
    "ax.text(\n",
    "    0.02,\n",
    "    0.98,\n",
    "    text_e,\n",
    "    transform=ax.transAxes,\n",
    "    va='top',\n",
    "    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(elev_plot_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Write outputs\n",
    "points_path = topography_dir / 'topography_points_with_terrain.parquet'\n",
    "try:\n",
    "    analysis_df.to_parquet(points_path, index=False)\n",
    "except Exception as exc:\n",
    "    fallback_path = topography_dir / 'topography_points_with_terrain.csv'\n",
    "    analysis_df.to_csv(fallback_path, index=False)\n",
    "    print(\n",
    "        f'Warning: Failed to write Parquet ({exc}). CSV fallback saved to {fallback_path}.'\n",
    "    )\n",
    "\n",
    "per_time_summary_df.to_csv(topography_dir / 'topography_per_time_summary.csv', index=False)\n",
    "overall_summary_df.to_csv(topography_dir / 'topography_overall_summary.csv', index=False)\n",
    "slope_binned_stats.to_csv(topography_dir / 'binned_slope_stats.csv', index=False)\n",
    "aspect_binned_stats.to_csv(topography_dir / 'binned_aspect_stats.csv', index=False)\n",
    "elev_binned_stats.to_csv(topography_dir / 'binned_elevation_stats.csv', index=False)\n",
    "\n",
    "print('Topographic analysis complete.')\n",
    "print(f'Points sampled: {len(analysis_df):,}')\n",
    "print(\n",
    "    f\"Overall correlations -> slope vs |residual|: {slope_r:.3f if np.isfinite(slope_r) else float('nan')}, \"\n",
    "    f\"elevation vs |residual|: {elev_r:.3f if np.isfinite(elev_r) else float('nan')}, \"\n",
    "    f\"sin(aspect): {sin_r:.3f if np.isfinite(sin_r) else float('nan')}, \"\n",
    "    f\"cos(aspect): {cos_r:.3f if np.isfinite(cos_r) else float('nan')}\"\n",
    ")\n",
    "print(f'Outputs saved to: {topography_dir}')\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
